{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liable-chain",
   "metadata": {},
   "source": [
    "## Text generation with a miniature GPT\n",
    "\n",
    "### Introduction\n",
    "This example demonstrates how to implement an autoregressive language model using a miniature version of the GPT model. The model consists of a single Transformer block with causal masking in its attention layer. We use the text from the IMDB sentiment classification dataset for training and generate new movie reviews for a given prompt. When using this script with your own dataset, make sure it has at least 1 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "christian-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-spoke",
   "metadata": {},
   "source": [
    "### Prepare the data for word-level language modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bored-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "# label -> pos : 1, neg : 0 \n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"datasets/aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"datasets/aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [1] * len(pos_texts) + [0] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    # sampling 후 reset_index : index 초기화 (https://yganalyst.github.io/data_handling/Pd_2/)\n",
    "    # 두 데이터 프레임을 합치면서 index를 0부터 초기화, drop=True : 기존 index를 버림\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continuous-characteristic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I feel like I've just watched a snuff film.......</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible story, poor acting and no humour at a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow what an episode! After last week seeing Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I saw this movie about a year ago, and found i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This movie should be nominated for a new genre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I feel like I've just watched a snuff film.......          0\n",
       "1  Terrible story, poor acting and no humour at a...          0\n",
       "2  Wow what an episode! After last week seeing Me...          1\n",
       "3  I saw this movie about a year ago, and found i...          1\n",
       "4  This movie should be nominated for a new genre...          0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-excellence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train :  25000\n",
      "# of test :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of train : \", len(train_df))\n",
    "print(\"# of test : \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dutch-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : not sampling\n",
    "train_df = train_df.sample(5000)\n",
    "test_df = test_df.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-reading",
   "metadata": {},
   "source": [
    "### Build Vocab and Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "measured-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor -> bytes(from numpy) -> string\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\").numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "speaking-australian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an interesting idea for a film , both show'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_standardization('An interesting idea for a film, both show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "incredible-treatment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for i in range(len(train_df.review.values)):\n",
    "    if (i+1) % (len(train_df.review.values)/10) == 0:\n",
    "        print(f'{(i+1)/(len(train_df)/100)} % Done')\n",
    "    tokens = custom_standardization((train_df.review.values[i])).split()\n",
    "    corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metric-maker",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'again',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'radio',\n",
       " 'djs',\n",
       " 'inviting',\n",
       " 'listeners',\n",
       " 'to',\n",
       " 'nominate',\n",
       " 'movies',\n",
       " 'that',\n",
       " 'the',\n",
       " 'listener',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'stand',\n",
       " 'or',\n",
       " 'never',\n",
       " 'watched',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'through',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'i',\n",
       " 'think',\n",
       " 'of',\n",
       " '.',\n",
       " '.',\n",
       " '.days',\n",
       " 'later',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'got',\n",
       " 'something',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'a',\n",
       " 'play',\n",
       " 'by',\n",
       " 'shakespeare',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'think',\n",
       " 'i',\n",
       " 'bailed',\n",
       " 'on',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'some',\n",
       " '20',\n",
       " 'odd',\n",
       " 'minutes',\n",
       " 'into',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.think',\n",
       " 'i',\n",
       " 'realised',\n",
       " 'that',\n",
       " 'my',\n",
       " 'toenails',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " 'clip',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'they',\n",
       " 'were',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'me',\n",
       " 'imploringly',\n",
       " 'to',\n",
       " 'get',\n",
       " 'cut',\n",
       " '.',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'boring',\n",
       " 'and',\n",
       " 'pretentious',\n",
       " 'to',\n",
       " 'me',\n",
       " '.',\n",
       " 'even',\n",
       " 'though',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'movie',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'given',\n",
       " 'such',\n",
       " 'a',\n",
       " 'low',\n",
       " 'score',\n",
       " 'to',\n",
       " '(which',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'actually',\n",
       " 'attempted',\n",
       " 'to',\n",
       " 'watch',\n",
       " ')',\n",
       " ',',\n",
       " 'i',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'put',\n",
       " 'you',\n",
       " 'off',\n",
       " 'other',\n",
       " 'movies',\n",
       " 'by',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'english',\n",
       " 'director',\n",
       " ',',\n",
       " 'peter',\n",
       " 'greenaway',\n",
       " '.',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'thinking',\n",
       " 'that',\n",
       " 'his',\n",
       " '\"the',\n",
       " 'cook',\n",
       " ',',\n",
       " 'the',\n",
       " 'thief',\n",
       " ',',\n",
       " 'his',\n",
       " 'wife',\n",
       " 'and',\n",
       " 'her',\n",
       " 'lover',\n",
       " '\"',\n",
       " 'was',\n",
       " 'a',\n",
       " 'truly',\n",
       " 'great',\n",
       " 'british',\n",
       " 'film',\n",
       " 'even',\n",
       " 'though',\n",
       " 'its',\n",
       " 'content',\n",
       " 'was',\n",
       " 'at',\n",
       " 'times',\n",
       " 'stomach',\n",
       " 'churning',\n",
       " '-a',\n",
       " 'brilliant',\n",
       " 'movie',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'people',\n",
       " 'would',\n",
       " 'balk',\n",
       " 'at',\n",
       " 'seeing',\n",
       " 'it',\n",
       " '.',\n",
       " 'another',\n",
       " 'good',\n",
       " 'film',\n",
       " 'by',\n",
       " 'greenaway',\n",
       " 'was',\n",
       " '\"a',\n",
       " 'zed',\n",
       " 'and',\n",
       " 'two',\n",
       " 'noughts',\n",
       " '\"',\n",
       " '.',\n",
       " 'again',\n",
       " ',',\n",
       " 'it',\n",
       " 'had',\n",
       " 'some',\n",
       " 'content',\n",
       " 'that',\n",
       " 'pushed',\n",
       " 'the',\n",
       " 'boundaries',\n",
       " 'of',\n",
       " 'good',\n",
       " 'taste',\n",
       " ',',\n",
       " 'but',\n",
       " 'was',\n",
       " 'intriguing',\n",
       " 'nonetheless',\n",
       " '.',\n",
       " 'the',\n",
       " 'other',\n",
       " 'film',\n",
       " 'that',\n",
       " 'i',\n",
       " 'usually',\n",
       " 'think',\n",
       " 'of',\n",
       " 'too',\n",
       " 'late',\n",
       " 'for',\n",
       " 'such',\n",
       " 'radio',\n",
       " 'show',\n",
       " 'topics',\n",
       " 'is',\n",
       " '\"brazil',\n",
       " '\"',\n",
       " '.',\n",
       " 'never',\n",
       " 'managed',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'through',\n",
       " 'either',\n",
       " '-kept',\n",
       " 'falling',\n",
       " 'asleep',\n",
       " '!',\n",
       " 'unless',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'taste',\n",
       " 'for',\n",
       " 'self',\n",
       " '-important',\n",
       " 'movies',\n",
       " 'which',\n",
       " 'are',\n",
       " 'off',\n",
       " '-puttingly',\n",
       " 'highly',\n",
       " 'stylised',\n",
       " ',',\n",
       " 'laboriously',\n",
       " 'paced',\n",
       " 'and',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'follow',\n",
       " ',',\n",
       " 'then',\n",
       " 'steer',\n",
       " 'clear',\n",
       " 'of',\n",
       " 'prosero',\n",
       " \"'s\",\n",
       " 'books',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "informative-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "corpus_flat=list(chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "japanese-weekly",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'again',\n",
       " 'you',\n",
       " 'hear',\n",
       " 'radio',\n",
       " 'djs',\n",
       " 'inviting',\n",
       " 'listeners',\n",
       " 'to',\n",
       " 'nominate',\n",
       " 'movies',\n",
       " 'that',\n",
       " 'the',\n",
       " 'listener',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'stand',\n",
       " 'or',\n",
       " 'never',\n",
       " 'watched',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'through',\n",
       " '.',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'i',\n",
       " 'think',\n",
       " 'of',\n",
       " '.',\n",
       " '.',\n",
       " '.days',\n",
       " 'later',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'got',\n",
       " 'something',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'a',\n",
       " 'play',\n",
       " 'by',\n",
       " 'shakespeare',\n",
       " '.',\n",
       " 'not',\n",
       " 'sure',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'think',\n",
       " 'i',\n",
       " 'bailed',\n",
       " 'on',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'some',\n",
       " '20',\n",
       " 'odd',\n",
       " 'minutes',\n",
       " 'into',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.think',\n",
       " 'i',\n",
       " 'realised',\n",
       " 'that',\n",
       " 'my',\n",
       " 'toenails',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " 'clip',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'they',\n",
       " 'were',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'me',\n",
       " 'imploringly',\n",
       " 'to',\n",
       " 'get',\n",
       " 'cut',\n",
       " '.',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'boring',\n",
       " 'and',\n",
       " 'pretentious',\n",
       " 'to',\n",
       " 'me',\n",
       " '.',\n",
       " 'even',\n",
       " 'though',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'movie',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'given',\n",
       " 'such',\n",
       " 'a',\n",
       " 'low',\n",
       " 'score',\n",
       " 'to',\n",
       " '(which',\n",
       " 'i',\n",
       " \"'ve\",\n",
       " 'actually',\n",
       " 'attempted',\n",
       " 'to',\n",
       " 'watch',\n",
       " ')',\n",
       " ',',\n",
       " 'i',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " 'want',\n",
       " 'to',\n",
       " 'put',\n",
       " 'you',\n",
       " 'off',\n",
       " 'other',\n",
       " 'movies',\n",
       " 'by',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'english',\n",
       " 'director',\n",
       " ',',\n",
       " 'peter',\n",
       " 'greenaway',\n",
       " '.',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'thinking',\n",
       " 'that',\n",
       " 'his',\n",
       " '\"the',\n",
       " 'cook',\n",
       " ',',\n",
       " 'the',\n",
       " 'thief',\n",
       " ',',\n",
       " 'his',\n",
       " 'wife',\n",
       " 'and',\n",
       " 'her',\n",
       " 'lover',\n",
       " '\"',\n",
       " 'was',\n",
       " 'a',\n",
       " 'truly',\n",
       " 'great',\n",
       " 'british',\n",
       " 'film',\n",
       " 'even',\n",
       " 'though',\n",
       " 'its',\n",
       " 'content',\n",
       " 'was',\n",
       " 'at',\n",
       " 'times',\n",
       " 'stomach',\n",
       " 'churning',\n",
       " '-a',\n",
       " 'brilliant',\n",
       " 'movie',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'people',\n",
       " 'would',\n",
       " 'balk',\n",
       " 'at',\n",
       " 'seeing',\n",
       " 'it',\n",
       " '.',\n",
       " 'another',\n",
       " 'good',\n",
       " 'film',\n",
       " 'by',\n",
       " 'greenaway',\n",
       " 'was',\n",
       " '\"a',\n",
       " 'zed',\n",
       " 'and',\n",
       " 'two',\n",
       " 'noughts',\n",
       " '\"',\n",
       " '.',\n",
       " 'again',\n",
       " ',',\n",
       " 'it',\n",
       " 'had',\n",
       " 'some',\n",
       " 'content',\n",
       " 'that',\n",
       " 'pushed',\n",
       " 'the',\n",
       " 'boundaries',\n",
       " 'of',\n",
       " 'good',\n",
       " 'taste',\n",
       " ',',\n",
       " 'but',\n",
       " 'was',\n",
       " 'intriguing',\n",
       " 'nonetheless',\n",
       " '.',\n",
       " 'the',\n",
       " 'other',\n",
       " 'film',\n",
       " 'that',\n",
       " 'i',\n",
       " 'usually',\n",
       " 'think',\n",
       " 'of',\n",
       " 'too',\n",
       " 'late',\n",
       " 'for',\n",
       " 'such',\n",
       " 'radio',\n",
       " 'show',\n",
       " 'topics',\n",
       " 'is',\n",
       " '\"brazil',\n",
       " '\"',\n",
       " '.',\n",
       " 'never',\n",
       " 'managed',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'through',\n",
       " 'either',\n",
       " '-kept',\n",
       " 'falling',\n",
       " 'asleep',\n",
       " '!',\n",
       " 'unless',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'taste',\n",
       " 'for',\n",
       " 'self',\n",
       " '-important',\n",
       " 'movies',\n",
       " 'which',\n",
       " 'are',\n",
       " 'off',\n",
       " '-puttingly',\n",
       " 'highly',\n",
       " 'stylised',\n",
       " ',',\n",
       " 'laboriously',\n",
       " 'paced',\n",
       " 'and',\n",
       " 'difficult',\n",
       " 'to',\n",
       " 'follow',\n",
       " ',',\n",
       " 'then',\n",
       " 'steer',\n",
       " 'clear',\n",
       " 'of',\n",
       " 'prosero',\n",
       " \"'s\",\n",
       " 'books',\n",
       " '.',\n",
       " 'this',\n",
       " 'sure',\n",
       " 'is',\n",
       " 'one',\n",
       " 'comedy',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'not',\n",
       " 'likely',\n",
       " 'to',\n",
       " 'forget',\n",
       " 'for',\n",
       " 'a',\n",
       " 'while',\n",
       " '.',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " 'normally',\n",
       " 'bother',\n",
       " 'to',\n",
       " 'comment',\n",
       " 'on',\n",
       " 'this',\n",
       " 'movie',\n",
       " ':',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'so',\n",
       " 'minor',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'would',\n",
       " 'watch',\n",
       " 'it',\n",
       " 'anyway',\n",
       " ',',\n",
       " 'but',\n",
       " 'as',\n",
       " 'it',\n",
       " 'happens',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'kind',\n",
       " 'of',\n",
       " 'popular',\n",
       " 'in',\n",
       " 'p2p',\n",
       " 'sharing',\n",
       " 'networks',\n",
       " 'such',\n",
       " 'as',\n",
       " 'kazaa',\n",
       " ',',\n",
       " 'and',\n",
       " 'so',\n",
       " 'this',\n",
       " 'saaad',\n",
       " 'production',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'be',\n",
       " 'exposed',\n",
       " 'for',\n",
       " 'what',\n",
       " 'it',\n",
       " 'is',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'is',\n",
       " 'it',\n",
       " 'then',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'really',\n",
       " 'a',\n",
       " 'comedy',\n",
       " ';',\n",
       " 'instead',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'intended',\n",
       " 'as',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'flick',\n",
       " '-',\n",
       " '-',\n",
       " '\"intended',\n",
       " '\"',\n",
       " 'very',\n",
       " 'much',\n",
       " 'being',\n",
       " 'the',\n",
       " 'key',\n",
       " 'word',\n",
       " 'here',\n",
       " '.',\n",
       " 'the',\n",
       " 'script',\n",
       " 'is',\n",
       " 'a',\n",
       " 'totally',\n",
       " 'incoherent',\n",
       " 'and',\n",
       " 'unbalanced',\n",
       " 'mess',\n",
       " ',',\n",
       " 'the',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'are',\n",
       " 'only',\n",
       " 'special',\n",
       " 'in',\n",
       " 'that',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'especially',\n",
       " 'pathetic',\n",
       " ',',\n",
       " 'and',\n",
       " 'as',\n",
       " 'for',\n",
       " 'the',\n",
       " 'acting',\n",
       " ',',\n",
       " 'well',\n",
       " ',',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'say',\n",
       " 'that',\n",
       " 'if',\n",
       " 'this',\n",
       " 'had',\n",
       " 'been',\n",
       " 'my',\n",
       " 'graduating',\n",
       " 'play',\n",
       " 'at',\n",
       " 'primary',\n",
       " 'school',\n",
       " ',',\n",
       " 'my',\n",
       " 'teachers',\n",
       " 'would',\n",
       " 'have',\n",
       " 'burst',\n",
       " 'out',\n",
       " 'crying',\n",
       " 'at',\n",
       " 'our',\n",
       " 'talent',\n",
       " '.',\n",
       " 'of',\n",
       " 'course',\n",
       " 'i',\n",
       " 'realise',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'low',\n",
       " 'budget',\n",
       " 'film',\n",
       " 'and',\n",
       " 'that',\n",
       " 'in',\n",
       " 'those',\n",
       " 'cases',\n",
       " 'one',\n",
       " 'should',\n",
       " 'lower',\n",
       " 'one',\n",
       " \"'s\",\n",
       " 'expectations',\n",
       " ',',\n",
       " 'certainly',\n",
       " 'as',\n",
       " 'far',\n",
       " 'as',\n",
       " 'things',\n",
       " 'like',\n",
       " 'special',\n",
       " 'effects',\n",
       " 'are',\n",
       " 'concerned',\n",
       " '.',\n",
       " 'also',\n",
       " ',',\n",
       " 'even',\n",
       " 'though',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'a',\n",
       " 'big',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'the',\n",
       " 'horror',\n",
       " 'genre',\n",
       " ',',\n",
       " 'i',\n",
       " \"'m\",\n",
       " 'aware',\n",
       " 'that',\n",
       " 'these',\n",
       " 'movies',\n",
       " 'are',\n",
       " 'only',\n",
       " 'rarely',\n",
       " 'the',\n",
       " 'places',\n",
       " 'to',\n",
       " 'look',\n",
       " 'for',\n",
       " 'interesting',\n",
       " 'scripts',\n",
       " 'and',\n",
       " 'top',\n",
       " 'notch',\n",
       " 'acting',\n",
       " '.',\n",
       " 'but',\n",
       " 'still',\n",
       " '.',\n",
       " 'b',\n",
       " '-movies',\n",
       " 'often',\n",
       " 'have',\n",
       " 'some',\n",
       " 'redeeming',\n",
       " 'features',\n",
       " 'to',\n",
       " 'make',\n",
       " 'up',\n",
       " 'for',\n",
       " 'the',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'funding',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'humour',\n",
       " '.',\n",
       " 'the',\n",
       " 'only',\n",
       " 'laughs',\n",
       " 'in',\n",
       " 'cradle',\n",
       " 'to',\n",
       " 'fear',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'the',\n",
       " 'ridiculous',\n",
       " 'performances',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'humour',\n",
       " 'in',\n",
       " 'that',\n",
       " '-',\n",
       " '-and',\n",
       " 'i',\n",
       " 'could',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'or',\n",
       " 'so',\n",
       " ',',\n",
       " 'gradually',\n",
       " 'dozing',\n",
       " 'off',\n",
       " 'after',\n",
       " 'that',\n",
       " '-',\n",
       " '-then',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'the',\n",
       " 'only',\n",
       " 'thing',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'has',\n",
       " 'to',\n",
       " 'offer',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'that',\n",
       " 'and',\n",
       " 'two',\n",
       " 'or',\n",
       " 'three',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'breasts',\n",
       " '.',\n",
       " 'woohoo',\n",
       " ',',\n",
       " 'how',\n",
       " 'exciting',\n",
       " '.',\n",
       " 'as',\n",
       " 'for',\n",
       " 'the',\n",
       " 'story',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'even',\n",
       " 'that',\n",
       " 'it',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'try',\n",
       " 'to',\n",
       " 'convey',\n",
       " 'anything',\n",
       " ':',\n",
       " 'the',\n",
       " 'victims',\n",
       " 'either',\n",
       " 'use',\n",
       " 'drugs',\n",
       " 'and',\n",
       " '/or',\n",
       " 'are',\n",
       " 'involved',\n",
       " 'in',\n",
       " 'serious',\n",
       " 'crime',\n",
       " '.',\n",
       " 'the',\n",
       " 'lesson',\n",
       " ':',\n",
       " 'watch',\n",
       " 'out',\n",
       " ',',\n",
       " 'naughty',\n",
       " 'boys',\n",
       " 'and',\n",
       " 'girls',\n",
       " ',',\n",
       " 'because',\n",
       " 'one',\n",
       " 'day',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'be',\n",
       " 'made',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'what',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'done',\n",
       " '.',\n",
       " 'i',\n",
       " 'rest',\n",
       " 'my',\n",
       " 'case',\n",
       " '.',\n",
       " 'so',\n",
       " ',',\n",
       " 'all',\n",
       " 'in',\n",
       " 'all',\n",
       " ',',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sex',\n",
       " ',',\n",
       " 'a',\n",
       " 'fair',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'drugs',\n",
       " ',',\n",
       " 'but',\n",
       " 'absolutely',\n",
       " 'zero',\n",
       " 'rock',\n",
       " \"'n\",\n",
       " 'roll',\n",
       " '.',\n",
       " 'i',\n",
       " 'rate',\n",
       " 'this',\n",
       " 'one',\n",
       " '1',\n",
       " 'out',\n",
       " 'of',\n",
       " '10',\n",
       " ',',\n",
       " 'but',\n",
       " 'would',\n",
       " 'go',\n",
       " 'to',\n",
       " '0',\n",
       " 'if',\n",
       " 'i',\n",
       " 'could',\n",
       " '.',\n",
       " 'or',\n",
       " 'perhaps',\n",
       " 'i',\n",
       " 'wouldn',\n",
       " \"'t\",\n",
       " ':',\n",
       " 'it',\n",
       " 'deserves',\n",
       " 'a',\n",
       " '1',\n",
       " 'for',\n",
       " 'spelling',\n",
       " 'the',\n",
       " 'actors',\n",
       " \"'\",\n",
       " 'names',\n",
       " 'correctly',\n",
       " 'in',\n",
       " 'the',\n",
       " 'titles',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'something',\n",
       " ',',\n",
       " 'innit',\n",
       " '?',\n",
       " 'every',\n",
       " 'once',\n",
       " 'in',\n",
       " 'a',\n",
       " 'while',\n",
       " 'the',\n",
       " 'conversation',\n",
       " 'will',\n",
       " 'turn',\n",
       " 'to',\n",
       " '\"favorite',\n",
       " 'movies',\n",
       " '.',\n",
       " '\"',\n",
       " 'i',\n",
       " \"'ll\",\n",
       " 'mention',\n",
       " 'titanic',\n",
       " ',',\n",
       " 'and',\n",
       " 'at',\n",
       " 'least',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'people',\n",
       " 'will',\n",
       " 'snicker',\n",
       " '.',\n",
       " 'i',\n",
       " 'pay',\n",
       " 'them',\n",
       " 'no',\n",
       " 'mind',\n",
       " 'because',\n",
       " 'i',\n",
       " 'know',\n",
       " 'that',\n",
       " 'five',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'these',\n",
       " 'same',\n",
       " 'people',\n",
       " 'were',\n",
       " 'moved',\n",
       " 'to',\n",
       " 'tears',\n",
       " 'by',\n",
       " 'that',\n",
       " 'very',\n",
       " 'movie',\n",
       " '.',\n",
       " 'and',\n",
       " 'they',\n",
       " \"'re\",\n",
       " 'too',\n",
       " 'embarrassed',\n",
       " 'now',\n",
       " 'to',\n",
       " 'admit',\n",
       " 'it',\n",
       " '.',\n",
       " 'i',\n",
       " 'just',\n",
       " 'rewatched',\n",
       " 'titanic',\n",
       " 'for',\n",
       " 'the',\n",
       " 'first',\n",
       " 'time',\n",
       " 'in',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " '.',\n",
       " 'expecting',\n",
       " 'to',\n",
       " 'simply',\n",
       " 'enjoy',\n",
       " 'the',\n",
       " 'story',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'surprised',\n",
       " 'to',\n",
       " 'find',\n",
       " 'that',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'has',\n",
       " 'lost',\n",
       " 'none',\n",
       " 'of',\n",
       " 'its',\n",
       " 'power',\n",
       " 'over',\n",
       " 'these',\n",
       " 'five',\n",
       " 'years',\n",
       " '.',\n",
       " 'i',\n",
       " 'cried',\n",
       " 'again',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'in',\n",
       " 'all',\n",
       " 'the',\n",
       " 'same',\n",
       " 'places',\n",
       " '.',\n",
       " 'it',\n",
       " 'brought',\n",
       " 'me',\n",
       " 'back',\n",
       " 'to',\n",
       " '1997',\n",
       " 'when',\n",
       " 'i',\n",
       " 'can',\n",
       " 'remember',\n",
       " 'how',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'no',\n",
       " 'one',\n",
       " 'thought',\n",
       " 'would',\n",
       " 'break',\n",
       " 'even',\n",
       " 'became',\n",
       " 'the',\n",
       " 'most',\n",
       " 'popular',\n",
       " 'movie',\n",
       " 'of',\n",
       " 'all',\n",
       " 'time',\n",
       " '.',\n",
       " 'a',\n",
       " 'movie',\n",
       " 'that',\n",
       " 'burst',\n",
       " 'into',\n",
       " 'the',\n",
       " 'public',\n",
       " 'consciousness',\n",
       " 'like',\n",
       " 'no',\n",
       " 'other',\n",
       " 'movie',\n",
       " 'i',\n",
       " 'can',\n",
       " 'recall',\n",
       " '(yes',\n",
       " ',',\n",
       " 'even',\n",
       " 'more',\n",
       " 'than',\n",
       " 'star',\n",
       " 'wars',\n",
       " ')',\n",
       " '.',\n",
       " 'and',\n",
       " 'today',\n",
       " ',',\n",
       " 'many',\n",
       " 'people',\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'even',\n",
       " 'admit',\n",
       " 'they',\n",
       " 'enjoyed',\n",
       " 'it',\n",
       " '.',\n",
       " 'folks',\n",
       " ',',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'get',\n",
       " 'something',\n",
       " 'straight',\n",
       " '-',\n",
       " '-',\n",
       " 'you',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'look',\n",
       " 'cool',\n",
       " 'when',\n",
       " 'you',\n",
       " 'badmouth',\n",
       " 'this',\n",
       " 'film',\n",
       " '.',\n",
       " 'you',\n",
       " 'look',\n",
       " 'like',\n",
       " 'an',\n",
       " 'out',\n",
       " 'of',\n",
       " 'touch',\n",
       " 'cynic',\n",
       " '.',\n",
       " 'no',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'perfect',\n",
       " 'and',\n",
       " 'this',\n",
       " 'one',\n",
       " 'has',\n",
       " 'a',\n",
       " 'few',\n",
       " 'faults',\n",
       " '.',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'dialogue',\n",
       " 'falls',\n",
       " 'flat',\n",
       " ',',\n",
       " 'and',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'surrounding',\n",
       " 'the',\n",
       " 'two',\n",
       " 'lovers',\n",
       " 'comes',\n",
       " 'together',\n",
       " 'a',\n",
       " 'little',\n",
       " 'too',\n",
       " 'neatly',\n",
       " '.',\n",
       " 'however',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pursuant-brave",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49003\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens_array = np.array(corpus_flat)\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "vocab_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "economic-rebel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rental-handling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "typical-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('datasets/pickle_data/vocab_text_tg_gpt.pickle', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-columbus",
   "metadata": {},
   "source": [
    "* 사실상 padding 거의 사용 안 됨, 긴 문장에서 80 만큼만 잘라서 input, target 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tamil-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {}\n",
    "token_to_id['[UNK]'] = 0\n",
    "token_to_id['[PAD]'] = 1\n",
    "for i, token in enumerate(vocabulary):\n",
    "    token_to_id[token] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "portuguese-settlement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[UNK]': 0,\n",
       " '[PAD]': 1,\n",
       " 'the': 2,\n",
       " '.': 3,\n",
       " ',': 4,\n",
       " 'and': 5,\n",
       " 'a': 6,\n",
       " 'of': 7,\n",
       " 'to': 8,\n",
       " 'is': 9,\n",
       " 'it': 10,\n",
       " 'in': 11,\n",
       " 'i': 12,\n",
       " 'this': 13,\n",
       " 'that': 14,\n",
       " \"'s\": 15,\n",
       " 'was': 16,\n",
       " 'as': 17,\n",
       " 'with': 18,\n",
       " 'for': 19,\n",
       " 'movie': 20,\n",
       " 'but': 21,\n",
       " 'film': 22,\n",
       " ')': 23,\n",
       " 'you': 24,\n",
       " \"'t\": 25,\n",
       " 'on': 26,\n",
       " '\"': 27,\n",
       " 'not': 28,\n",
       " 'are': 29,\n",
       " 'his': 30,\n",
       " 'he': 31,\n",
       " 'have': 32,\n",
       " 'be': 33,\n",
       " 'one': 34,\n",
       " '!': 35,\n",
       " 'all': 36,\n",
       " 'at': 37,\n",
       " 'by': 38,\n",
       " 'they': 39,\n",
       " 'an': 40,\n",
       " 'who': 41,\n",
       " 'so': 42,\n",
       " 'from': 43,\n",
       " '-': 44,\n",
       " 'her': 45,\n",
       " 'like': 46,\n",
       " 'there': 47,\n",
       " 'just': 48,\n",
       " 'about': 49,\n",
       " 'has': 50,\n",
       " 'out': 51,\n",
       " 'or': 52,\n",
       " 'if': 53,\n",
       " 'some': 54,\n",
       " 'what': 55,\n",
       " '?': 56,\n",
       " 'can': 57,\n",
       " 'good': 58,\n",
       " 'she': 59,\n",
       " 'more': 60,\n",
       " 'very': 61,\n",
       " 'when': 62,\n",
       " 'time': 63,\n",
       " 'up': 64,\n",
       " 'no': 65,\n",
       " 'my': 66,\n",
       " 'even': 67,\n",
       " 'would': 68,\n",
       " 'only': 69,\n",
       " 'their': 70,\n",
       " 'story': 71,\n",
       " 'really': 72,\n",
       " 'see': 73,\n",
       " 'which': 74,\n",
       " 'had': 75,\n",
       " 'we': 76,\n",
       " 'well': 77,\n",
       " 'me': 78,\n",
       " 'were': 79,\n",
       " ':': 80,\n",
       " 'than': 81,\n",
       " \"'\": 82,\n",
       " 'much': 83,\n",
       " 'been': 84,\n",
       " 'into': 85,\n",
       " 'will': 86,\n",
       " 'get': 87,\n",
       " 'him': 88,\n",
       " 'people': 89,\n",
       " 'do': 90,\n",
       " 'also': 91,\n",
       " 'first': 92,\n",
       " 'other': 93,\n",
       " 'great': 94,\n",
       " 'don': 95,\n",
       " 'how': 96,\n",
       " 'because': 97,\n",
       " 'its': 98,\n",
       " 'bad': 99,\n",
       " 'most': 100,\n",
       " 'made': 101,\n",
       " 'way': 102,\n",
       " 'then': 103,\n",
       " 'them': 104,\n",
       " 'make': 105,\n",
       " 'could': 106,\n",
       " 'movies': 107,\n",
       " 'too': 108,\n",
       " 'any': 109,\n",
       " 'think': 110,\n",
       " 'after': 111,\n",
       " 'characters': 112,\n",
       " 'character': 113,\n",
       " 'films': 114,\n",
       " 'two': 115,\n",
       " 'being': 116,\n",
       " 'watch': 117,\n",
       " 'plot': 118,\n",
       " 'many': 119,\n",
       " 'never': 120,\n",
       " 'seen': 121,\n",
       " 'life': 122,\n",
       " ';': 123,\n",
       " 'little': 124,\n",
       " 'best': 125,\n",
       " 'know': 126,\n",
       " '*': 127,\n",
       " 'love': 128,\n",
       " 'over': 129,\n",
       " 'acting': 130,\n",
       " 'did': 131,\n",
       " 'does': 132,\n",
       " 'ever': 133,\n",
       " 'show': 134,\n",
       " 'where': 135,\n",
       " 'your': 136,\n",
       " 'here': 137,\n",
       " 'man': 138,\n",
       " 'off': 139,\n",
       " 'these': 140,\n",
       " 'still': 141,\n",
       " 'end': 142,\n",
       " 'better': 143,\n",
       " \"'ve\": 144,\n",
       " 'say': 145,\n",
       " 'through': 146,\n",
       " 'scene': 147,\n",
       " 'go': 148,\n",
       " 'while': 149,\n",
       " 'scenes': 150,\n",
       " 'why': 151,\n",
       " 'back': 152,\n",
       " 'should': 153,\n",
       " 'such': 154,\n",
       " 'something': 155,\n",
       " 'actors': 156,\n",
       " 'years': 157,\n",
       " 'now': 158,\n",
       " 'those': 159,\n",
       " \"'m\": 160,\n",
       " 'thing': 161,\n",
       " 'doesn': 162,\n",
       " 'before': 163,\n",
       " 'real': 164,\n",
       " 'watching': 165,\n",
       " 'same': 166,\n",
       " 'nothing': 167,\n",
       " 'though': 168,\n",
       " 'another': 169,\n",
       " 'didn': 170,\n",
       " 'new': 171,\n",
       " 'director': 172,\n",
       " 'few': 173,\n",
       " 'work': 174,\n",
       " 'actually': 175,\n",
       " 'look': 176,\n",
       " 'going': 177,\n",
       " 'again': 178,\n",
       " 'find': 179,\n",
       " 'part': 180,\n",
       " 'every': 181,\n",
       " 'funny': 182,\n",
       " 'cast': 183,\n",
       " 'makes': 184,\n",
       " 'lot': 185,\n",
       " 'us': 186,\n",
       " \"'re\": 187,\n",
       " 'old': 188,\n",
       " 'pretty': 189,\n",
       " 'want': 190,\n",
       " '&': 191,\n",
       " 'take': 192,\n",
       " 'world': 193,\n",
       " 'long': 194,\n",
       " 'quite': 195,\n",
       " 'own': 196,\n",
       " 'around': 197,\n",
       " 'between': 198,\n",
       " 'young': 199,\n",
       " 'down': 200,\n",
       " 'may': 201,\n",
       " 'seems': 202,\n",
       " 'things': 203,\n",
       " 'got': 204,\n",
       " 'give': 205,\n",
       " 'fact': 206,\n",
       " 'enough': 207,\n",
       " 'action': 208,\n",
       " 'gets': 209,\n",
       " 'thought': 210,\n",
       " 'family': 211,\n",
       " 'big': 212,\n",
       " 'point': 213,\n",
       " 'original': 214,\n",
       " 'times': 215,\n",
       " 'right': 216,\n",
       " 'both': 217,\n",
       " 'however': 218,\n",
       " 'comedy': 219,\n",
       " 'horror': 220,\n",
       " 'interesting': 221,\n",
       " 'without': 222,\n",
       " 'come': 223,\n",
       " 'must': 224,\n",
       " 'always': 225,\n",
       " 'performance': 226,\n",
       " 'whole': 227,\n",
       " 'least': 228,\n",
       " 'guy': 229,\n",
       " 'saw': 230,\n",
       " 'script': 231,\n",
       " 'almost': 232,\n",
       " 'series': 233,\n",
       " 'done': 234,\n",
       " 'kind': 235,\n",
       " 'far': 236,\n",
       " 'minutes': 237,\n",
       " 'music': 238,\n",
       " 'isn': 239,\n",
       " 'since': 240,\n",
       " 'bit': 241,\n",
       " '\"the': 242,\n",
       " 'feel': 243,\n",
       " 'role': 244,\n",
       " 'anything': 245,\n",
       " \"'ll\": 246,\n",
       " 'might': 247,\n",
       " 'last': 248,\n",
       " 'away': 249,\n",
       " 'probably': 250,\n",
       " 'sure': 251,\n",
       " 'girl': 252,\n",
       " 'woman': 253,\n",
       " 'fun': 254,\n",
       " 'yet': 255,\n",
       " 'worst': 256,\n",
       " 'anyone': 257,\n",
       " 'making': 258,\n",
       " 'each': 259,\n",
       " 'rather': 260,\n",
       " 'believe': 261,\n",
       " 'book': 262,\n",
       " 'tv': 263,\n",
       " 'hard': 264,\n",
       " 'wasn': 265,\n",
       " 'course': 266,\n",
       " 'beautiful': 267,\n",
       " 'looks': 268,\n",
       " 'trying': 269,\n",
       " 'comes': 270,\n",
       " 'day': 271,\n",
       " 'once': 272,\n",
       " 'played': 273,\n",
       " '10': 274,\n",
       " 'found': 275,\n",
       " 'put': 276,\n",
       " 'am': 277,\n",
       " 'reason': 278,\n",
       " 'sense': 279,\n",
       " 'someone': 280,\n",
       " 'having': 281,\n",
       " 'our': 282,\n",
       " 'actor': 283,\n",
       " 'different': 284,\n",
       " 'money': 285,\n",
       " 'place': 286,\n",
       " 'maybe': 287,\n",
       " 'true': 288,\n",
       " 'screen': 289,\n",
       " 'let': 290,\n",
       " 'job': 291,\n",
       " 'dvd': 292,\n",
       " 'main': 293,\n",
       " 'version': 294,\n",
       " 'set': 295,\n",
       " 'play': 296,\n",
       " 'father': 297,\n",
       " 'everything': 298,\n",
       " 'ending': 299,\n",
       " 'although': 300,\n",
       " 'plays': 301,\n",
       " 'three': 302,\n",
       " 'wife': 303,\n",
       " \"'d\": 304,\n",
       " 'looking': 305,\n",
       " '2': 306,\n",
       " 'together': 307,\n",
       " 'war': 308,\n",
       " 'goes': 309,\n",
       " 'instead': 310,\n",
       " 'during': 311,\n",
       " 'half': 312,\n",
       " 'especially': 313,\n",
       " 'audience': 314,\n",
       " 'high': 315,\n",
       " 'seeing': 316,\n",
       " 'left': 317,\n",
       " 'everyone': 318,\n",
       " 'shows': 319,\n",
       " 'later': 320,\n",
       " 'effects': 321,\n",
       " 'seem': 322,\n",
       " 'idea': 323,\n",
       " 'excellent': 324,\n",
       " 'special': 325,\n",
       " 'said': 326,\n",
       " 'year': 327,\n",
       " 'watched': 328,\n",
       " 'takes': 329,\n",
       " 'read': 330,\n",
       " 'himself': 331,\n",
       " 'nice': 332,\n",
       " 'house': 333,\n",
       " 'hollywood': 334,\n",
       " 'black': 335,\n",
       " 'shot': 336,\n",
       " 'night': 337,\n",
       " 'worth': 338,\n",
       " 'poor': 339,\n",
       " 'john': 340,\n",
       " 'else': 341,\n",
       " 'second': 342,\n",
       " 'simply': 343,\n",
       " 'completely': 344,\n",
       " 'american': 345,\n",
       " 'dead': 346,\n",
       " 'men': 347,\n",
       " 'used': 348,\n",
       " 'need': 349,\n",
       " 'short': 350,\n",
       " 'truly': 351,\n",
       " 'mind': 352,\n",
       " 'try': 353,\n",
       " 'performances': 354,\n",
       " 'death': 355,\n",
       " 'rest': 356,\n",
       " 'less': 357,\n",
       " 'help': 358,\n",
       " 'camera': 359,\n",
       " 'full': 360,\n",
       " 'boring': 361,\n",
       " 'either': 362,\n",
       " 'women': 363,\n",
       " 'star': 364,\n",
       " 'start': 365,\n",
       " 'remember': 366,\n",
       " 'production': 367,\n",
       " 'kids': 368,\n",
       " 'along': 369,\n",
       " 'understand': 370,\n",
       " 'friends': 371,\n",
       " 'given': 372,\n",
       " 'home': 373,\n",
       " 'face': 374,\n",
       " 'top': 375,\n",
       " 'wrong': 376,\n",
       " 'classic': 377,\n",
       " 'use': 378,\n",
       " 'sex': 379,\n",
       " 'couple': 380,\n",
       " 'perhaps': 381,\n",
       " 'lines': 382,\n",
       " 'line': 383,\n",
       " 'moments': 384,\n",
       " 'name': 385,\n",
       " 'enjoy': 386,\n",
       " 'human': 387,\n",
       " 'often': 388,\n",
       " 'fan': 389,\n",
       " 'perfect': 390,\n",
       " 'came': 391,\n",
       " 'low': 392,\n",
       " 'until': 393,\n",
       " 'small': 394,\n",
       " 'tell': 395,\n",
       " 'against': 396,\n",
       " '\\x96': 397,\n",
       " 'mother': 398,\n",
       " 'terrible': 399,\n",
       " 'stupid': 400,\n",
       " '(and': 401,\n",
       " 'recommend': 402,\n",
       " 'doing': 403,\n",
       " 'playing': 404,\n",
       " 'mean': 405,\n",
       " '3': 406,\n",
       " 'dialogue': 407,\n",
       " 'video': 408,\n",
       " 'person': 409,\n",
       " '(': 410,\n",
       " 'heart': 411,\n",
       " 'wonderful': 412,\n",
       " 'next': 413,\n",
       " 'awful': 414,\n",
       " 'gives': 415,\n",
       " '(the': 416,\n",
       " '/10': 417,\n",
       " 'sort': 418,\n",
       " 'early': 419,\n",
       " 'waste': 420,\n",
       " 'definitely': 421,\n",
       " 'head': 422,\n",
       " 'itself': 423,\n",
       " 'drama': 424,\n",
       " 'picture': 425,\n",
       " '1': 426,\n",
       " 'keep': 427,\n",
       " 'stars': 428,\n",
       " 'couldn': 429,\n",
       " 'school': 430,\n",
       " 'lost': 431,\n",
       " 'getting': 432,\n",
       " 'felt': 433,\n",
       " 'title': 434,\n",
       " 'style': 435,\n",
       " 'won': 436,\n",
       " 'already': 437,\n",
       " 'piece': 438,\n",
       " 'others': 439,\n",
       " 'supposed': 440,\n",
       " 'worse': 441,\n",
       " 'finally': 442,\n",
       " 'become': 443,\n",
       " 'direction': 444,\n",
       " 'becomes': 445,\n",
       " 'loved': 446,\n",
       " 'absolutely': 447,\n",
       " 'boy': 448,\n",
       " 'episode': 449,\n",
       " 'beginning': 450,\n",
       " 'certainly': 451,\n",
       " 'written': 452,\n",
       " 'hope': 453,\n",
       " 'liked': 454,\n",
       " 'behind': 455,\n",
       " 'fine': 456,\n",
       " 'history': 457,\n",
       " 'totally': 458,\n",
       " 'case': 459,\n",
       " 'care': 460,\n",
       " 'mr': 461,\n",
       " 'went': 462,\n",
       " 'fans': 463,\n",
       " 'overall': 464,\n",
       " 'entire': 465,\n",
       " 'example': 466,\n",
       " 'children': 467,\n",
       " 'laugh': 468,\n",
       " 'live': 469,\n",
       " 'budget': 470,\n",
       " 'dark': 471,\n",
       " 'child': 472,\n",
       " 'expect': 473,\n",
       " 'problem': 474,\n",
       " 'turn': 475,\n",
       " 'killer': 476,\n",
       " 'michael': 477,\n",
       " 'friend': 478,\n",
       " 'several': 479,\n",
       " 'flick': 480,\n",
       " 'sound': 481,\n",
       " 'amazing': 482,\n",
       " 'white': 483,\n",
       " 'car': 484,\n",
       " 'entertaining': 485,\n",
       " 'despite': 486,\n",
       " 'cinema': 487,\n",
       " 'son': 488,\n",
       " 'lead': 489,\n",
       " 'actress': 490,\n",
       " 'final': 491,\n",
       " 'city': 492,\n",
       " 'self': 493,\n",
       " 'days': 494,\n",
       " 'wants': 495,\n",
       " 'works': 496,\n",
       " 'yes': 497,\n",
       " 'turns': 498,\n",
       " 'themselves': 499,\n",
       " 'game': 500,\n",
       " 'lives': 501,\n",
       " 'throughout': 502,\n",
       " 'evil': 503,\n",
       " 'wanted': 504,\n",
       " 'close': 505,\n",
       " 'guess': 506,\n",
       " 'past': 507,\n",
       " 'soon': 508,\n",
       " 'favorite': 509,\n",
       " 'unfortunately': 510,\n",
       " 'feeling': 511,\n",
       " 'humor': 512,\n",
       " 'quality': 513,\n",
       " 'based': 514,\n",
       " 'voice': 515,\n",
       " 'tries': 516,\n",
       " 'today': 517,\n",
       " 'writing': 518,\n",
       " 'able': 519,\n",
       " 'daughter': 520,\n",
       " 'viewer': 521,\n",
       " 'under': 522,\n",
       " 'guys': 523,\n",
       " 'side': 524,\n",
       " 'oh': 525,\n",
       " 'girls': 526,\n",
       " 'enjoyed': 527,\n",
       " 'called': 528,\n",
       " 'starts': 529,\n",
       " 'lack': 530,\n",
       " 'horrible': 531,\n",
       " 'parts': 532,\n",
       " 'hand': 533,\n",
       " 'wouldn': 534,\n",
       " 'late': 535,\n",
       " 'seemed': 536,\n",
       " 'gave': 537,\n",
       " 'sometimes': 538,\n",
       " 'thinking': 539,\n",
       " 'hour': 540,\n",
       " 'kid': 541,\n",
       " 'kill': 542,\n",
       " 'decent': 543,\n",
       " 'happens': 544,\n",
       " 'moment': 545,\n",
       " 'genre': 546,\n",
       " 'art': 547,\n",
       " 'matter': 548,\n",
       " 'run': 549,\n",
       " 'strong': 550,\n",
       " 'blood': 551,\n",
       " '4': 552,\n",
       " 'directed': 553,\n",
       " 'hit': 554,\n",
       " 'save': 555,\n",
       " 'stories': 556,\n",
       " 'took': 557,\n",
       " 'town': 558,\n",
       " 'leave': 559,\n",
       " 'husband': 560,\n",
       " 'god': 561,\n",
       " 'violence': 562,\n",
       " 'crap': 563,\n",
       " 'eyes': 564,\n",
       " 'extremely': 565,\n",
       " 'heard': 566,\n",
       " 'experience': 567,\n",
       " 'anyway': 568,\n",
       " 'ago': 569,\n",
       " 'says': 570,\n",
       " 'slow': 571,\n",
       " 'brother': 572,\n",
       " 'number': 573,\n",
       " 'power': 574,\n",
       " '(i': 575,\n",
       " 'murder': 576,\n",
       " 'hours': 577,\n",
       " 'brilliant': 578,\n",
       " 'involved': 579,\n",
       " 'stuff': 580,\n",
       " 'happened': 581,\n",
       " 'writer': 582,\n",
       " 'ridiculous': 583,\n",
       " 'obviously': 584,\n",
       " 'exactly': 585,\n",
       " 'killed': 586,\n",
       " 'whose': 587,\n",
       " 'musical': 588,\n",
       " '5': 589,\n",
       " 'hilarious': 590,\n",
       " 'coming': 591,\n",
       " 'etc': 592,\n",
       " 'highly': 593,\n",
       " 'james': 594,\n",
       " 'roles': 595,\n",
       " 'scary': 596,\n",
       " 'complete': 597,\n",
       " 'myself': 598,\n",
       " 'chance': 599,\n",
       " 'gore': 600,\n",
       " 'paul': 601,\n",
       " 'career': 602,\n",
       " 'released': 603,\n",
       " 'cannot': 604,\n",
       " 'score': 605,\n",
       " 'b': 606,\n",
       " 'hell': 607,\n",
       " 'annoying': 608,\n",
       " 'jack': 609,\n",
       " 'usual': 610,\n",
       " 'attempt': 611,\n",
       " 'act': 612,\n",
       " 'change': 613,\n",
       " 'obvious': 614,\n",
       " 'seriously': 615,\n",
       " 'lady': 616,\n",
       " 'taken': 617,\n",
       " 'serious': 618,\n",
       " 'across': 619,\n",
       " 'due': 620,\n",
       " 'country': 621,\n",
       " 'living': 622,\n",
       " 'alone': 623,\n",
       " 'attention': 624,\n",
       " 'supporting': 625,\n",
       " 'wonder': 626,\n",
       " 'particularly': 627,\n",
       " 'police': 628,\n",
       " 'saying': 629,\n",
       " 'group': 630,\n",
       " 'happen': 631,\n",
       " 'strange': 632,\n",
       " 'ends': 633,\n",
       " 'opening': 634,\n",
       " 'age': 635,\n",
       " 'beyond': 636,\n",
       " 'except': 637,\n",
       " 'tells': 638,\n",
       " 'female': 639,\n",
       " 'interest': 640,\n",
       " 'lee': 641,\n",
       " 'relationship': 642,\n",
       " 'cinematography': 643,\n",
       " 'light': 644,\n",
       " 'song': 645,\n",
       " 'order': 646,\n",
       " 'king': 647,\n",
       " 'level': 648,\n",
       " 'type': 649,\n",
       " 'running': 650,\n",
       " 'started': 651,\n",
       " 'please': 652,\n",
       " 'opinion': 653,\n",
       " 'shown': 654,\n",
       " 'wish': 655,\n",
       " 'fight': 656,\n",
       " 'jokes': 657,\n",
       " 'cut': 658,\n",
       " 'somewhat': 659,\n",
       " 'sad': 660,\n",
       " 'simple': 661,\n",
       " 'stop': 662,\n",
       " 'none': 663,\n",
       " 'york': 664,\n",
       " 'happy': 665,\n",
       " 'hero': 666,\n",
       " 'huge': 667,\n",
       " 'looked': 668,\n",
       " 'usually': 669,\n",
       " 'word': 670,\n",
       " 'told': 671,\n",
       " 'eye': 672,\n",
       " 'possible': 673,\n",
       " 'cool': 674,\n",
       " 'call': 675,\n",
       " 'novel': 676,\n",
       " 'reality': 677,\n",
       " 'body': 678,\n",
       " 'british': 679,\n",
       " 'talent': 680,\n",
       " 'ok': 681,\n",
       " 'entertainment': 682,\n",
       " 'four': 683,\n",
       " 'rating': 684,\n",
       " 'knows': 685,\n",
       " 'falls': 686,\n",
       " 'enjoyable': 687,\n",
       " 'yourself': 688,\n",
       " 'fast': 689,\n",
       " 'comic': 690,\n",
       " 'knew': 691,\n",
       " 'words': 692,\n",
       " 'basically': 693,\n",
       " 'known': 694,\n",
       " '-up': 695,\n",
       " 'finds': 696,\n",
       " 'easy': 697,\n",
       " 'disappointed': 698,\n",
       " 'cheap': 699,\n",
       " 'mostly': 700,\n",
       " 'theater': 701,\n",
       " 'taking': 702,\n",
       " 'important': 703,\n",
       " 'earth': 704,\n",
       " 'whether': 705,\n",
       " 'upon': 706,\n",
       " 'above': 707,\n",
       " 'problems': 708,\n",
       " 'effort': 709,\n",
       " 'predictable': 710,\n",
       " 'shots': 711,\n",
       " 'haven': 712,\n",
       " 'apparently': 713,\n",
       " 'single': 714,\n",
       " 'events': 715,\n",
       " 'miss': 716,\n",
       " 'begins': 717,\n",
       " 'sequence': 718,\n",
       " 'message': 719,\n",
       " 'feels': 720,\n",
       " 'animation': 721,\n",
       " 'clearly': 722,\n",
       " '.the': 723,\n",
       " 'appears': 724,\n",
       " 'english': 725,\n",
       " 'brought': 726,\n",
       " 'view': 727,\n",
       " 'bring': 728,\n",
       " 'future': 729,\n",
       " 'turned': 730,\n",
       " 'typical': 731,\n",
       " 'oscar': 732,\n",
       " 'moving': 733,\n",
       " 'talk': 734,\n",
       " 'similar': 735,\n",
       " 'mention': 736,\n",
       " 'disney': 737,\n",
       " 'room': 738,\n",
       " 'george': 739,\n",
       " 'bunch': 740,\n",
       " 'add': 741,\n",
       " 'episodes': 742,\n",
       " 'team': 743,\n",
       " 'material': 744,\n",
       " 'nearly': 745,\n",
       " 'sets': 746,\n",
       " 'local': 747,\n",
       " 'giving': 748,\n",
       " 'move': 749,\n",
       " 'five': 750,\n",
       " 'straight': 751,\n",
       " 'sister': 752,\n",
       " 'hate': 753,\n",
       " 're': 754,\n",
       " 'near': 755,\n",
       " 'rock': 756,\n",
       " 'working': 757,\n",
       " 'ones': 758,\n",
       " 'romantic': 759,\n",
       " 'needs': 760,\n",
       " 'sequel': 761,\n",
       " 'using': 762,\n",
       " 'non': 763,\n",
       " 'actual': 764,\n",
       " 'modern': 765,\n",
       " 'talking': 766,\n",
       " 'television': 767,\n",
       " 'certain': 768,\n",
       " 'documentary': 769,\n",
       " 'within': 770,\n",
       " 'david': 771,\n",
       " 'easily': 772,\n",
       " 'songs': 773,\n",
       " 'famous': 774,\n",
       " 'expected': 775,\n",
       " 'middle': 776,\n",
       " '-the': 777,\n",
       " 'stand': 778,\n",
       " 'surprised': 779,\n",
       " 'ways': 780,\n",
       " 'mess': 781,\n",
       " 'ten': 782,\n",
       " 'dialog': 783,\n",
       " 'named': 784,\n",
       " 'doubt': 785,\n",
       " 'showing': 786,\n",
       " 'leads': 787,\n",
       " 'difficult': 788,\n",
       " '7': 789,\n",
       " 'average': 790,\n",
       " 'subject': 791,\n",
       " 'robert': 792,\n",
       " 'comments': 793,\n",
       " 'decided': 794,\n",
       " 'dull': 795,\n",
       " 'silly': 796,\n",
       " 'french': 797,\n",
       " 'sit': 798,\n",
       " 'form': 799,\n",
       " 'keeps': 800,\n",
       " 'street': 801,\n",
       " 'class': 802,\n",
       " '9': 803,\n",
       " 'romance': 804,\n",
       " '\"i': 805,\n",
       " 'avoid': 806,\n",
       " 'feature': 807,\n",
       " 'means': 808,\n",
       " 'mystery': 809,\n",
       " '8': 810,\n",
       " 'follow': 811,\n",
       " 'japanese': 812,\n",
       " 'older': 813,\n",
       " 'aren': 814,\n",
       " 'stay': 815,\n",
       " 'sequences': 816,\n",
       " 'filmed': 817,\n",
       " 'somehow': 818,\n",
       " 'hear': 819,\n",
       " 'lots': 820,\n",
       " 'points': 821,\n",
       " '(which': 822,\n",
       " 'believable': 823,\n",
       " 'poorly': 824,\n",
       " 'review': 825,\n",
       " 'nature': 826,\n",
       " '.i': 827,\n",
       " 'major': 828,\n",
       " 'elements': 829,\n",
       " 'space': 830,\n",
       " 'clear': 831,\n",
       " 'tried': 832,\n",
       " 'question': 833,\n",
       " 'among': 834,\n",
       " 'richard': 835,\n",
       " 'crime': 836,\n",
       " 'including': 837,\n",
       " 'footage': 838,\n",
       " 'fall': 839,\n",
       " 'thriller': 840,\n",
       " 'suspense': 841,\n",
       " 'general': 842,\n",
       " 'herself': 843,\n",
       " 'stage': 844,\n",
       " 'emotional': 845,\n",
       " '20': 846,\n",
       " 'deep': 847,\n",
       " 'storyline': 848,\n",
       " 'sorry': 849,\n",
       " '(as': 850,\n",
       " 'free': 851,\n",
       " 'soundtrack': 852,\n",
       " 'comment': 853,\n",
       " 'success': 854,\n",
       " 'theme': 855,\n",
       " 'red': 856,\n",
       " 'greatest': 857,\n",
       " 'fantastic': 858,\n",
       " 'reviews': 859,\n",
       " 'viewers': 860,\n",
       " 'dance': 861,\n",
       " 'particular': 862,\n",
       " 'baby': 863,\n",
       " 'filmmakers': 864,\n",
       " 'atmosphere': 865,\n",
       " 'figure': 866,\n",
       " 'screenplay': 867,\n",
       " 'release': 868,\n",
       " 'credits': 869,\n",
       " 'total': 870,\n",
       " 'brothers': 871,\n",
       " 'reading': 872,\n",
       " 'potential': 873,\n",
       " 'powerful': 874,\n",
       " 'appear': 875,\n",
       " 'remake': 876,\n",
       " 'laughs': 877,\n",
       " 'boys': 878,\n",
       " '(or': 879,\n",
       " 'situation': 880,\n",
       " 'gone': 881,\n",
       " 'superb': 882,\n",
       " 'indeed': 883,\n",
       " 'weak': 884,\n",
       " 'whatever': 885,\n",
       " 'period': 886,\n",
       " 'sci': 887,\n",
       " 'imagine': 888,\n",
       " 'hardly': 889,\n",
       " 'earlier': 890,\n",
       " 'perfectly': 891,\n",
       " 'parents': 892,\n",
       " 'viewing': 893,\n",
       " 'okay': 894,\n",
       " 'background': 895,\n",
       " 'tale': 896,\n",
       " 'otherwise': 897,\n",
       " 'mark': 898,\n",
       " 'note': 899,\n",
       " 'kept': 900,\n",
       " 'wait': 901,\n",
       " 'leaves': 902,\n",
       " 'meets': 903,\n",
       " 'possibly': 904,\n",
       " 'minute': 905,\n",
       " 'setting': 906,\n",
       " 'monster': 907,\n",
       " 'dumb': 908,\n",
       " 'meant': 909,\n",
       " 'o': 910,\n",
       " 'crazy': 911,\n",
       " 'forget': 912,\n",
       " 'nor': 913,\n",
       " 'die': 914,\n",
       " 'third': 915,\n",
       " 'public': 916,\n",
       " 'society': 917,\n",
       " 'realize': 918,\n",
       " 'whom': 919,\n",
       " 'buy': 920,\n",
       " 'write': 921,\n",
       " 'present': 922,\n",
       " 'deal': 923,\n",
       " 'towards': 924,\n",
       " 'directors': 925,\n",
       " 'pay': 926,\n",
       " 'expecting': 927,\n",
       " 'became': 928,\n",
       " 'open': 929,\n",
       " 'missing': 930,\n",
       " 'fantasy': 931,\n",
       " 'check': 932,\n",
       " 'premise': 933,\n",
       " '-fi': 934,\n",
       " 'result': 935,\n",
       " 'business': 936,\n",
       " 'male': 937,\n",
       " 'de': 938,\n",
       " 'needed': 939,\n",
       " 'rent': 940,\n",
       " 'truth': 941,\n",
       " 'imdb': 942,\n",
       " 'killing': 943,\n",
       " 'eventually': 944,\n",
       " 'development': 945,\n",
       " 'america': 946,\n",
       " 'ben': 947,\n",
       " 'christmas': 948,\n",
       " 'learn': 949,\n",
       " 'interested': 950,\n",
       " 'brings': 951,\n",
       " 'zombie': 952,\n",
       " 'revenge': 953,\n",
       " 'joe': 954,\n",
       " 'wrote': 955,\n",
       " 'writers': 956,\n",
       " 'casting': 957,\n",
       " 'forced': 958,\n",
       " 'surprise': 959,\n",
       " 'battle': 960,\n",
       " 'meet': 961,\n",
       " 'fails': 962,\n",
       " 'creepy': 963,\n",
       " 'stewart': 964,\n",
       " 'odd': 965,\n",
       " 'memorable': 966,\n",
       " 'further': 967,\n",
       " 'badly': 968,\n",
       " 'cheesy': 969,\n",
       " 'talented': 970,\n",
       " 'apart': 971,\n",
       " 'wasted': 972,\n",
       " 'decides': 973,\n",
       " 'shame': 974,\n",
       " 'tony': 975,\n",
       " 'peter': 976,\n",
       " 'credit': 977,\n",
       " 'directing': 978,\n",
       " 'plus': 979,\n",
       " 'william': 980,\n",
       " 'begin': 981,\n",
       " 'unique': 982,\n",
       " 'fire': 983,\n",
       " 'sexual': 984,\n",
       " 'sounds': 985,\n",
       " 'villain': 986,\n",
       " 'spirit': 987,\n",
       " 'inside': 988,\n",
       " 'attempts': 989,\n",
       " 'twist': 990,\n",
       " '(a': 991,\n",
       " 'box': 992,\n",
       " 'sees': 993,\n",
       " 'hands': 994,\n",
       " 'reasons': 995,\n",
       " 'following': 996,\n",
       " 'bored': 997,\n",
       " 'waiting': 998,\n",
       " '80': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "engaging-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "endless-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input : list, output : list\n",
    "max_seq_len = 80\n",
    "\n",
    "def encode_tokens(tokens, max_seq_len=max_seq_len):\n",
    "    \n",
    "    encoded = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in list(token_to_id.keys()):\n",
    "            encoded.append(token_to_id[token])\n",
    "        else:\n",
    "            encoded.append(token_to_id['[UNK]']) # unknown token\n",
    "    # padding\n",
    "    if len(tokens) < max_seq_len:\n",
    "        encoded = encoded + [0] * (max_seq_len - len(tokens))\n",
    "    # truncate\n",
    "    elif len(tokens) >= max_seq_len:\n",
    "        encoded = encoded[:max_seq_len]\n",
    "        \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "secure-helicopter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[181,\n",
       " 158,\n",
       " 5,\n",
       " 178,\n",
       " 24,\n",
       " 819,\n",
       " 2081,\n",
       " 0,\n",
       " 15737,\n",
       " 19418,\n",
       " 8,\n",
       " 15738,\n",
       " 107,\n",
       " 14,\n",
       " 2,\n",
       " 6087,\n",
       " 57,\n",
       " 25,\n",
       " 778,\n",
       " 52,\n",
       " 120,\n",
       " 328,\n",
       " 36,\n",
       " 2,\n",
       " 102,\n",
       " 146,\n",
       " 3,\n",
       " 13,\n",
       " 9,\n",
       " 2,\n",
       " 20,\n",
       " 14,\n",
       " 12,\n",
       " 110,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 0,\n",
       " 320,\n",
       " 3,\n",
       " 10,\n",
       " 15,\n",
       " 204,\n",
       " 155,\n",
       " 8,\n",
       " 90,\n",
       " 18,\n",
       " 6,\n",
       " 296,\n",
       " 38,\n",
       " 1475,\n",
       " 3,\n",
       " 28,\n",
       " 251,\n",
       " 4,\n",
       " 21,\n",
       " 12,\n",
       " 110,\n",
       " 12,\n",
       " 0,\n",
       " 26,\n",
       " 13,\n",
       " 20,\n",
       " 54,\n",
       " 846,\n",
       " 965,\n",
       " 237,\n",
       " 85,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 15739,\n",
       " 12,\n",
       " 7341,\n",
       " 14,\n",
       " 66,\n",
       " 11653,\n",
       " 534,\n",
       " 25,\n",
       " 7899,\n",
       " 499]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_tokens(corpus[0], 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-resort",
   "metadata": {},
   "source": [
    "* `max_seq_len + 1` 만큼 encoding 하는 이유\n",
    "* time step 1씩 차이가 나도록\n",
    "\n",
    "```\n",
    "tokenized_sentences=vectorize_layer(tf.expand_dims('text is good',-1))\n",
    "x = tokenized_sentences[:, :-1]\n",
    "y = tokenized_sentences[:, 1:]\n",
    "\n",
    ">> tokenized_sentences\n",
    "... <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 1, 0, 0]])>\n",
    "\n",
    ">> x\n",
    "... <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[1, 1, 1, 0]])>\n",
    "\n",
    ">> y\n",
    "... <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[1, 1, 0, 0]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "flying-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n",
      "train_text shape : (5000, 81)\n"
     ]
    }
   ],
   "source": [
    "train_text=[]\n",
    "for i in range(len(corpus)):\n",
    "    if (i+1) % (len(corpus)/10) == 0:\n",
    "        print(f'{(i+1)/(len(corpus)/100)} % Done')\n",
    "    train_text.append(encode_tokens(corpus[i], max_seq_len = max_seq_len + 1))  # encode reviews with vectorizer\n",
    "train_text = np.array(train_text)\n",
    "print(\"train_text shape :\", train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "missing-margin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('datasets/pickle_data/train_text_tg_gpt.pickle', 'wb') as f:\n",
    "    pickle.dump(train_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "historic-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('datasets/pickle_data/train_text_tg_gpt.pickle', 'rb') as f:\n",
    "    train_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wrong-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape : (5000, 80)\n",
      "train_y shape : (5000, 80)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_text[:, :-1]\n",
    "train_y = train_text[:, 1:]\n",
    "\n",
    "print('train_x shape :', train_x.shape)\n",
    "print('train_y shape :', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-proof",
   "metadata": {},
   "source": [
    "* position_(i) 에 해당하는 input token의 target이 position_(i+1)의 token이 되도록 input target 구성\n",
    "\n",
    "```Python\n",
    "# from keras\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "requested-fighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    6,   267,     0,    11,  1431,     9,  6419,   139,    45,\n",
       "         2414,    38,     6,  7476,  3134, 14267,  1764,     5,   508,\n",
       "          696,   843,  1035,     5,   622,    18,    88,    37,    30,\n",
       "         9902,    11,   679, 16935,     3,   300,   514,   706,     2,\n",
       "          262,    38,   792,     0,     4,    13,  2989,   295,   695,\n",
       "            9,   593,  2918,     7,     0,    15,     0,    27,     4,\n",
       "           18,  1061,   616,  2315,  1559, 14557,    18,     2, 12080,\n",
       "         2488,     7,  3288,    37,     2,  2594,     5,  8852,  1281,\n",
       "           23,    45,   196,   560,     4,    41,     9,   141]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cleared-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  267,     0,    11,  1431,     9,  6419,   139,    45,  2414,\n",
       "           38,     6,  7476,  3134, 14267,  1764,     5,   508,   696,\n",
       "          843,  1035,     5,   622,    18,    88,    37,    30,  9902,\n",
       "           11,   679, 16935,     3,   300,   514,   706,     2,   262,\n",
       "           38,   792,     0,     4,    13,  2989,   295,   695,     9,\n",
       "          593,  2918,     7,     0,    15,     0,    27,     4,    18,\n",
       "         1061,   616,  2315,  1559, 14557,    18,     2, 12080,  2488,\n",
       "            7,  3288,    37,     2,  2594,     5,  8852,  1281,    23,\n",
       "           45,   196,   560,     4,    41,     9,   141,   522]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dangerous-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextGenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_tokens, target_tokens):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.target_tokens = target_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inputs = self.input_tokens[idx]\n",
    "        targets = self.target_tokens[idx]\n",
    "        \n",
    "        output = {\"inputs\": inputs,\n",
    "                  \"targets\": targets}\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "geographic-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = TextGenDataset(train_x, train_y)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "stuffed-helen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': tensor([[ 381,   34,    7,  ...,    3,   96,   10],\n",
      "        [ 167,   11,   13,  ...,    0,    0,    0],\n",
      "        [1521,  116,  361,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [9401, 4259,    4,  ...,  867,    8,    0],\n",
      "        [  12,  304,  566,  ...,   11,  335,    6],\n",
      "        [   2,  692,    0,  ...,  245,    8,   87]]), 'targets': tensor([[   34,     7,     2,  ...,    96,    10, 10373],\n",
      "        [   11,    13,    22,  ...,     0,     0,     0],\n",
      "        [  116,   361,     4,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 4259,     4,   103,  ...,     8,     0,    64],\n",
      "        [  304,   566,    49,  ...,   335,     6,   173],\n",
      "        [  692,     0,    27,  ...,     8,    87,    10]])}\n"
     ]
    }
   ],
   "source": [
    "print(iter(train_loader).next())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-ensemble",
   "metadata": {},
   "source": [
    "### Implement an embedding layer\n",
    "\n",
    "Create two seperate embedding layers: one for tokens and one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "similar-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embed_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param max_len: max length of seqeunce\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_len, embedding_dim=embed_size)\n",
    "\n",
    "    def forward(self, sequence, device): # [128, 80]\n",
    "\n",
    "        max_seq_len = sequence.shape[-1]\n",
    "\n",
    "        positions = torch.tensor(range(max_seq_len)).to(device)\n",
    "        positions = self.pos_emb(positions)\n",
    "        emb_out = self.token_emb(sequence) + positions # torch.Size([128, 80, 256])\n",
    "        return emb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-annual",
   "metadata": {},
   "source": [
    "### Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "durable-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SamLynnEvans/Transformer/tree/e06ae2810f119c75aa34585442872026875e6462\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "                \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "       # calculate attention using function we will define next\n",
    "        scores = self.attention(q, k, v, self.d_k, mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.0):\n",
    "        super().__init__() \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-handy",
   "metadata": {},
   "source": [
    "#### Decoder의 Masked Self-attention 을 위한 Causal mask 생성\n",
    "https://machinereads.wordpress.com/2020/05/10/xlnet-generalized-autoregressive-pretraining-for-language-understanding-1-3/\n",
    "\n",
    "**질문?** query, key의 길이가 달라지는 경우가 있는지?\n",
    "- Keras 구현에 의하면, query, key의 길이를 각각 받아서 mask 생성\n",
    "- Self-attention이면 query 길이 하나만 받아도 되지 않나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sufficient-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#  Keras Implementation\n",
    "#####\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    \n",
    "    return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "genetic-penguin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 5), dtype=bool, numpy=\n",
       "array([[[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]],\n",
       "\n",
       "       [[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked self attention\n",
    "\n",
    "causal_mask = causal_attention_mask(batch_size=2, n_dest=5, n_src=5, dtype=tf.bool)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dying-theme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 5), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,3,5, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unnecessary-sensitivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 3), dtype=bool, numpy=\n",
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]]])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,5,3, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "secret-cartridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 5), dtype=bool, numpy=\n",
       "array([[[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,5,5, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "utility-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#  Simple Implementation\n",
    "#####\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.tril.html\n",
    "\n",
    "def simple_causal_attention_mask(batch_size, n_dest, n_src):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "#     print(np.ones((batch_size, n_dest, n_src)))\n",
    "#     print(np.tril(np.ones((batch_size, n_dest, n_src))))\n",
    "    mask = np.tril(np.ones((batch_size, n_dest, n_src)), n_src-n_dest)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "chief-detail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-5=-2, 음수 값 만큼 내려가서 diagonal 시작\n",
    "simple_causal_attention_mask(1,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "refined-gospel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-3=2, 양수 값 만큼 올라간 후 diagonal 시작\n",
    "simple_causal_attention_mask(1,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "binding-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! No Encoder-decoder attention\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, heads, d_ff, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, batch_size, x, device, mask=None):\n",
    "        \n",
    "        causal_mask = simple_causal_attention_mask(batch_size, self.seq_len, self.seq_len)\n",
    "        causal_mask = torch.tensor(np.array(causal_mask)).to(device)\n",
    "        \n",
    "        # self attention + add&norm\n",
    "        attention_output = self.attn(x, x, x, mask=causal_mask) # Masked self attention\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        masked_self_att_out = self.norm_1(x + attention_output)\n",
    "\n",
    "        # ffn + add&norm\n",
    "        ffn_output = self.ff(masked_self_att_out)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        out = self.norm_2(masked_self_att_out + ffn_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-bangkok",
   "metadata": {},
   "source": [
    "### Implement the miniature GPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "superior-vertex",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, device, vocab_size, max_seq_len, d_model, heads, d_ff):\n",
    "        super(MiniGPT, self).__init__()\n",
    "        self.device = device\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size=vocab_size, max_len=max_seq_len, embed_size=d_model)\n",
    "        self.transformer_block = TransformerDecoderLayer(seq_len=max_seq_len, d_model=d_model, heads=heads, d_ff=d_ff, dropout=0.1)\n",
    "        self.ff = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, batch_size, inputs):\n",
    "        x = self.token_pos_embedding(inputs, self.device) \n",
    "        x = self.transformer_block(batch_size, x, self.device)\n",
    "        x = self.ff(x) # torch.Size([batch_size, max_seq_len, vocab_size])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-debate",
   "metadata": {},
   "source": [
    "### Implement a callback for generating text -> PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "known-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : reimp\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, device, max_seq_len, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        \n",
    "    # random choince from top_k predicted tokens\n",
    "    # logits : (20000,)\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = self.max_seq_len - len(start_tokens)\n",
    "            # current token index\n",
    "            current_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:self.max_seq_len]\n",
    "                current_index = self.max_seq_len - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = torch.tensor([x]).to(self.device)\n",
    "            y = self.model(x.shape[0], x)\n",
    "            y = y.to('cpu').detach().numpy() # torch.Size([128, 80, 20000])\n",
    "            \n",
    "            sample_token = self.sample_from(y[0][current_index]) # token id\n",
    "            \n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            \n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-shuttle",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "reasonable-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device : cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "max_seq_len = 80  # Max sequence size\n",
    "d_model = 256  # Embedding size for each token\n",
    "heads = 2  # Number of attention heads\n",
    "d_ff = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "# https://githubmemory.com/repo/arogozhnikov/einops/issues\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Current device :', device)\n",
    "model = MiniGPT(device=device, vocab_size=vocab_size, max_seq_len=max_seq_len, d_model=d_model, heads=heads, d_ff=d_ff)\n",
    "\n",
    "learning_rate = 0.001 \n",
    "\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "controversial-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eleven-providence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6996892f34544af6a0208e4227aadba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [4/40], Loss: 9.3989\n",
      "Epoch [1/25], Step [9/40], Loss: 7.7562\n",
      "Epoch [1/25], Step [14/40], Loss: 6.8931\n",
      "Epoch [1/25], Step [19/40], Loss: 6.6223\n",
      "Epoch [1/25], Step [24/40], Loss: 6.6605\n",
      "Epoch [1/25], Step [29/40], Loss: 6.6743\n",
      "Epoch [1/25], Step [34/40], Loss: 6.6496\n",
      "Epoch [1/25], Step [39/40], Loss: 6.5476\n",
      ">> Epoch 1\n",
      "generated text:\n",
      "this movie is . the of a . it is the . and and it the most the is the movie , , and , to that , that the movie , the first . and the . . , a this and of\n",
      "\n",
      "None\n",
      "Epoch [2/25], Step [4/40], Loss: 6.5672\n",
      "Epoch [2/25], Step [9/40], Loss: 6.4747\n",
      "Epoch [2/25], Step [14/40], Loss: 6.5128\n",
      "Epoch [2/25], Step [19/40], Loss: 6.5029\n",
      "Epoch [2/25], Step [24/40], Loss: 6.4911\n",
      "Epoch [2/25], Step [29/40], Loss: 6.5543\n",
      "Epoch [2/25], Step [34/40], Loss: 6.4964\n",
      "Epoch [2/25], Step [39/40], Loss: 6.1367\n",
      ">> Epoch 2\n",
      "generated text:\n",
      "this movie is this , . and and of the first is , . the plot , a movie . it , of the plot a great , and i was a film . it . the first the film of to of the\n",
      "\n",
      "None\n",
      "Epoch [3/25], Step [4/40], Loss: 6.3736\n",
      "Epoch [3/25], Step [9/40], Loss: 6.3249\n",
      "Epoch [3/25], Step [14/40], Loss: 6.2457\n",
      "Epoch [3/25], Step [19/40], Loss: 6.2741\n",
      "Epoch [3/25], Step [24/40], Loss: 6.1804\n",
      "Epoch [3/25], Step [29/40], Loss: 6.1265\n",
      "Epoch [3/25], Step [34/40], Loss: 6.0845\n",
      "Epoch [3/25], Step [39/40], Loss: 6.3610\n",
      ">> Epoch 3\n",
      "generated text:\n",
      "this movie is this movie of the movie . a film . it is a little to see the first of the best that the first , it is in a very is in a film is an of the same , they is\n",
      "\n",
      "None\n",
      "Epoch [4/25], Step [4/40], Loss: 5.9865\n",
      "Epoch [4/25], Step [9/40], Loss: 6.0523\n",
      "Epoch [4/25], Step [14/40], Loss: 6.0167\n",
      "Epoch [4/25], Step [19/40], Loss: 5.9133\n",
      "Epoch [4/25], Step [24/40], Loss: 5.8386\n",
      "Epoch [4/25], Step [29/40], Loss: 5.9035\n",
      "Epoch [4/25], Step [34/40], Loss: 5.9678\n",
      "Epoch [4/25], Step [39/40], Loss: 5.7480\n",
      ">> Epoch 4\n",
      "generated text:\n",
      "this movie is a film . the acting , but this and then a little in a movie . it 's all to watch a very good . the movie , i would be to do the acting is an to be to see\n",
      "\n",
      "None\n",
      "Epoch [5/25], Step [4/40], Loss: 5.7537\n",
      "Epoch [5/25], Step [9/40], Loss: 5.7819\n",
      "Epoch [5/25], Step [14/40], Loss: 5.8047\n",
      "Epoch [5/25], Step [19/40], Loss: 5.7028\n",
      "Epoch [5/25], Step [24/40], Loss: 5.5584\n",
      "Epoch [5/25], Step [29/40], Loss: 5.7449\n",
      "Epoch [5/25], Step [34/40], Loss: 5.7333\n",
      "Epoch [5/25], Step [39/40], Loss: 5.4913\n",
      ">> Epoch 5\n",
      "generated text:\n",
      "this movie is the first time , it was very good . the acting . the end . the movie and the film . it was the first of the most of his wife of the most of the acting . the movie of\n",
      "\n",
      "None\n",
      "Epoch [6/25], Step [4/40], Loss: 5.5003\n",
      "Epoch [6/25], Step [9/40], Loss: 5.4530\n",
      "Epoch [6/25], Step [14/40], Loss: 5.4857\n",
      "Epoch [6/25], Step [19/40], Loss: 5.4447\n",
      "Epoch [6/25], Step [24/40], Loss: 5.5238\n",
      "Epoch [6/25], Step [29/40], Loss: 5.4506\n",
      "Epoch [6/25], Step [34/40], Loss: 5.4117\n",
      "Epoch [6/25], Step [39/40], Loss: 5.5869\n",
      ">> Epoch 6\n",
      "generated text:\n",
      "this movie is great , one of my favorite of the film and it . i don 't get it . i was the acting . the film is just the movie is very good , but it is about to the characters that\n",
      "\n",
      "None\n",
      "Epoch [7/25], Step [4/40], Loss: 5.3643\n",
      "Epoch [7/25], Step [9/40], Loss: 5.2672\n",
      "Epoch [7/25], Step [14/40], Loss: 5.3367\n",
      "Epoch [7/25], Step [19/40], Loss: 5.2990\n",
      "Epoch [7/25], Step [24/40], Loss: 5.2421\n",
      "Epoch [7/25], Step [29/40], Loss: 5.3062\n",
      "Epoch [7/25], Step [34/40], Loss: 5.2757\n",
      "Epoch [7/25], Step [39/40], Loss: 5.3383\n",
      ">> Epoch 7\n",
      "generated text:\n",
      "this movie is so well . i was made . the most movie is not a great story line . it 's a movie about the worst . it was made a great movie that a very good and there is a film .\n",
      "\n",
      "None\n",
      "Epoch [8/25], Step [4/40], Loss: 5.1344\n",
      "Epoch [8/25], Step [9/40], Loss: 5.0689\n",
      "Epoch [8/25], Step [14/40], Loss: 5.0599\n",
      "Epoch [8/25], Step [19/40], Loss: 5.1480\n",
      "Epoch [8/25], Step [24/40], Loss: 5.0493\n",
      "Epoch [8/25], Step [29/40], Loss: 5.0887\n",
      "Epoch [8/25], Step [34/40], Loss: 5.0534\n",
      "Epoch [8/25], Step [39/40], Loss: 5.0729\n",
      ">> Epoch 8\n",
      "generated text:\n",
      "this movie is the most of the most famous movie , it 's a few of an interesting man of the same time the acting of the movie , but i was expecting it was an actor , the film . i thought this\n",
      "\n",
      "None\n",
      "Epoch [9/25], Step [4/40], Loss: 4.9416\n",
      "Epoch [9/25], Step [9/40], Loss: 4.8685\n",
      "Epoch [9/25], Step [14/40], Loss: 4.9356\n",
      "Epoch [9/25], Step [19/40], Loss: 4.9293\n",
      "Epoch [9/25], Step [24/40], Loss: 4.9345\n",
      "Epoch [9/25], Step [29/40], Loss: 4.9306\n",
      "Epoch [9/25], Step [34/40], Loss: 4.8741\n",
      "Epoch [9/25], Step [39/40], Loss: 4.9039\n",
      ">> Epoch 9\n",
      "generated text:\n",
      "this movie is a terrible film of my friends in the only reason i saw it . it is that the plot is that i found myself not be seen a great cast . what they are a film about this is one as\n",
      "\n",
      "None\n",
      "Epoch [10/25], Step [4/40], Loss: 4.7783\n",
      "Epoch [10/25], Step [9/40], Loss: 4.7358\n",
      "Epoch [10/25], Step [14/40], Loss: 4.7548\n",
      "Epoch [10/25], Step [19/40], Loss: 4.7462\n",
      "Epoch [10/25], Step [24/40], Loss: 4.7725\n",
      "Epoch [10/25], Step [29/40], Loss: 4.7617\n",
      "Epoch [10/25], Step [34/40], Loss: 4.7591\n",
      "Epoch [10/25], Step [39/40], Loss: 4.8446\n",
      ">> Epoch 10\n",
      "generated text:\n",
      "this movie is one of its best movies in my favorite film and a film . the story between one . the characters were a movie that you don 't have been better in the film , you like the movie that you have\n",
      "\n",
      "None\n",
      "Epoch [11/25], Step [4/40], Loss: 4.5798\n",
      "Epoch [11/25], Step [9/40], Loss: 4.5992\n",
      "Epoch [11/25], Step [14/40], Loss: 4.5862\n",
      "Epoch [11/25], Step [19/40], Loss: 4.5775\n",
      "Epoch [11/25], Step [24/40], Loss: 4.6388\n",
      "Epoch [11/25], Step [29/40], Loss: 4.5876\n",
      "Epoch [11/25], Step [34/40], Loss: 4.6149\n",
      "Epoch [11/25], Step [39/40], Loss: 4.4912\n",
      ">> Epoch 11\n",
      "generated text:\n",
      "this movie is the worst movie ever made . . . . . . . . . . . and i don 't know it 's a movie . . . and i have been a lot of time , i think of the\n",
      "\n",
      "None\n",
      "Epoch [12/25], Step [4/40], Loss: 4.4078\n",
      "Epoch [12/25], Step [9/40], Loss: 4.4306\n",
      "Epoch [12/25], Step [14/40], Loss: 4.4628\n",
      "Epoch [12/25], Step [19/40], Loss: 4.4690\n",
      "Epoch [12/25], Step [24/40], Loss: 4.5200\n",
      "Epoch [12/25], Step [29/40], Loss: 4.4880\n",
      "Epoch [12/25], Step [34/40], Loss: 4.4979\n",
      "Epoch [12/25], Step [39/40], Loss: 4.3618\n",
      ">> Epoch 12\n",
      "generated text:\n",
      "this movie is a very well -written , this movie . it 's not as a very good story , and a very cute girl . this film . . . . . . . .i just like this film was also funny ,\n",
      "\n",
      "None\n",
      "Epoch [13/25], Step [4/40], Loss: 4.2376\n",
      "Epoch [13/25], Step [9/40], Loss: 4.3867\n",
      "Epoch [13/25], Step [14/40], Loss: 4.3401\n",
      "Epoch [13/25], Step [19/40], Loss: 4.3636\n",
      "Epoch [13/25], Step [24/40], Loss: 4.3543\n",
      "Epoch [13/25], Step [29/40], Loss: 4.3506\n",
      "Epoch [13/25], Step [34/40], Loss: 4.4022\n",
      "Epoch [13/25], Step [39/40], Loss: 4.4454\n",
      ">> Epoch 13\n",
      "generated text:\n",
      "this movie is a very thin by a man of time . it was a film of time . i can be really bad movies . it 's not very good , but the movie was too hard to go to get to find\n",
      "\n",
      "None\n",
      "Epoch [14/25], Step [4/40], Loss: 4.1807\n",
      "Epoch [14/25], Step [9/40], Loss: 4.2316\n",
      "Epoch [14/25], Step [14/40], Loss: 4.2699\n",
      "Epoch [14/25], Step [19/40], Loss: 4.2373\n",
      "Epoch [14/25], Step [24/40], Loss: 4.2363\n",
      "Epoch [14/25], Step [29/40], Loss: 4.2673\n",
      "Epoch [14/25], Step [34/40], Loss: 4.2459\n",
      "Epoch [14/25], Step [39/40], Loss: 4.2314\n",
      ">> Epoch 14\n",
      "generated text:\n",
      "this movie is an interesting , so historically inaccurate . this , one , but this is not a bad film is one . the story of a young woman in the tough to the story of the actors do in my opinion that\n",
      "\n",
      "None\n",
      "Epoch [15/25], Step [4/40], Loss: 4.0445\n",
      "Epoch [15/25], Step [9/40], Loss: 4.1092\n",
      "Epoch [15/25], Step [14/40], Loss: 4.1036\n",
      "Epoch [15/25], Step [19/40], Loss: 4.1162\n",
      "Epoch [15/25], Step [24/40], Loss: 4.1237\n",
      "Epoch [15/25], Step [29/40], Loss: 4.1442\n",
      "Epoch [15/25], Step [34/40], Loss: 4.1564\n",
      "Epoch [15/25], Step [39/40], Loss: 4.0652\n",
      ">> Epoch 15\n",
      "generated text:\n",
      "this movie is a terrible acting , it is a very funny film , the main is , but as well as an excellent plots , the movie . . . . . . the acting are great , but the plot is the\n",
      "\n",
      "None\n",
      "Epoch [16/25], Step [4/40], Loss: 3.9448\n",
      "Epoch [16/25], Step [9/40], Loss: 3.9884\n",
      "Epoch [16/25], Step [14/40], Loss: 3.9916\n",
      "Epoch [16/25], Step [19/40], Loss: 4.0143\n",
      "Epoch [16/25], Step [24/40], Loss: 4.0201\n",
      "Epoch [16/25], Step [29/40], Loss: 4.0233\n",
      "Epoch [16/25], Step [34/40], Loss: 4.0891\n",
      "Epoch [16/25], Step [39/40], Loss: 4.1404\n",
      ">> Epoch 16\n",
      "generated text:\n",
      "this movie is about a house and that 's , is very good movie . they were the great . the graphics will be a man in that she would a peep of a life . the film has to be a movie about\n",
      "\n",
      "None\n",
      "Epoch [17/25], Step [4/40], Loss: 3.8922\n",
      "Epoch [17/25], Step [9/40], Loss: 3.9165\n",
      "Epoch [17/25], Step [14/40], Loss: 3.9210\n",
      "Epoch [17/25], Step [19/40], Loss: 3.9590\n",
      "Epoch [17/25], Step [24/40], Loss: 3.9016\n",
      "Epoch [17/25], Step [29/40], Loss: 4.0476\n",
      "Epoch [17/25], Step [34/40], Loss: 3.9635\n",
      "Epoch [17/25], Step [39/40], Loss: 3.9111\n",
      ">> Epoch 17\n",
      "generated text:\n",
      "this movie is a movie . i have a lot . i think i was quite surprised as a good old boy 's voice , not to mention , but i really didn 't want the way through and it . the story .\n",
      "\n",
      "None\n",
      "Epoch [18/25], Step [4/40], Loss: 3.8449\n",
      "Epoch [18/25], Step [9/40], Loss: 3.7737\n",
      "Epoch [18/25], Step [14/40], Loss: 3.8333\n",
      "Epoch [18/25], Step [19/40], Loss: 3.8452\n",
      "Epoch [18/25], Step [24/40], Loss: 3.8712\n",
      "Epoch [18/25], Step [29/40], Loss: 3.8621\n",
      "Epoch [18/25], Step [34/40], Loss: 3.8739\n",
      "Epoch [18/25], Step [39/40], Loss: 3.9090\n",
      ">> Epoch 18\n",
      "generated text:\n",
      "this movie is a great film . if you are either , or have been done in glimpses . . i have to see . . the only problem is one of the film is just that i found the idea that could be\n",
      "\n",
      "None\n",
      "Epoch [19/25], Step [4/40], Loss: 3.7503\n",
      "Epoch [19/25], Step [9/40], Loss: 3.7725\n",
      "Epoch [19/25], Step [14/40], Loss: 3.7029\n",
      "Epoch [19/25], Step [19/40], Loss: 3.7252\n",
      "Epoch [19/25], Step [24/40], Loss: 3.6994\n",
      "Epoch [19/25], Step [29/40], Loss: 3.8008\n",
      "Epoch [19/25], Step [34/40], Loss: 3.8535\n",
      "Epoch [19/25], Step [39/40], Loss: 3.9954\n",
      ">> Epoch 19\n",
      "generated text:\n",
      "this movie is a lot of things . it contains all the movie starts of dreams that the only other things about the movie and that makes a few fleeting snicker -inducing . the story is the best thing is , the actors are\n",
      "\n",
      "None\n",
      "Epoch [20/25], Step [4/40], Loss: 3.5935\n",
      "Epoch [20/25], Step [9/40], Loss: 3.6174\n",
      "Epoch [20/25], Step [14/40], Loss: 3.6377\n",
      "Epoch [20/25], Step [19/40], Loss: 3.6944\n",
      "Epoch [20/25], Step [24/40], Loss: 3.6906\n",
      "Epoch [20/25], Step [29/40], Loss: 3.7277\n",
      "Epoch [20/25], Step [34/40], Loss: 3.7223\n",
      "Epoch [20/25], Step [39/40], Loss: 3.6164\n",
      ">> Epoch 20\n",
      "generated text:\n",
      "this movie is a movie that makes me ! . i think it has some great and then , but this film i 've seen . i am not seen some parts . i am worried . the movie has some funny scenes are\n",
      "\n",
      "None\n",
      "Epoch [21/25], Step [4/40], Loss: 3.5621\n",
      "Epoch [21/25], Step [9/40], Loss: 3.5303\n",
      "Epoch [21/25], Step [14/40], Loss: 3.5953\n",
      "Epoch [21/25], Step [19/40], Loss: 3.6104\n",
      "Epoch [21/25], Step [24/40], Loss: 3.5896\n",
      "Epoch [21/25], Step [29/40], Loss: 3.6556\n",
      "Epoch [21/25], Step [34/40], Loss: 3.6476\n",
      "Epoch [21/25], Step [39/40], Loss: 3.5632\n",
      ">> Epoch 21\n",
      "generated text:\n",
      "this movie is a pretty good movie . . i have to see it and i have seen everything i 've seen them how to do . it has the plot , i found myself to see how bad this show , and this\n",
      "\n",
      "None\n",
      "Epoch [22/25], Step [4/40], Loss: 3.4558\n",
      "Epoch [22/25], Step [9/40], Loss: 3.4929\n",
      "Epoch [22/25], Step [14/40], Loss: 3.5297\n",
      "Epoch [22/25], Step [19/40], Loss: 3.5251\n",
      "Epoch [22/25], Step [24/40], Loss: 3.5467\n",
      "Epoch [22/25], Step [29/40], Loss: 3.5887\n",
      "Epoch [22/25], Step [34/40], Loss: 3.5767\n",
      "Epoch [22/25], Step [39/40], Loss: 3.5678\n",
      ">> Epoch 22\n",
      "generated text:\n",
      "this movie is absolutely nothing besides over the top , and the mother of the stunts and the actors . it 's like a repeat , the viewer is not killing the movie as a sanitarium ! she 's just shallow . i adored\n",
      "\n",
      "None\n",
      "Epoch [23/25], Step [4/40], Loss: 3.3632\n",
      "Epoch [23/25], Step [9/40], Loss: 3.4301\n",
      "Epoch [23/25], Step [14/40], Loss: 3.4819\n",
      "Epoch [23/25], Step [19/40], Loss: 3.4340\n",
      "Epoch [23/25], Step [24/40], Loss: 3.4447\n",
      "Epoch [23/25], Step [29/40], Loss: 3.5011\n",
      "Epoch [23/25], Step [34/40], Loss: 3.5004\n",
      "Epoch [23/25], Step [39/40], Loss: 3.5504\n",
      ">> Epoch 23\n",
      "generated text:\n",
      "this movie is the awkward of the greatest film , and to the plot synopsis of fabled audience , i had to say a decent movie . . . . . . . . . . . but . . . . . .\n",
      "\n",
      "None\n",
      "Epoch [24/25], Step [4/40], Loss: 3.3829\n",
      "Epoch [24/25], Step [9/40], Loss: 3.3623\n",
      "Epoch [24/25], Step [14/40], Loss: 3.3452\n",
      "Epoch [24/25], Step [19/40], Loss: 3.4491\n",
      "Epoch [24/25], Step [24/40], Loss: 3.4965\n",
      "Epoch [24/25], Step [29/40], Loss: 3.4630\n",
      "Epoch [24/25], Step [34/40], Loss: 3.4226\n",
      "Epoch [24/25], Step [39/40], Loss: 3.3797\n",
      ">> Epoch 24\n",
      "generated text:\n",
      "this movie is an awful , this was one of the worst film i have ever . i never ever tried that it was just sick . i didn 't know if they 've been a few philo vance lack of explaining . i\n",
      "\n",
      "None\n",
      "Epoch [25/25], Step [4/40], Loss: 3.2604\n",
      "Epoch [25/25], Step [9/40], Loss: 3.3554\n",
      "Epoch [25/25], Step [14/40], Loss: 3.3383\n",
      "Epoch [25/25], Step [19/40], Loss: 3.3944\n",
      "Epoch [25/25], Step [24/40], Loss: 3.3113\n",
      "Epoch [25/25], Step [29/40], Loss: 3.4104\n",
      "Epoch [25/25], Step [34/40], Loss: 3.3655\n",
      "Epoch [25/25], Step [39/40], Loss: 3.4558\n",
      ">> Epoch 25\n",
      "generated text:\n",
      "this movie is an terrible ! it 's hard to see a book that this show was not in it . it had a bad film ! i just felt that i didn 't find a way it would have been so many of\n",
      "\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs=25\n",
    "total_step = len(train_loader)\n",
    "\n",
    "model = model.to(device)\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [token_to_id.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "for epoch in tqdm(range(0, num_epochs)):    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "\n",
    "        batch_inputs = sample_batched['inputs'].to(device) # torch.Size([128, 80])\n",
    "        batch_targets = sample_batched['targets'].to(device)\n",
    "        \n",
    "        batch_size = batch_targets.size(0)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_size, batch_inputs) # torch.Size([128, 80, 20000])\n",
    "        \n",
    "        # Compute loss\n",
    "        batch_predicts = rearrange(outputs, 'b c l -> b l c') # torch.Size([128, 20000, 80])\n",
    "        loss = criterion(batch_predicts, batch_targets) # torch.Size([128, 80])\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i_batch+1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i_batch, total_step, loss.item())) \n",
    "    \n",
    "    # Save the model checkpoints\n",
    "    torch.save(model.state_dict(), './models/text_gen_gpt-{}.ckpt'.format(epoch+1))\n",
    "    text_gen_callback = TextGenerator(model, device, max_seq_len, num_tokens_generated, start_tokens, id_to_token)\n",
    "    print(f'>> Epoch {epoch + 1}')\n",
    "    print(text_gen_callback.on_epoch_end(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
