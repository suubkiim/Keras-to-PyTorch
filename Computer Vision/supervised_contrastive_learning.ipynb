{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWaoYDd-WuLR"
   },
   "source": [
    "# Supervised Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiacMndAWuLW"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "[Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
    "(Prannay Khosla et al.) is a training methodology that outperforms\n",
    "supervised training with crossentropy on classification tasks.\n",
    "\n",
    "Essentially, training an image classification model with Supervised Contrastive\n",
    "Learning is performed in two phases:\n",
    "\n",
    "1. Training an encoder to learn to produce vector representations of input images such\n",
    "that representations of images in the same class will be more similar compared to\n",
    "representations of images in different classes.\n",
    "2. Training a classifier on top of the frozen encoder.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wqXt6pGiWuLX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe708c98f00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yvBM24OWuLY"
   },
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pkj4Nob8WuLY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# trainset : 50000\n",
      "# testset  : 10000\n"
     ]
    }
   ],
   "source": [
    "print('# trainset :', len(trainset))\n",
    "print('# testset  :', len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga8FGFkTWuLY"
   },
   "source": [
    "## Using image data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_G0coixyWuLZ"
   },
   "outputs": [],
   "source": [
    "# https://github.com/Spijkervet/SimCLR/blob/cd85c4366d2e6ac1b0a16798b76ac0a2c8a94e58/simclr/modules/transformations/simclr.py#L4\n",
    "\n",
    "class TransformsSimCLR:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module that transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \n",
    "        self.alpha = (-0.2 - 0.2) * torch.rand(1) + 0.2 # -20% ~ + 20% 사이 랜덤 생성\n",
    "        self.beta = (-0.2 - 0.2) * torch.rand(1) + 0.2\n",
    "        self.h = int(size[0]*(1+self.alpha))\n",
    "        self.w = int(size[1]*(1+self.beta))\n",
    "        self.random_size = (self.h, self.w)\n",
    "        self.train_transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # adjusted to CIFAR10\n",
    "                torchvision.transforms.RandomHorizontalFlip(),  # with 0.5 probability\n",
    "                torchvision.transforms.RandomRotation(7.2),\n",
    "                torchvision.transforms.Resize(self.random_size), # random size로 조정\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.train_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_aug(im):\n",
    "    # approximate unnormalization \n",
    "    im = im.detach().cpu()\n",
    "    im[0] = im[0]*0.229 + 0.485\n",
    "    im[1] = im[1]*0.224 + 0.456\n",
    "    im[2] = im[2]*0.225 + 0.406\n",
    "    plt.imshow(np.transpose(im, (1, 2, 0)))\n",
    "    plt.show()\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAchElEQVR4nO2da4yc53Xf/2dmdvbOXS2XXC4vNmXdHNlwJJcQXMQI3AQJFDeA7MAVbCeqWhhhUMRADaQfBBeoXaAfnKK24Q+NC7oSohSuLvUlVhohiSMIUF0kMilVliipNiWKFEktd8nl3q9zOf0wQ5RUnv/Z5ezu7NrP/wcsduY587zv8z7ve+adef5zzjF3hxDiF5/Cdg9ACNEe5OxCZIKcXYhMkLMLkQlydiEyQc4uRCaUNtLZzO4F8A0ARQD/1d2/Er1+cGDA943sI9tqaQTB2FrZ3lp7IxttdV+B6umBkY4josUxRvuK53gLTgAhmqudgoXnevO4MPYOpqank5PfsrObWRHAfwbwGwDOAzhuZk+5+2usz76RfXjkT/6EbY/uq1gs3nCfyFZo8UJk4ygUWvuAVK/XqS36/UMrxx32oRago5A+ZgAolfjl08o4IurBfNQCWx1kjgMPa3WMFp2zOrdtprP/zoMPUNtGPsbfA+ANdz/t7qsAHgdw3wa2J4TYQjbi7AcAnLvm+flmmxBiB7LlC3RmdtTMTpjZiemZ6a3enRCCsBFnvwDg0DXPDzbbrsPdj7n7EXc/MjgwuIHdCSE2wkac/TiA28zsZjMrA/g0gKc2Z1hCiM2m5dV4d6+a2ecB/DUa0tsj7v7qGr1QrVaTllZXtBmtrsazFXeAr5DX6zXap1bjK+7Ranw0H5GNHXfUJzrmYouKRyur8ZECEUZnBovnxQK5xFvcV8sqSYHb6rX09RNJik5X93mfDens7v40gKc3sg0hRHvQL+iEyAQ5uxCZIGcXIhPk7EJkgpxdiEzY0Gr8jeLOpYtW5I5WgzuiOIdW5KRIuiqVWpOaWpXe2FiioJVwX5HkFZiYrNiqdFUIpKtwPshxR+OIJNEakckAwAOZNZL66HFHETItxOrozi5EJsjZhcgEObsQmSBnFyIT5OxCZEJbV+PNWgvU6OjoSLZHq+DR25iV+DJnuVimtmI9vdHFhUXaZ2pqitpmZ7ltemqS2paW+P7YKnNvby/ts2vXLmrr6+W2/v4Batu7dyTZ3t3dQ/tUVivUVg/SOlmU4I0saceZp6J0YZEtWKkPxt8KxWL6WoxyBurOLkQmyNmFyAQ5uxCZIGcXIhPk7EJkgpxdiExoq/QW8fbbb1PbhQv/IGktAGB5ZZn26Sx3UltXB5fXvLJCbUuLs8n2mdnLtM/syhy1rS5xqakS2KK8duVy+tg8CO5YXuHHvLDEbb19XJYb3ZcuIfCBD3yA9rn77n9EbQODg9RGq76AS2UsF+JatihYp1QKpOBAeosCb254X4GkqDu7EJkgZxciE+TsQmSCnF2ITJCzC5EJcnYhMmFD0puZnQEwB6AGoOruR1rdVpQTbIVIQ2fPcLluefEKtZWdS2XlIo9cWq2kJZn55UDGKXIJcGGOy2sT7/AxRtLQwQP7k+179uyhfTyQrlYqXN6sLfBzNn96Idn+ymsnaZ+/P/5javvUp/4Ztd16663UVqnwOWZE0ZSR9FYsBP0KfK5YXruWymsF2ttm6Oz/xD3wHiHEjkAf44XIhI06uwP4GzN7wcyObsaAhBBbw0Y/xn/U3S+Y2V4APzSz/+vuz137guabwFEAGNm7d4O7E0K0yobu7O5+ofl/AsD3AdyTeM0xdz/i7kcGB3gaIyHE1tKys5tZr5n1X30M4DcB8KVWIcS2spGP8SMAvt+UAEoA/ru7/1XcxWgZokg+uf3225PtFSKFAcDyEk/muDh/ltomJ7jt7OnTyfbTp8don5Uql97QyaPvdu3iSSWLRX7aBgb70gbjkuLyCt9Xpc6lq4V5Lst1lNLHViLtAPDGW6eo7YknH6e2T37id6jt/Xe8P9leDxJHRslPY1orbcUSqoZ7YlJ1EPXWsrO7+2kAv9xqfyFEe5H0JkQmyNmFyAQ5uxCZIGcXIhPk7EJkwo6p9RZF6xSJFFLu4XXD+oNkiD58mNr27UsnlQSA4eE3ku2ljuO0z8TEJWqLEj0ODPDabJXVVWorsIitQGrqIHXDAGBXTxe1zS8tURtLBhqVPCt2cJnyzIXz1PYXT/8FtXX3difbDx08RPuEMlmJy2SlIFqulbtqFAnKklRGY9edXYhMkLMLkQlydiEyQc4uRCbI2YXIhPaWf3LA2AJjsPLI4guikkYOHsBhwVtcdy9fBb/ltg8l2/v6Bmmfl156ntoujqXLWgFANQryWeYBKAsL6dxvEb096RVrIA4K6QwCOKZm0+OoF/glVwnKWlVq/Hy++tPXqO2J76QDaB743Qdpn0MH+Ep9wfiKe5SDrljgq+Rs1b2VslARurMLkQlydiEyQc4uRCbI2YXIBDm7EJkgZxciE9orvQWBMPVAemPlcaJAgeh9rBhpb9EmSb+9e/bRLiMjo9S2vMRlspnpGWqLyj+xfGbxXHEKRS6v+RIPyKmTiJe687HXqjxPHjxIrhYE8pw8mc6B+ud//n3a54HP/nNqGxocorbVVT7+jlJrJaU2s4/u7EJkgpxdiEyQswuRCXJ2ITJBzi5EJsjZhciENaU3M3sEwG8DmHD3DzbbhgA8AeAwgDMA7nd3Xm/p+u0l21vJt9U6/D0uVqjSxlIQyTU4wKWaiW6eQ8+DZG1RJFovidqL+szM8Lx758cvU9v8Eo++K5AyXx2kHQD6+gKZL8hRWA1sTM578YUXaZ/uMj8vn7n/09Q2MMDzHkZyaZHkrgvzyZHzudEcdH8K4N53tT0E4Bl3vw3AM83nQogdzJrO3qy3fuVdzfcBeLT5+FEAn9jcYQkhNptWv7OPuPvV0qUX0ajoKoTYwWx4gc4bX7bpF0wzO2pmJ8zsxHTwE1AhxNbSqrOPm9koADT/T7AXuvsxdz/i7kcGBwda3J0QYqO06uxPAbiaxOtBAD/YnOEIIbaK9UhvjwH4GIBhMzsP4EsAvgLgSTP7HICzAO7f6EBaicpikVUAYMbluijyCvUgmojsLnrHXFnhiRIXF7l0xSL9gFhGY7YoEeXsHJfeyp28JNOh4T3UxmaxWuHzgUBiXVzmpbJmFhapjZVrqgSn+dlnn6W2KDLvdz/7GWobGrqJb5Mk2mSSHBD4S+BGazq7u7Mj+PW1+gohdg76BZ0QmSBnFyIT5OxCZIKcXYhMkLMLkQntTTgZEMlJLJLHgminyFQocomngEDyQloKGRt/h/Z5+eRL1DY7zSPK6oH0FkUBthJV2N/XT20DxTIfR5AEktXhW0YgewaJQC2Yj2Jg6yynx++d/NIvBckh/+7431Hb3PI8tf3LB/8Fte0fTSclra1ymdLJXHmgvenOLkQmyNmFyAQ5uxCZIGcXIhPk7EJkgpxdiEzYMdJbK0QygzmXT1DjtlIHn5J3LpxJtp84/iPaZ2ZqnNqqVS6t1KqtJZxkslycvJDPx9LSErXNzMxRW4kkliwTKayxPZ7cJDrXlQqPiOsop8exusojDqtBftOu7m5qO378eWq7ODZGbb/32d9Ltn/knnv4QGjE58YSTgohfgGQswuRCXJ2ITJBzi5EJsjZhciEHbMav+klnoJcXMUiP+zxsXPU9uO/fy7Zfv7cadqnGqwUR2n3KkGus84gL1yF5Hibn+dBGgMDg9S2ssrHMTXFK36x1f+eHl5aKVqND8QEkPgkAMDCQjq/3mqFH1eU7y66PXZ1dVHb2NhFajt27Fiy/exbb9E+//S3Pp5sr9ejQC4hRBbI2YXIBDm7EJkgZxciE+TsQmSCnF2ITFhP+adHAPw2gAl3/2Cz7csAfh/ApebLvujuT2/VIKMgDkYQK4L5OZ777cUX/je1vfXWT5Pt05PvLl9/DTU+9n3707nHAGB8/AzfZjAfAwPp4pkrK1xOWlrk5ZOioJAoOKVEShdF5aT6+3dR28wsl/mWlvj4Sx3p8k8dnVwmK3VwLW9mOpAbC+l9AUC5xANo5kj5rccee4z2uXzpUrJ9cnKS9lnPnf1PAdybaP+6u9/V/NsyRxdCbA5rOru7PwcguHUJIX4e2Mh39s+b2ctm9oiZ8RKVQogdQavO/k0AtwC4C8AYgK+yF5rZUTM7YWYnpqf5zyGFEFtLS87u7uPuXnP3OoBvAaApNdz9mLsfcfcjg4PpxSMhxNbTkrOb2bXLyJ8EcHJzhiOE2CrWI709BuBjAIbN7DyALwH4mJndhUZs2RkAf7DeHbIyRJG8ViQyjgX6WimQhd5462fU9vb5U9Q2ReSfggXTWOeyUH8XP+bB/j5qO3v+bWq7aSj96ck8iPKa41+vegcHqa1/gMtJRkLR5uZ53rrFBZ7vbmj3Hr6v4JZ1ZSq9tuyBpFiIIiaDUMXVlSCvXZWXvSp3pOVICw7sL//6r5LtM7P8XK7p7O7+mUTzw2v1E0LsLPQLOiEyQc4uRCbI2YXIBDm7EJkgZxciE9qacNLdUaulJaBWIttKQZ9qjUsdV4LIoHptlW9zNR05dlP/XtrHOvn76coyl5o6g8irYvAWXSql52T/6Ajtc+aNN6mtHpyW7v5+ajNyH+np5pFtczPT1DZ1hZ+z9xw+TG2lUjoS7ezZs7RPFCE4OsojFaeChJmLwTbr9XSSUA8m3yzIsknQnV2ITJCzC5EJcnYhMkHOLkQmyNmFyAQ5uxCZ0PZab5tZ043JeABgQXRSuZMnBhzs45Fc1T3phDyTl3kdta4enthwcSUtuQBAbZVHUO3q4/XSukppSWagr5f22beXR5RNLfBx9PfwyDxWW64j0A1ved9Bars8yRM9Xhx7h9qGdg8n2/ft20f7jI2NUVt0/R48sJ/aZmbTSSUBYH5uIdleDbJ9lsh5jiRs3dmFyAQ5uxCZIGcXIhPk7EJkgpxdiExoeyBMtZJegS6W+FDYqnslyusVrPp2lXkJono1CILYM5RsX5znNTQmZ3kOuqUCX43vL1MTSKwLAKBWTQfyrC7zcawspVeDAWB5gc/H6hK3LZDV56LxFeb9+3mwTmegoJw5ly6FBADT09PJ9oMH+cr/7t27qe3cuXPUdvkyH8fBgweora83rZRMjPPt0Xx3QXI93dmFyAQ5uxCZIGcXIhPk7EJkgpxdiEyQswuRCesp/3QIwJ8BGEGj3NMxd/+GmQ0BeALAYTRKQN3v7jxaoYkT2ahS5TIUyI/7PQhKqDvP0dW5i+dBK1kQJFNOT9cto7xi9dIZLg9eCoI7fIDLg73B+Gem0zLgTUGQxs3v5XnVui/xAI6pKS4NTVwYT7b3d72P76vEj3lyieeg6wzSsS0spstNLc7zfHH79/O5WljgxUnPX+ABOT87xfP87SbBOrtuGqR9lpfS0luBlEoD1ndnrwL4I3e/E8BHAPyhmd0J4CEAz7j7bQCeaT4XQuxQ1nR2dx9z9xebj+cAvA7gAID7ADzafNmjAD6xRWMUQmwCN/Sd3cwOA7gbwPMARtz9auDvRTQ+5gshdijrdnYz6wPwXQBfcPfrvsh5ow5zMluEmR01sxNmdmImyKsthNha1uXsZtaBhqN/292/12weN7PRpn0UwESqr7sfc/cj7n5kYIAvbgghtpY1nd0aeW4eBvC6u3/tGtNTAB5sPn4QwA82f3hCiM1iPVFvvwLgAQCvmNlLzbYvAvgKgCfN7HMAzgK4fz07rJPccJuZm665I2oaGuYRSLfc8iFqmxs7nWzfcxPPW7d3hkeUTU7z8LX5VT4fXX1cHix1pHPeXZlOS1AAsGeYS4cHDnPbwALPrze8Ny0PBqkBUSrznILDwzzfXXcPz8m3Wkvv0ApcoorKcl28eJHaasE119PHS2Utk0jQjiAStKsnfc0VCvz+vaazu/uPALCr8tfX6i+E2BnoF3RCZIKcXYhMkLMLkQlydiEyQc4uRCa0vfwTK09TDKJ1eEmbQMcB354H73E33/5hajtDQvbeevM12qe3k+/r4D5edunyLJfsllaipILp416t8T7LEzyyrauHj9+COe7oSktvNSKFAcB4UEarr5/La3tHuPRZIfubmeMJOMeDRI8dJS43lspcSi2VeQZRdu1XiCQHACsr6WSfkYStO7sQmSBnFyIT5OxCZIKcXYhMkLMLkQlydiEyYcdIbx6EQ3G5jg+/FNiqQY24gvHopPe8Py3LVetcchkLZLmz55MpAAAAl69wOayjxKPeykTiieeX1A0D0N/HpaZqhcs801PpY4vkus4uLk/t6ufnc3AgkG1JFNjkJJf5ZmZ41NtSEI3IkqkCQLHMx8iux85OnoCznyQdNeP3b93ZhcgEObsQmSBnFyIT5OxCZIKcXYhMaPtqfLQqvMl7opZigS+bVoPhFXrSedDu+ODdtE9nEJgwt8RVgVJ3uowTAIxPctv0AllJLvBTvbDIg0KuBOm/+3t7qa1MajIZ+HwUC+ngDgDo7Q4CSYJzvTDPAop4n/4BrsjYIg9OmZnnCsrMVJBGnVwjHgQvLc2n1YRqZZX20Z1diEyQswuRCXJ2ITJBzi5EJsjZhcgEObsQmbCm9GZmhwD8GRolmR3AMXf/hpl9GcDvA7iasOuL7v50qwPheeY4UY6u6iqXtYrBYVeLQaCDp8sT9XTywJSR995KbbPTk9TWz+NPcGAoHQQBAKffSQegXApy2h06dAe1lYJyQlcu81xtA71pqWzfHl7cs6+Ln5fuTm5bXuLSYRcpoWRBDrqpOT5XCKTD3l5+0oolHghTr6Wvq1JQoqqHlLwqFvk5WY/OXgXwR+7+opn1A3jBzH7YtH3d3f/TOrYhhNhm1lPrbQzAWPPxnJm9DoBXRhRC7Ehu6Du7mR0GcDeA55tNnzezl83sETPj5T6FENvOup3dzPoAfBfAF9x9FsA3AdwC4C407vxfJf2OmtkJMzsxHfz0UgixtazL2c2sAw1H/7a7fw8A3H3c3WvuXgfwLQD3pPq6+zF3P+LuRwYH+OKMEGJrWdPZrbFM/jCA1939a9e0j17zsk8COLn5wxNCbBbrWY3/FQAPAHjFzF5qtn0RwGfM7C405LgzAP5gPTssECknkt6YrVZPSxYAUCNyBgCgGOTpCnJ4maWlkFoQ2dY3NExtt/3SndT26vHL1Dbxzjt8m/vSSyd33Hoz7TMf5FVbWOT52EqDPDqsi0S9lYLz3NXJpau+IBfe0CCXIqvkOhhZ5hF2U7M8P934BD8vl65wOW8luBxXltMS8rJxaXmeRLdVgut+PavxPwKQOkMta+pCiPajX9AJkQlydiEyQc4uRCbI2YXIBDm7EJnQ1oST7rzUzWYnooy2trzCyx3Vlrh0wcrxFAI5qRLYevZyOewOnsMSq9X/RW1j595Mtg8RKQwAusrpCCoAmB7nkXmdQURcX1daKqsFkYqTV3gizflFnnCyv59LgH196aSY/QP8mLt7+fb6+9JJRwFgaJhfV1eCklLsl6WTU3w+Zufm0gbnMqru7EJkgpxdiEyQswuRCXJ2ITJBzi5EJsjZhciEtkpv1UoFFy9eTNqWlrg0wRJL7t69m/ZZDqKa6kENrY4OLvGUO9KJJaPIsO6gHtr+/Yeobdd7fonaPhRE7Q2RSLQ3T71O+3T28uSchw+MUNv0HI8O6+5Kz6MV0vIlANTBZc+lVS7ZXZzkNdaKMywSjYuztSofx+oqv66WA1mxUuHbLFp6/ncHkX7DPen5vXCZnxPd2YXIBDm7EJkgZxciE+TsQmSCnF2ITJCzC5EJbZXe6l7HykpaumCJKAEebXZpIl3XDAD6gkioW2/n9deKwTiqRP5h7UAcEddV4nJMlIBz1+hhattfSMuDpYE9tM/bZ39KbTMzJLoKQLnMa9ytrKTlyF0D/Lx0d/NItL5AupqZ59Fm9WT6RMCKPApwtcD3tbzCZcqlhahGHGd4KC0h776J113p7U77xAtneEJM3dmFyAQ5uxCZIGcXIhPk7EJkgpxdiExYczXezLoAPAegs/n677j7l8zsZgCPA9gN4AUAD7h7uibN/99aS+WfWCBMb5APrBistp48ycvS9QWBK6N700Eh3UHZIn5UwOpqMF1Bx3KRr4IPjRxIti8s8gCJXfM8kMQ6+RxPXZmitjrS83/p8jTtsxiMsbe3m9rKHfwyHhgkxUQLvM9ykQfJ9PUOUdvI8CC1mfMT2tufLl81u8DLSZ16eyzZvhJcU+u5s68A+DV3/2U0yjPfa2YfAfDHAL7u7rcCmALwuXVsSwixTazp7N7g6ltuR/PPAfwagO802x8F8ImtGKAQYnNYb332YrOC6wSAHwJ4E8C0u1/9hcF5AOnPj0KIHcG6nN3da+5+F4CDAO4B8P717sDMjprZCTM7MT/Pv5MJIbaWG1qNd/dpAM8C+McABs3s6irHQQAXSJ9j7n7E3Y/0BQtqQoitZU1nN7M9ZjbYfNwN4DcAvI6G03+q+bIHAfxgi8YohNgE1hMIMwrgUTMrovHm8KS7/08zew3A42b2HwD8HwAPr7Uh9zqVm6LyT6xPdzeXYyIpj5WgivYF8Dx5fT1croskwNVKIL0VeL9KnR9bqZiWAQ/cfAft07WLB6ecOXWK2mA8n1yhyMbIg0wmLo3z7QW3pd4efh2wQJhSiculHcFxFUtc9qwEAVGXLl2itnNjaZsbP+iF5fS1U6tzP1rT2d39ZQD/oPKYu59G4/u7EOLnAP2CTohMkLMLkQlydiEyQc4uRCbI2YXIBIskr03fmdklAGebT4cB8IRZ7UPjuB6N43p+3sbxXndPJhxsq7Nft2OzE+5+ZFt2rnFoHBmOQx/jhcgEObsQmbCdzn5sG/d9LRrH9Wgc1/MLM45t+84uhGgv+hgvRCZsi7Ob2b1m9lMze8PMHtqOMTTHccbMXjGzl8zsRBv3+4iZTZjZyWvahszsh2Z2qvmf1/7Z2nF82cwuNOfkJTP7eBvGccjMnjWz18zsVTP71832ts5JMI62zomZdZnZj83sJ81x/Ptm+81m9nzTb54ws/INbdjd2/oHoIhGWqv3ASgD+AmAO9s9juZYzgAY3ob9/iqADwM4eU3bfwTwUPPxQwD+eJvG8WUA/6bN8zEK4MPNx/0AfgbgznbPSTCOts4JGrmF+5qPOwA8D+AjAJ4E8Olm+38B8K9uZLvbcWe/B8Ab7n7aG6mnHwdw3zaMY9tw9+cAXHlX831oJO4E2pTAk4yj7bj7mLu/2Hw8h0ZylANo85wE42gr3mDTk7xuh7MfAHDumufbmazSAfyNmb1gZke3aQxXGXH3q8nALwJIJ6lvD583s5ebH/O3/OvEtZjZYTTyJzyPbZyTd40DaPOcbEWS19wX6D7q7h8G8FsA/tDMfnW7BwQ03tnReCPaDr4J4BY0agSMAfhqu3ZsZn0AvgvgC+5+XeWKds5JYhxtnxPfQJJXxnY4+wUAh655TpNVbjXufqH5fwLA97G9mXfGzWwUAJr/efH5LcTdx5sXWh3At9CmOTGzDjQc7Nvu/r1mc9vnJDWO7ZqT5r6ncYNJXhnb4ezHAdzWXFksA/g0gKfaPQgz6zWz/quPAfwmAF4Xaut5Co3EncA2JvC86lxNPok2zIk1EgY+DOB1d//aNaa2zgkbR7vnZMuSvLZrhfFdq40fR2Ol800A/3abxvA+NJSAnwB4tZ3jAPAYGh8HK2h89/ocGjXzngFwCsDfAhjapnH8NwCvAHgZDWcbbcM4PorGR/SXAbzU/Pt4u+ckGEdb5wTAh9BI4voyGm8s/+6aa/bHAN4A8D8AdN7IdvULOiEyIfcFOiGyQc4uRCbI2YXIBDm7EJkgZxciE+TsQmSCnF2ITJCzC5EJ/w9+q65Hc26+4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_augmentation = TransformsSimCLR((32,32))\n",
    "orig_img = trainset[18][0]\n",
    "\n",
    "plt.imshow(np.transpose(orig_img, (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "augmented_img = data_augmentation(orig_img)\n",
    "plot_aug(augmented_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_augmentation = TransformsSimCLR((32,32))\n",
    "orig_img = trainset[123][0]\n",
    "\n",
    "plt.imshow(np.transpose(orig_img, (1,2,0)))\n",
    "plt.show()\n",
    "\n",
    "augmented_img = data_augmentation(orig_img)\n",
    "plot_aug(augmented_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3dy4kU5WuLZ"
   },
   "source": [
    "## Build the encoder model\n",
    "\n",
    "The encoder model takes the image as input and turns it into a 2048-dimensional\n",
    "feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2048 차원의 feature vector 생성\n",
    "* 기존 ResNet18의 마지막 FC Layer(classifier)를 빼고 encoder를 만듦\n",
    "* 케라스는 ResNet50 사용하였으나, 메모리/시간 상의 문제로 ResNet18 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "PCeL6tslWuLa"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, resnet_pt, data_augmentation):\n",
    "        super().__init__()\n",
    "        # drop last fully connected layer\n",
    "        self.resnet = nn.Sequential(*list(resnet_pt.children())[:-1]) \n",
    "        self.data_augmentation = data_augmentation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        aug_x = self.data_augmentation(x)\n",
    "        out = self.resnet(aug_x)\n",
    "        return out\n",
    "\n",
    "resnet_pt = models.resnet18(pretrained=True)\n",
    "encoder = Encoder(resnet_pt, data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Encoder                                       --                        --\n",
       "├─Sequential: 1-1                             [1, 512, 1, 1]            --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 17, 15]           9,408\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 17, 15]           128\n",
       "│    └─ReLU: 2-3                              [1, 64, 17, 15]           --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 9, 8]             --\n",
       "│    └─Sequential: 2-5                        [1, 64, 9, 8]             --\n",
       "│    │    └─BasicBlock: 3-1                   [1, 64, 9, 8]             73,984\n",
       "│    │    └─BasicBlock: 3-2                   [1, 64, 9, 8]             73,984\n",
       "│    └─Sequential: 2-6                        [1, 128, 5, 4]            --\n",
       "│    │    └─BasicBlock: 3-3                   [1, 128, 5, 4]            230,144\n",
       "│    │    └─BasicBlock: 3-4                   [1, 128, 5, 4]            295,424\n",
       "│    └─Sequential: 2-7                        [1, 256, 3, 2]            --\n",
       "│    │    └─BasicBlock: 3-5                   [1, 256, 3, 2]            919,040\n",
       "│    │    └─BasicBlock: 3-6                   [1, 256, 3, 2]            1,180,672\n",
       "│    └─Sequential: 2-8                        [1, 512, 2, 1]            --\n",
       "│    │    └─BasicBlock: 3-7                   [1, 512, 2, 1]            3,673,088\n",
       "│    │    └─BasicBlock: 3-8                   [1, 512, 2, 1]            4,720,640\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [1, 512, 1, 1]            --\n",
       "===============================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 52.87\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.97\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 45.68\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F0ZP4K8WuLa"
   },
   "source": [
    "## Build the classification model\n",
    "\n",
    "The classification model adds a fully-connected layer on top of the encoder,\n",
    "plus a softmax layer with the target classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 앞의 encoder에 붙일 MLP classifier를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v0eJoiYEWuLb"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# batch_size = 256\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05\n",
    "num_classes = 10\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, encoder, dropout_rate, hidden_units, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.train()\n",
    "        self.Dropout_1 = nn.Dropout(dropout_rate)\n",
    "        self.Dropout_2 = nn.Dropout(dropout_rate)\n",
    "        self.Linear_1 = nn.Linear(512, hidden_units)\n",
    "        self.Linear_2 = nn.Linear(hidden_units, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.Dropout_1(out)\n",
    "        out = self.Linear_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.Dropout_2(out)\n",
    "        out = self.Linear_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVOgw_yEWuLb"
   },
   "source": [
    "## Experiment 1: Train the baseline classification model\n",
    "\n",
    "In this experiment, a baseline classifier is trained as usual, i.e., `the\n",
    "encoder and the classifier parts are trained together` as a single model\n",
    "to minimize the crossentropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **[Experiment 1]** 은 encoder와 classifier를 함께 붙여서 CE Loss로 한번에 학습 진행\n",
    "* **[Experiment 2]** 에서는 encoder를 Contrastive Loss를 활용하여 한번 학습을 시키고, 이후에 classifier를 따로 학습\n",
    "* classifier는 동일한 것을 사용하여 contrastive loss로의 encoder 학습 여부에 따른 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c23BlMkTWuLb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Classifier                                         --                        --\n",
       "├─Encoder: 1-1                                     [1, 512, 1, 1]            --\n",
       "│    └─Sequential: 2-1                             [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 17, 15]           9,408\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 17, 15]           128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 17, 15]           --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 9, 8]             --\n",
       "│    │    └─Sequential: 3-5                        [1, 64, 9, 8]             147,968\n",
       "│    │    └─Sequential: 3-6                        [1, 128, 5, 4]            525,568\n",
       "│    │    └─Sequential: 3-7                        [1, 256, 3, 2]            2,099,712\n",
       "│    │    └─Sequential: 3-8                        [1, 512, 2, 1]            8,393,728\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 [1, 512, 1, 1]            --\n",
       "├─Dropout: 1-2                                     [1, 512]                  --\n",
       "├─Linear: 1-3                                      [1, 512]                  262,656\n",
       "├─Dropout: 1-4                                     [1, 512]                  --\n",
       "├─Linear: 1-5                                      [1, 10]                   5,130\n",
       "====================================================================================================\n",
       "Total params: 11,444,298\n",
       "Trainable params: 11,444,298\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 53.14\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.97\n",
       "Params size (MB): 45.78\n",
       "Estimated Total Size (MB): 46.76\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = Classifier(encoder, dropout_rate, hidden_units, num_classes)\n",
    "summary(classifier, (1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device :  cuda\n",
      "resnet classifier model training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a70787340a64216bc8bf11d207fe96d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [49/196], Loss: 1.1489\n",
      "Epoch [1/50], Step [99/196], Loss: 0.8601\n",
      "Epoch [1/50], Step [149/196], Loss: 1.0026\n",
      "=> Train Accuracy : 66.102%  |  Valid Accuracy : 71.69%\n",
      "==================================================\n",
      "Epoch [2/50], Step [49/196], Loss: 0.6781\n",
      "Epoch [2/50], Step [99/196], Loss: 0.7634\n",
      "Epoch [2/50], Step [149/196], Loss: 0.5819\n",
      "=> Train Accuracy : 77.5%  |  Valid Accuracy : 75.9%\n",
      "==================================================\n",
      "Epoch [3/50], Step [49/196], Loss: 0.5878\n",
      "Epoch [3/50], Step [99/196], Loss: 0.5459\n",
      "Epoch [3/50], Step [149/196], Loss: 0.6590\n",
      "=> Train Accuracy : 80.54%  |  Valid Accuracy : 76.85%\n",
      "==================================================\n",
      "Epoch [4/50], Step [49/196], Loss: 0.4247\n",
      "Epoch [4/50], Step [99/196], Loss: 0.5327\n",
      "Epoch [4/50], Step [149/196], Loss: 0.5291\n",
      "=> Train Accuracy : 82.954%  |  Valid Accuracy : 77.13%\n",
      "==================================================\n",
      "Epoch [5/50], Step [49/196], Loss: 0.4936\n",
      "Epoch [5/50], Step [99/196], Loss: 0.5033\n",
      "Epoch [5/50], Step [149/196], Loss: 0.4827\n",
      "=> Train Accuracy : 84.378%  |  Valid Accuracy : 79.25%\n",
      "==================================================\n",
      "Epoch [6/50], Step [49/196], Loss: 0.4056\n",
      "Epoch [6/50], Step [99/196], Loss: 0.4592\n",
      "Epoch [6/50], Step [149/196], Loss: 0.4270\n",
      "=> Train Accuracy : 85.646%  |  Valid Accuracy : 77.09%\n",
      "==================================================\n",
      "Epoch [7/50], Step [49/196], Loss: 0.4518\n",
      "Epoch [7/50], Step [99/196], Loss: 0.3724\n",
      "Epoch [7/50], Step [149/196], Loss: 0.5464\n",
      "=> Train Accuracy : 86.474%  |  Valid Accuracy : 80.93%\n",
      "==================================================\n",
      "Epoch [8/50], Step [49/196], Loss: 0.3807\n",
      "Epoch [8/50], Step [99/196], Loss: 0.3634\n",
      "Epoch [8/50], Step [149/196], Loss: 0.3489\n",
      "=> Train Accuracy : 87.856%  |  Valid Accuracy : 77.93%\n",
      "==================================================\n",
      "Epoch [9/50], Step [49/196], Loss: 0.3354\n",
      "Epoch [9/50], Step [99/196], Loss: 0.3167\n",
      "Epoch [9/50], Step [149/196], Loss: 0.4978\n",
      "=> Train Accuracy : 88.632%  |  Valid Accuracy : 82.4%\n",
      "==================================================\n",
      "Epoch [10/50], Step [49/196], Loss: 0.3453\n",
      "Epoch [10/50], Step [99/196], Loss: 0.2755\n",
      "Epoch [10/50], Step [149/196], Loss: 0.3003\n",
      "=> Train Accuracy : 89.446%  |  Valid Accuracy : 81.16%\n",
      "==================================================\n",
      "Epoch [11/50], Step [49/196], Loss: 0.4911\n",
      "Epoch [11/50], Step [99/196], Loss: 0.2726\n",
      "Epoch [11/50], Step [149/196], Loss: 0.3349\n",
      "=> Train Accuracy : 90.244%  |  Valid Accuracy : 81.25%\n",
      "==================================================\n",
      "Epoch [12/50], Step [49/196], Loss: 0.1763\n",
      "Epoch [12/50], Step [99/196], Loss: 0.2761\n",
      "Epoch [12/50], Step [149/196], Loss: 0.3006\n",
      "=> Train Accuracy : 90.722%  |  Valid Accuracy : 81.91%\n",
      "==================================================\n",
      "Epoch [13/50], Step [49/196], Loss: 0.2803\n",
      "Epoch [13/50], Step [99/196], Loss: 0.2517\n",
      "Epoch [13/50], Step [149/196], Loss: 0.2073\n",
      "=> Train Accuracy : 91.476%  |  Valid Accuracy : 81.52%\n",
      "==================================================\n",
      "Epoch [14/50], Step [49/196], Loss: 0.3093\n",
      "Epoch [14/50], Step [99/196], Loss: 0.3127\n",
      "Epoch [14/50], Step [149/196], Loss: 0.2629\n",
      "=> Train Accuracy : 92.178%  |  Valid Accuracy : 82.38%\n",
      "==================================================\n",
      "Epoch [15/50], Step [49/196], Loss: 0.1519\n",
      "Epoch [15/50], Step [99/196], Loss: 0.2900\n",
      "Epoch [15/50], Step [149/196], Loss: 0.2928\n",
      "=> Train Accuracy : 92.51%  |  Valid Accuracy : 81.8%\n",
      "==================================================\n",
      "Epoch [16/50], Step [49/196], Loss: 0.1636\n",
      "Epoch [16/50], Step [99/196], Loss: 0.1744\n",
      "Epoch [16/50], Step [149/196], Loss: 0.1901\n",
      "=> Train Accuracy : 93.066%  |  Valid Accuracy : 81.43%\n",
      "==================================================\n",
      "Epoch [17/50], Step [49/196], Loss: 0.2007\n",
      "Epoch [17/50], Step [99/196], Loss: 0.2188\n",
      "Epoch [17/50], Step [149/196], Loss: 0.2476\n",
      "=> Train Accuracy : 93.278%  |  Valid Accuracy : 81.33%\n",
      "==================================================\n",
      "Epoch [18/50], Step [49/196], Loss: 0.1151\n",
      "Epoch [18/50], Step [99/196], Loss: 0.1437\n",
      "Epoch [18/50], Step [149/196], Loss: 0.1230\n",
      "=> Train Accuracy : 93.552%  |  Valid Accuracy : 81.86%\n",
      "==================================================\n",
      "Epoch [19/50], Step [49/196], Loss: 0.1976\n",
      "Epoch [19/50], Step [99/196], Loss: 0.1064\n",
      "Epoch [19/50], Step [149/196], Loss: 0.2113\n",
      "=> Train Accuracy : 94.254%  |  Valid Accuracy : 81.55%\n",
      "==================================================\n",
      "Epoch [20/50], Step [49/196], Loss: 0.2136\n",
      "Epoch [20/50], Step [99/196], Loss: 0.1876\n",
      "Epoch [20/50], Step [149/196], Loss: 0.1021\n",
      "=> Train Accuracy : 94.21%  |  Valid Accuracy : 81.52%\n",
      "==================================================\n",
      "Epoch [21/50], Step [49/196], Loss: 0.0832\n",
      "Epoch [21/50], Step [99/196], Loss: 0.1009\n",
      "Epoch [21/50], Step [149/196], Loss: 0.1505\n",
      "=> Train Accuracy : 94.794%  |  Valid Accuracy : 82.04%\n",
      "==================================================\n",
      "Epoch [22/50], Step [49/196], Loss: 0.1798\n",
      "Epoch [22/50], Step [99/196], Loss: 0.1200\n",
      "Epoch [22/50], Step [149/196], Loss: 0.1126\n",
      "=> Train Accuracy : 95.116%  |  Valid Accuracy : 81.25%\n",
      "==================================================\n",
      "Epoch [23/50], Step [49/196], Loss: 0.1435\n",
      "Epoch [23/50], Step [99/196], Loss: 0.1858\n",
      "Epoch [23/50], Step [149/196], Loss: 0.1929\n",
      "=> Train Accuracy : 95.324%  |  Valid Accuracy : 82.47%\n",
      "==================================================\n",
      "Epoch [24/50], Step [49/196], Loss: 0.2473\n",
      "Epoch [24/50], Step [99/196], Loss: 0.1183\n",
      "Epoch [24/50], Step [149/196], Loss: 0.1508\n",
      "=> Train Accuracy : 95.446%  |  Valid Accuracy : 82.61%\n",
      "==================================================\n",
      "Epoch [25/50], Step [49/196], Loss: 0.1162\n",
      "Epoch [25/50], Step [99/196], Loss: 0.1294\n",
      "Epoch [25/50], Step [149/196], Loss: 0.0742\n",
      "=> Train Accuracy : 95.484%  |  Valid Accuracy : 82.94%\n",
      "==================================================\n",
      "Epoch [26/50], Step [49/196], Loss: 0.1225\n",
      "Epoch [26/50], Step [99/196], Loss: 0.2001\n",
      "Epoch [26/50], Step [149/196], Loss: 0.1361\n",
      "=> Train Accuracy : 95.802%  |  Valid Accuracy : 82.7%\n",
      "==================================================\n",
      "Epoch [27/50], Step [49/196], Loss: 0.1595\n",
      "Epoch [27/50], Step [99/196], Loss: 0.1062\n",
      "Epoch [27/50], Step [149/196], Loss: 0.1435\n",
      "=> Train Accuracy : 96.026%  |  Valid Accuracy : 82.36%\n",
      "==================================================\n",
      "Epoch [28/50], Step [49/196], Loss: 0.1367\n",
      "Epoch [28/50], Step [99/196], Loss: 0.0518\n",
      "Epoch [28/50], Step [149/196], Loss: 0.1319\n",
      "=> Train Accuracy : 96.172%  |  Valid Accuracy : 81.97%\n",
      "==================================================\n",
      "Epoch [29/50], Step [49/196], Loss: 0.0449\n",
      "Epoch [29/50], Step [99/196], Loss: 0.0748\n",
      "Epoch [29/50], Step [149/196], Loss: 0.0721\n",
      "=> Train Accuracy : 96.428%  |  Valid Accuracy : 82.26%\n",
      "==================================================\n",
      "Epoch [30/50], Step [49/196], Loss: 0.0871\n",
      "Epoch [30/50], Step [99/196], Loss: 0.0747\n",
      "Epoch [30/50], Step [149/196], Loss: 0.0882\n",
      "=> Train Accuracy : 96.672%  |  Valid Accuracy : 82.59%\n",
      "==================================================\n",
      "Epoch [31/50], Step [49/196], Loss: 0.1271\n",
      "Epoch [31/50], Step [99/196], Loss: 0.1427\n",
      "Epoch [31/50], Step [149/196], Loss: 0.1018\n",
      "=> Train Accuracy : 96.508%  |  Valid Accuracy : 82.4%\n",
      "==================================================\n",
      "Epoch [32/50], Step [49/196], Loss: 0.0622\n",
      "Epoch [32/50], Step [99/196], Loss: 0.0795\n",
      "Epoch [32/50], Step [149/196], Loss: 0.0929\n",
      "=> Train Accuracy : 96.726%  |  Valid Accuracy : 82.71%\n",
      "==================================================\n",
      "Epoch [33/50], Step [49/196], Loss: 0.0651\n",
      "Epoch [33/50], Step [99/196], Loss: 0.0478\n",
      "Epoch [33/50], Step [149/196], Loss: 0.1013\n",
      "=> Train Accuracy : 96.918%  |  Valid Accuracy : 82.96%\n",
      "==================================================\n",
      "Epoch [34/50], Step [49/196], Loss: 0.2003\n",
      "Epoch [34/50], Step [99/196], Loss: 0.0988\n",
      "Epoch [34/50], Step [149/196], Loss: 0.0391\n",
      "=> Train Accuracy : 96.918%  |  Valid Accuracy : 83.0%\n",
      "==================================================\n",
      "Epoch [35/50], Step [49/196], Loss: 0.0949\n",
      "Epoch [35/50], Step [99/196], Loss: 0.0337\n",
      "Epoch [35/50], Step [149/196], Loss: 0.1408\n",
      "=> Train Accuracy : 97.286%  |  Valid Accuracy : 83.48%\n",
      "==================================================\n",
      "Epoch [36/50], Step [49/196], Loss: 0.0408\n",
      "Epoch [36/50], Step [99/196], Loss: 0.1139\n",
      "Epoch [36/50], Step [149/196], Loss: 0.0586\n",
      "=> Train Accuracy : 96.952%  |  Valid Accuracy : 82.33%\n",
      "==================================================\n",
      "Epoch [37/50], Step [49/196], Loss: 0.1024\n",
      "Epoch [37/50], Step [99/196], Loss: 0.0348\n",
      "Epoch [37/50], Step [149/196], Loss: 0.1677\n",
      "=> Train Accuracy : 97.034%  |  Valid Accuracy : 82.7%\n",
      "==================================================\n",
      "Epoch [38/50], Step [49/196], Loss: 0.0924\n",
      "Epoch [38/50], Step [99/196], Loss: 0.0688\n",
      "Epoch [38/50], Step [149/196], Loss: 0.0581\n",
      "=> Train Accuracy : 97.306%  |  Valid Accuracy : 81.95%\n",
      "==================================================\n",
      "Epoch [39/50], Step [49/196], Loss: 0.2033\n",
      "Epoch [39/50], Step [99/196], Loss: 0.0259\n",
      "Epoch [39/50], Step [149/196], Loss: 0.1259\n",
      "=> Train Accuracy : 97.312%  |  Valid Accuracy : 82.57%\n",
      "==================================================\n",
      "Epoch [40/50], Step [49/196], Loss: 0.0690\n",
      "Epoch [40/50], Step [99/196], Loss: 0.0354\n",
      "Epoch [40/50], Step [149/196], Loss: 0.0798\n",
      "=> Train Accuracy : 97.556%  |  Valid Accuracy : 82.9%\n",
      "==================================================\n",
      "Epoch [41/50], Step [49/196], Loss: 0.0686\n",
      "Epoch [41/50], Step [99/196], Loss: 0.0504\n",
      "Epoch [41/50], Step [149/196], Loss: 0.1808\n",
      "=> Train Accuracy : 97.504%  |  Valid Accuracy : 82.68%\n",
      "==================================================\n",
      "Epoch [42/50], Step [49/196], Loss: 0.0714\n",
      "Epoch [42/50], Step [99/196], Loss: 0.0558\n",
      "Epoch [42/50], Step [149/196], Loss: 0.0465\n",
      "=> Train Accuracy : 97.628%  |  Valid Accuracy : 82.3%\n",
      "==================================================\n",
      "Epoch [43/50], Step [49/196], Loss: 0.0334\n",
      "Epoch [43/50], Step [99/196], Loss: 0.1062\n",
      "Epoch [43/50], Step [149/196], Loss: 0.0484\n",
      "=> Train Accuracy : 97.562%  |  Valid Accuracy : 82.86%\n",
      "==================================================\n",
      "Epoch [44/50], Step [49/196], Loss: 0.0718\n",
      "Epoch [44/50], Step [99/196], Loss: 0.0868\n",
      "Epoch [44/50], Step [149/196], Loss: 0.0450\n",
      "=> Train Accuracy : 97.33%  |  Valid Accuracy : 82.42%\n",
      "==================================================\n",
      "Epoch [45/50], Step [49/196], Loss: 0.0315\n",
      "Epoch [45/50], Step [99/196], Loss: 0.0556\n",
      "Epoch [45/50], Step [149/196], Loss: 0.0557\n",
      "=> Train Accuracy : 97.524%  |  Valid Accuracy : 82.19%\n",
      "==================================================\n",
      "Epoch [46/50], Step [49/196], Loss: 0.0568\n",
      "Epoch [46/50], Step [99/196], Loss: 0.1079\n",
      "Epoch [46/50], Step [149/196], Loss: 0.0288\n",
      "=> Train Accuracy : 97.846%  |  Valid Accuracy : 82.52%\n",
      "==================================================\n",
      "Epoch [47/50], Step [49/196], Loss: 0.0684\n",
      "Epoch [47/50], Step [99/196], Loss: 0.0841\n",
      "Epoch [47/50], Step [149/196], Loss: 0.0575\n",
      "=> Train Accuracy : 97.722%  |  Valid Accuracy : 82.3%\n",
      "==================================================\n",
      "Epoch [48/50], Step [49/196], Loss: 0.1011\n",
      "Epoch [48/50], Step [99/196], Loss: 0.0657\n",
      "Epoch [48/50], Step [149/196], Loss: 0.0420\n",
      "=> Train Accuracy : 97.798%  |  Valid Accuracy : 83.11%\n",
      "==================================================\n",
      "Epoch [49/50], Step [49/196], Loss: 0.1312\n",
      "Epoch [49/50], Step [99/196], Loss: 0.0732\n",
      "Epoch [49/50], Step [149/196], Loss: 0.0488\n",
      "=> Train Accuracy : 97.918%  |  Valid Accuracy : 83.65%\n",
      "==================================================\n",
      "Epoch [50/50], Step [49/196], Loss: 0.0876\n",
      "Epoch [50/50], Step [99/196], Loss: 0.0426\n",
      "Epoch [50/50], Step [149/196], Loss: 0.0366\n",
      "=> Train Accuracy : 97.974%  |  Valid Accuracy : 83.13%\n",
      "==================================================\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Current device : ', device)\n",
    "\n",
    "total_step = len(trainloader)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model.\n",
    "print(\"resnet classifier model training...\")\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    running_loss = 0\n",
    "    classifier.train()\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = classifier(inputs)  # torch.Size([256, 10])\n",
    "        predictions = torch.argmax(F.softmax(outputs, dim=-1), dim=-1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if (i+1)%50 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i, total_step, loss.item())) \n",
    "            \n",
    "    train_loss.append(loss)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_acc.append(train_accuracy)\n",
    "    \n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    for i, test_data in enumerate(testloader):\n",
    "        classifier.eval()\n",
    "        \n",
    "        test_inputs, test_labels = test_data\n",
    "\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        \n",
    "        test_outputs = classifier(test_inputs)\n",
    "        val_loss = criterion(test_outputs, test_labels)\n",
    "        \n",
    "        test_predictions = torch.argmax(F.softmax(test_outputs, dim=-1), dim=-1)\n",
    "        valid_total += test_labels.size(0)\n",
    "        valid_correct += (test_predictions == test_labels).sum().item()\n",
    "    \n",
    "    valid_loss.append(val_loss)\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    valid_acc.append(valid_accuracy)\n",
    "    print(f'=> Train Accuracy : {train_accuracy}%  |  Valid Accuracy : {valid_accuracy}%')\n",
    "    print(\"==================================================\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "inv_history = {'train_loss': train_loss,\n",
    "               'valid_loss': valid_loss,\n",
    "               'train_acc': train_acc,\n",
    "               'valid_acc': valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIyCWai3WuLc"
   },
   "source": [
    "## Experiment 2: Use supervised contrastive learning\n",
    "\n",
    "In this experiment, the model is trained in two phases. In the first phase,\n",
    "`the encoder is pretrained to optimize the supervised contrastive loss`,\n",
    "described in [Prannay Khosla et al.](https://arxiv.org/abs/2004.11362).\n",
    "\n",
    "In the second phase, `the classifier is trained using the trained encoder with\n",
    "its weights freezed`; only the weights of fully-connected layers with the\n",
    "softmax are optimized.\n",
    "\n",
    "* **Phase 1**: Encoder + Single Linear projection layer(Contrastive loss 계산을 위한 feature projection 역할) + Contrastive Loss 로 학습\n",
    "* **Phase 2**: Encoder Freeze 시키고 **[Experiment 1]** 과 동일한 classifier를 붙여서 학습하여 비교\n",
    "\n",
    "### 1. Supervised contrastive learning loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.eq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False,  True, False],\n",
      "        [False,  True, False, False, False,  True],\n",
      "        [False, False,  True, False, False, False],\n",
      "        [False, False, False,  True, False, False],\n",
      "        [ True, False, False, False,  True, False],\n",
      "        [False,  True, False, False, False,  True]])\n",
      "tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000],\n",
      "        [0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([1,2,3,4,1,2]) # labels , size : [batch]\n",
    "print(torch.eq(y_true.unsqueeze(-1), y_true.unsqueeze(-1).T))\n",
    "y_true = torch.eq(y_true.unsqueeze(-1), y_true.unsqueeze(-1).T).float()\n",
    "y_true /= torch.sum(y_true, dim=1, keepdim=True)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* True 부분에서 유사도 값이 최대가 되어야 함\n",
    "* Symmetric matrix\n",
    "* 궁극적 목적 => prediction(logits)에 softmax를 취한 것과 y_true 의 분포가 유사하도록 \n",
    "> positive pair similarity 크게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HFslS-gYWuLc"
   },
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/addons/blob/v0.15.0/tensorflow_addons/losses/npairs.py#L23-L73\n",
    "\n",
    "class SupervisedContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def npairs_loss(self, y_true, y_pred):\n",
    "        \"\"\"Computes the npairs loss between `y_true` and `y_pred`.\n",
    "        Npairs loss expects paired data where a pair is composed of samples from\n",
    "        the same labels and each pairs in the minibatch have different labels.\n",
    "        The loss takes each row of the pair-wise similarity matrix, `y_pred`,\n",
    "        as logits and the remapped multi-class labels, `y_true`, as labels.\n",
    "        Args:\n",
    "          y_true: 1-D integer `Tensor` with shape `[batch_size]` of\n",
    "            multi-class labels.\n",
    "          y_pred: 2-D float `Tensor` with shape `[batch_size, batch_size]` of\n",
    "            similarity matrix between embedding matrices.\n",
    "        Returns:\n",
    "          npairs_loss: float scalar.\n",
    "        \"\"\"\n",
    "        y_true = y_true.type(y_pred.dtype)\n",
    "\n",
    "        # Expand to [batch_size, 1]\n",
    "        y_true = torch.eq(y_true.unsqueeze(-1), y_true.unsqueeze(-1).T).float()\n",
    "        y_true /= torch.sum(y_true, dim=1, keepdim=True)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/65458736/pytorch-equivalent-to-tf-nn-softmax-cross-entropy-with-logits-and-tf-nn-sigmoid\n",
    "        loss = F.cross_entropy(y_pred, y_true)\n",
    "\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "    def forward(self, feature_vectors, labels):\n",
    "        \n",
    "        # L2 Normalize feature vectors\n",
    "        feature_vectors_normalized = F.normalize(feature_vectors, dim=1, p=2)\n",
    "    \n",
    "        # Compute logits (Similarity)\n",
    "        logits = torch.div(\n",
    "            torch.matmul(\n",
    "                feature_vectors_normalized, feature_vectors_normalized.T\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        \n",
    "        return self.npairs_loss(labels, logits)\n",
    "\n",
    "\n",
    "# Encoder -> ProjectionHead -> Contrastive Loss 계산\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, encoder, projection_units):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.Linear = nn.Linear(512, projection_units)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.Linear(out)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2Tp96MGWuLc"
   },
   "source": [
    "### 2. Pretrain the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "_my2I1n3WuLd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ProjectionHead                                     --                        --\n",
       "├─Encoder: 1-1                                     [1, 512, 1, 1]            --\n",
       "│    └─Sequential: 2-1                             [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 17, 15]           9,408\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 17, 15]           128\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 17, 15]           --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 9, 8]             --\n",
       "│    │    └─Sequential: 3-5                        [1, 64, 9, 8]             147,968\n",
       "│    │    └─Sequential: 3-6                        [1, 128, 5, 4]            525,568\n",
       "│    │    └─Sequential: 3-7                        [1, 256, 3, 2]            2,099,712\n",
       "│    │    └─Sequential: 3-8                        [1, 512, 2, 1]            8,393,728\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 [1, 512, 1, 1]            --\n",
       "├─Linear: 1-2                                      [1, 128]                  65,664\n",
       "====================================================================================================\n",
       "Total params: 11,242,176\n",
       "Trainable params: 11,242,176\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 52.94\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.97\n",
       "Params size (MB): 44.97\n",
       "Estimated Total Size (MB): 45.95\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_pt = models.resnet18(pretrained=True)\n",
    "encoder = Encoder(resnet_pt, data_augmentation)\n",
    "encoder_with_projection_head = ProjectionHead(encoder, projection_units)\n",
    "\n",
    "summary(encoder_with_projection_head, [1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device :  cuda\n",
      "encoder_with_projection_head model training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bf673edc214347bde4b119c1cfefe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [49/196], Loss: 4.8825\n",
      "Epoch [1/50], Step [99/196], Loss: 4.5738\n",
      "Epoch [1/50], Step [149/196], Loss: 4.4532\n",
      "=> Valid Loss: 4.4937\n",
      "Epoch [2/50], Step [49/196], Loss: 4.3345\n",
      "Epoch [2/50], Step [99/196], Loss: 4.3210\n",
      "Epoch [2/50], Step [149/196], Loss: 4.2978\n",
      "=> Valid Loss: 4.4294\n",
      "Epoch [3/50], Step [49/196], Loss: 4.2897\n",
      "Epoch [3/50], Step [99/196], Loss: 4.1814\n",
      "Epoch [3/50], Step [149/196], Loss: 4.3602\n",
      "=> Valid Loss: 4.2806\n",
      "Epoch [4/50], Step [49/196], Loss: 4.1043\n",
      "Epoch [4/50], Step [99/196], Loss: 4.1326\n",
      "Epoch [4/50], Step [149/196], Loss: 4.0592\n",
      "=> Valid Loss: 4.2654\n",
      "Epoch [5/50], Step [49/196], Loss: 4.1652\n",
      "Epoch [5/50], Step [99/196], Loss: 4.1951\n",
      "Epoch [5/50], Step [149/196], Loss: 4.0101\n",
      "=> Valid Loss: 4.2180\n",
      "Epoch [6/50], Step [49/196], Loss: 3.9058\n",
      "Epoch [6/50], Step [99/196], Loss: 3.9026\n",
      "Epoch [6/50], Step [149/196], Loss: 4.2063\n",
      "=> Valid Loss: 4.1662\n",
      "Epoch [7/50], Step [49/196], Loss: 4.0136\n",
      "Epoch [7/50], Step [99/196], Loss: 3.8441\n",
      "Epoch [7/50], Step [149/196], Loss: 3.9501\n",
      "=> Valid Loss: 4.1736\n",
      "Epoch [8/50], Step [49/196], Loss: 3.9434\n",
      "Epoch [8/50], Step [99/196], Loss: 4.0148\n",
      "Epoch [8/50], Step [149/196], Loss: 3.8533\n",
      "=> Valid Loss: 4.1224\n",
      "Epoch [9/50], Step [49/196], Loss: 3.8936\n",
      "Epoch [9/50], Step [99/196], Loss: 4.0768\n",
      "Epoch [9/50], Step [149/196], Loss: 3.8562\n",
      "=> Valid Loss: 4.1533\n",
      "Epoch [10/50], Step [49/196], Loss: 3.8005\n",
      "Epoch [10/50], Step [99/196], Loss: 3.7587\n",
      "Epoch [10/50], Step [149/196], Loss: 3.5974\n",
      "=> Valid Loss: 4.1601\n",
      "Epoch [11/50], Step [49/196], Loss: 3.5706\n",
      "Epoch [11/50], Step [99/196], Loss: 3.7847\n",
      "Epoch [11/50], Step [149/196], Loss: 3.8215\n",
      "=> Valid Loss: 4.1391\n",
      "Epoch [12/50], Step [49/196], Loss: 3.7571\n",
      "Epoch [12/50], Step [99/196], Loss: 3.7191\n",
      "Epoch [12/50], Step [149/196], Loss: 3.7458\n",
      "=> Valid Loss: 4.2090\n",
      "Epoch [13/50], Step [49/196], Loss: 3.8696\n",
      "Epoch [13/50], Step [99/196], Loss: 3.5982\n",
      "Epoch [13/50], Step [149/196], Loss: 3.7929\n",
      "=> Valid Loss: 4.2445\n",
      "Epoch [14/50], Step [49/196], Loss: 3.6564\n",
      "Epoch [14/50], Step [99/196], Loss: 3.6780\n",
      "Epoch [14/50], Step [149/196], Loss: 3.8232\n",
      "=> Valid Loss: 4.1741\n",
      "Epoch [15/50], Step [49/196], Loss: 3.6884\n",
      "Epoch [15/50], Step [99/196], Loss: 3.5955\n",
      "Epoch [15/50], Step [149/196], Loss: 3.6564\n",
      "=> Valid Loss: 4.1816\n",
      "Epoch [16/50], Step [49/196], Loss: 3.7588\n",
      "Epoch [16/50], Step [99/196], Loss: 3.6639\n",
      "Epoch [16/50], Step [149/196], Loss: 3.7239\n",
      "=> Valid Loss: 4.1789\n",
      "Epoch [17/50], Step [49/196], Loss: 3.6305\n",
      "Epoch [17/50], Step [99/196], Loss: 3.6902\n",
      "Epoch [17/50], Step [149/196], Loss: 3.6744\n",
      "=> Valid Loss: 4.2101\n",
      "Epoch [18/50], Step [49/196], Loss: 3.6125\n",
      "Epoch [18/50], Step [99/196], Loss: 3.6311\n",
      "Epoch [18/50], Step [149/196], Loss: 3.5321\n",
      "=> Valid Loss: 4.1686\n",
      "Epoch [19/50], Step [49/196], Loss: 3.5452\n",
      "Epoch [19/50], Step [99/196], Loss: 3.6551\n",
      "Epoch [19/50], Step [149/196], Loss: 3.4992\n",
      "=> Valid Loss: 4.2302\n",
      "Epoch [20/50], Step [49/196], Loss: 3.5055\n",
      "Epoch [20/50], Step [99/196], Loss: 3.4702\n",
      "Epoch [20/50], Step [149/196], Loss: 3.6392\n",
      "=> Valid Loss: 4.2124\n",
      "Epoch [21/50], Step [49/196], Loss: 3.4901\n",
      "Epoch [21/50], Step [99/196], Loss: 3.5352\n",
      "Epoch [21/50], Step [149/196], Loss: 3.5971\n",
      "=> Valid Loss: 4.1966\n",
      "Epoch [22/50], Step [49/196], Loss: 3.6019\n",
      "Epoch [22/50], Step [99/196], Loss: 3.4822\n",
      "Epoch [22/50], Step [149/196], Loss: 3.5761\n",
      "=> Valid Loss: 4.2151\n",
      "Epoch [23/50], Step [49/196], Loss: 3.4286\n",
      "Epoch [23/50], Step [99/196], Loss: 3.4117\n",
      "Epoch [23/50], Step [149/196], Loss: 3.7444\n",
      "=> Valid Loss: 4.2503\n",
      "Epoch [24/50], Step [49/196], Loss: 3.4750\n",
      "Epoch [24/50], Step [99/196], Loss: 3.4125\n",
      "Epoch [24/50], Step [149/196], Loss: 3.4780\n",
      "=> Valid Loss: 4.2198\n",
      "Epoch [25/50], Step [49/196], Loss: 3.5883\n",
      "Epoch [25/50], Step [99/196], Loss: 3.4645\n",
      "Epoch [25/50], Step [149/196], Loss: 3.5055\n",
      "=> Valid Loss: 4.3173\n",
      "Epoch [26/50], Step [49/196], Loss: 3.4923\n",
      "Epoch [26/50], Step [99/196], Loss: 3.6437\n",
      "Epoch [26/50], Step [149/196], Loss: 3.5618\n",
      "=> Valid Loss: 4.1963\n",
      "Epoch [27/50], Step [49/196], Loss: 3.4292\n",
      "Epoch [27/50], Step [99/196], Loss: 3.5953\n",
      "Epoch [27/50], Step [149/196], Loss: 3.4320\n",
      "=> Valid Loss: 4.2610\n",
      "Epoch [28/50], Step [49/196], Loss: 3.4491\n",
      "Epoch [28/50], Step [99/196], Loss: 3.4551\n",
      "Epoch [28/50], Step [149/196], Loss: 3.5484\n",
      "=> Valid Loss: 4.2433\n",
      "Epoch [29/50], Step [49/196], Loss: 3.5141\n",
      "Epoch [29/50], Step [99/196], Loss: 3.3974\n",
      "Epoch [29/50], Step [149/196], Loss: 3.5376\n",
      "=> Valid Loss: 4.2273\n",
      "Epoch [30/50], Step [49/196], Loss: 3.4639\n",
      "Epoch [30/50], Step [99/196], Loss: 3.4843\n",
      "Epoch [30/50], Step [149/196], Loss: 3.4379\n",
      "=> Valid Loss: 4.3015\n",
      "Epoch [31/50], Step [49/196], Loss: 3.3965\n",
      "Epoch [31/50], Step [99/196], Loss: 3.3704\n",
      "Epoch [31/50], Step [149/196], Loss: 3.4815\n",
      "=> Valid Loss: 4.3218\n",
      "Epoch [32/50], Step [49/196], Loss: 3.4293\n",
      "Epoch [32/50], Step [99/196], Loss: 3.3261\n",
      "Epoch [32/50], Step [149/196], Loss: 3.4246\n",
      "=> Valid Loss: 4.3016\n",
      "Epoch [33/50], Step [49/196], Loss: 3.5693\n",
      "Epoch [33/50], Step [99/196], Loss: 3.3147\n",
      "Epoch [33/50], Step [149/196], Loss: 3.5001\n",
      "=> Valid Loss: 4.2966\n",
      "Epoch [34/50], Step [49/196], Loss: 3.4331\n",
      "Epoch [34/50], Step [99/196], Loss: 3.3155\n",
      "Epoch [34/50], Step [149/196], Loss: 3.5105\n",
      "=> Valid Loss: 4.2783\n",
      "Epoch [35/50], Step [49/196], Loss: 3.4178\n",
      "Epoch [35/50], Step [99/196], Loss: 3.4214\n",
      "Epoch [35/50], Step [149/196], Loss: 3.4012\n",
      "=> Valid Loss: 4.2915\n",
      "Epoch [36/50], Step [49/196], Loss: 3.4295\n",
      "Epoch [36/50], Step [99/196], Loss: 3.3764\n",
      "Epoch [36/50], Step [149/196], Loss: 3.5761\n",
      "=> Valid Loss: 4.3358\n",
      "Epoch [37/50], Step [49/196], Loss: 3.3641\n",
      "Epoch [37/50], Step [99/196], Loss: 3.4825\n",
      "Epoch [37/50], Step [149/196], Loss: 3.3392\n",
      "=> Valid Loss: 4.2418\n",
      "Epoch [38/50], Step [49/196], Loss: 3.6406\n",
      "Epoch [38/50], Step [99/196], Loss: 3.3747\n",
      "Epoch [38/50], Step [149/196], Loss: 3.3755\n",
      "=> Valid Loss: 4.3884\n",
      "Epoch [39/50], Step [49/196], Loss: 3.3559\n",
      "Epoch [39/50], Step [99/196], Loss: 3.4465\n",
      "Epoch [39/50], Step [149/196], Loss: 3.3791\n",
      "=> Valid Loss: 4.3337\n",
      "Epoch [40/50], Step [49/196], Loss: 3.6545\n",
      "Epoch [40/50], Step [99/196], Loss: 3.4119\n",
      "Epoch [40/50], Step [149/196], Loss: 3.4397\n",
      "=> Valid Loss: 4.2720\n",
      "Epoch [41/50], Step [49/196], Loss: 3.3394\n",
      "Epoch [41/50], Step [99/196], Loss: 3.4312\n",
      "Epoch [41/50], Step [149/196], Loss: 3.3618\n",
      "=> Valid Loss: 4.3507\n",
      "Epoch [42/50], Step [49/196], Loss: 3.3447\n",
      "Epoch [42/50], Step [99/196], Loss: 3.3847\n",
      "Epoch [42/50], Step [149/196], Loss: 3.4230\n",
      "=> Valid Loss: 4.3178\n",
      "Epoch [43/50], Step [49/196], Loss: 3.4471\n",
      "Epoch [43/50], Step [99/196], Loss: 3.4866\n",
      "Epoch [43/50], Step [149/196], Loss: 3.3097\n",
      "=> Valid Loss: 4.3402\n",
      "Epoch [44/50], Step [49/196], Loss: 3.3991\n",
      "Epoch [44/50], Step [99/196], Loss: 3.4927\n",
      "Epoch [44/50], Step [149/196], Loss: 3.3698\n",
      "=> Valid Loss: 4.3273\n",
      "Epoch [45/50], Step [49/196], Loss: 3.4294\n",
      "Epoch [45/50], Step [99/196], Loss: 3.3556\n",
      "Epoch [45/50], Step [149/196], Loss: 3.5139\n",
      "=> Valid Loss: 4.3654\n",
      "Epoch [46/50], Step [49/196], Loss: 3.3496\n",
      "Epoch [46/50], Step [99/196], Loss: 3.4190\n",
      "Epoch [46/50], Step [149/196], Loss: 3.4519\n",
      "=> Valid Loss: 4.3410\n",
      "Epoch [47/50], Step [49/196], Loss: 3.2915\n",
      "Epoch [47/50], Step [99/196], Loss: 3.3825\n",
      "Epoch [47/50], Step [149/196], Loss: 3.4679\n",
      "=> Valid Loss: 4.3541\n",
      "Epoch [48/50], Step [49/196], Loss: 3.4271\n",
      "Epoch [48/50], Step [99/196], Loss: 3.3260\n",
      "Epoch [48/50], Step [149/196], Loss: 3.2969\n",
      "=> Valid Loss: 4.3138\n",
      "Epoch [49/50], Step [49/196], Loss: 3.6105\n",
      "Epoch [49/50], Step [99/196], Loss: 3.3267\n",
      "Epoch [49/50], Step [149/196], Loss: 3.4187\n",
      "=> Valid Loss: 4.3327\n",
      "Epoch [50/50], Step [49/196], Loss: 3.4129\n",
      "Epoch [50/50], Step [99/196], Loss: 3.3238\n",
      "Epoch [50/50], Step [149/196], Loss: 3.3645\n",
      "=> Valid Loss: 4.3351\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Current device : ', device)\n",
    "\n",
    "total_step = len(trainloader)\n",
    "criterion = SupervisedContrastiveLoss(temperature)\n",
    "optimizer = torch.optim.Adam(encoder_with_projection_head.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model.\n",
    "print(\"encoder_with_projection_head model training...\")\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    running_loss = 0\n",
    "    encoder_with_projection_head.train()\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = encoder_with_projection_head(inputs)  # torch.Size([256, 10])\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if (i+1)%50 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i, total_step, loss.item())) \n",
    "    \n",
    "    val_losses = []\n",
    "    for i, test_data in enumerate(testloader):\n",
    "        encoder_with_projection_head.eval()\n",
    "        \n",
    "        test_inputs, test_labels = test_data\n",
    "\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        \n",
    "        test_outputs = encoder_with_projection_head(test_inputs)\n",
    "        val_loss = criterion(test_outputs, test_labels)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    print('=> Valid Loss: {:.4f}'.format((sum(val_losses)/len(val_losses)).item()))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePwgdXu0WuLd"
   },
   "source": [
    "### 3. Train the classifier with the frozen encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* encoder_with_projection_head 의 encoder만 불러와서 classifier 붙임\n",
    "* encoder는 `requires_grad = False`로 freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_scl = Classifier(encoder_with_projection_head.encoder, dropout_rate, hidden_units, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze encoder\n",
    "for param in classifier_scl.encoder.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Encoder는 non-trainbale 함을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Encoder                                       --                        --\n",
       "├─Sequential: 1-1                             [1, 512, 1, 1]            --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 17, 15]           (9,408)\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 17, 15]           (128)\n",
       "│    └─ReLU: 2-3                              [1, 64, 17, 15]           --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 9, 8]             --\n",
       "│    └─Sequential: 2-5                        [1, 64, 9, 8]             --\n",
       "│    │    └─BasicBlock: 3-1                   [1, 64, 9, 8]             (73,984)\n",
       "│    │    └─BasicBlock: 3-2                   [1, 64, 9, 8]             (73,984)\n",
       "│    └─Sequential: 2-6                        [1, 128, 5, 4]            --\n",
       "│    │    └─BasicBlock: 3-3                   [1, 128, 5, 4]            (230,144)\n",
       "│    │    └─BasicBlock: 3-4                   [1, 128, 5, 4]            (295,424)\n",
       "│    └─Sequential: 2-7                        [1, 256, 3, 2]            --\n",
       "│    │    └─BasicBlock: 3-5                   [1, 256, 3, 2]            (919,040)\n",
       "│    │    └─BasicBlock: 3-6                   [1, 256, 3, 2]            (1,180,672)\n",
       "│    └─Sequential: 2-8                        [1, 512, 2, 1]            --\n",
       "│    │    └─BasicBlock: 3-7                   [1, 512, 2, 1]            (3,673,088)\n",
       "│    │    └─BasicBlock: 3-8                   [1, 512, 2, 1]            (4,720,640)\n",
       "│    └─AdaptiveAvgPool2d: 2-9                 [1, 512, 1, 1]            --\n",
       "===============================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 0\n",
       "Non-trainable params: 11,176,512\n",
       "Total mult-adds (M): 52.87\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.97\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 45.68\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(classifier_scl.encoder, [1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Classifier                                         --                        --\n",
       "├─Encoder: 1-1                                     [1, 512, 1, 1]            --\n",
       "│    └─Sequential: 2-1                             [1, 512, 1, 1]            --\n",
       "│    │    └─Conv2d: 3-1                            [1, 64, 17, 15]           (9,408)\n",
       "│    │    └─BatchNorm2d: 3-2                       [1, 64, 17, 15]           (128)\n",
       "│    │    └─ReLU: 3-3                              [1, 64, 17, 15]           --\n",
       "│    │    └─MaxPool2d: 3-4                         [1, 64, 9, 8]             --\n",
       "│    │    └─Sequential: 3-5                        [1, 64, 9, 8]             (147,968)\n",
       "│    │    └─Sequential: 3-6                        [1, 128, 5, 4]            (525,568)\n",
       "│    │    └─Sequential: 3-7                        [1, 256, 3, 2]            (2,099,712)\n",
       "│    │    └─Sequential: 3-8                        [1, 512, 2, 1]            (8,393,728)\n",
       "│    │    └─AdaptiveAvgPool2d: 3-9                 [1, 512, 1, 1]            --\n",
       "├─Dropout: 1-2                                     [1, 512]                  --\n",
       "├─Linear: 1-3                                      [1, 512]                  262,656\n",
       "├─Dropout: 1-4                                     [1, 512]                  --\n",
       "├─Linear: 1-5                                      [1, 10]                   5,130\n",
       "====================================================================================================\n",
       "Total params: 11,444,298\n",
       "Trainable params: 267,786\n",
       "Non-trainable params: 11,176,512\n",
       "Total mult-adds (M): 53.14\n",
       "====================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.97\n",
       "Params size (MB): 45.78\n",
       "Estimated Total Size (MB): 46.76\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(classifier_scl, [1, 3, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "14BpOwzfWuLe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device :  cuda\n",
      "resnet classifier_scl model training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d661b17c269f4df5aad7b89721bdce61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [49/196], Loss: 0.0386\n",
      "Epoch [1/50], Step [99/196], Loss: 0.0950\n",
      "Epoch [1/50], Step [149/196], Loss: 0.1223\n",
      "=> Train Accuracy : 97.662%  |  Valid Accuracy : 83.65%\n",
      "==================================================\n",
      "Epoch [2/50], Step [49/196], Loss: 0.0177\n",
      "Epoch [2/50], Step [99/196], Loss: 0.0837\n",
      "Epoch [2/50], Step [149/196], Loss: 0.1304\n",
      "=> Train Accuracy : 98.366%  |  Valid Accuracy : 83.51%\n",
      "==================================================\n",
      "Epoch [3/50], Step [49/196], Loss: 0.0733\n",
      "Epoch [3/50], Step [99/196], Loss: 0.1401\n",
      "Epoch [3/50], Step [149/196], Loss: 0.0946\n",
      "=> Train Accuracy : 98.398%  |  Valid Accuracy : 83.34%\n",
      "==================================================\n",
      "Epoch [4/50], Step [49/196], Loss: 0.0393\n",
      "Epoch [4/50], Step [99/196], Loss: 0.0288\n",
      "Epoch [4/50], Step [149/196], Loss: 0.0732\n",
      "=> Train Accuracy : 98.36%  |  Valid Accuracy : 83.63%\n",
      "==================================================\n",
      "Epoch [5/50], Step [49/196], Loss: 0.0677\n",
      "Epoch [5/50], Step [99/196], Loss: 0.0302\n",
      "Epoch [5/50], Step [149/196], Loss: 0.0383\n",
      "=> Train Accuracy : 98.512%  |  Valid Accuracy : 83.73%\n",
      "==================================================\n",
      "Epoch [6/50], Step [49/196], Loss: 0.0234\n",
      "Epoch [6/50], Step [99/196], Loss: 0.0641\n",
      "Epoch [6/50], Step [149/196], Loss: 0.0271\n",
      "=> Train Accuracy : 98.296%  |  Valid Accuracy : 83.44%\n",
      "==================================================\n",
      "Epoch [7/50], Step [49/196], Loss: 0.0423\n",
      "Epoch [7/50], Step [99/196], Loss: 0.0261\n",
      "Epoch [7/50], Step [149/196], Loss: 0.0754\n",
      "=> Train Accuracy : 98.334%  |  Valid Accuracy : 84.0%\n",
      "==================================================\n",
      "Epoch [8/50], Step [49/196], Loss: 0.0462\n",
      "Epoch [8/50], Step [99/196], Loss: 0.0742\n",
      "Epoch [8/50], Step [149/196], Loss: 0.0316\n",
      "=> Train Accuracy : 98.242%  |  Valid Accuracy : 83.79%\n",
      "==================================================\n",
      "Epoch [9/50], Step [49/196], Loss: 0.0164\n",
      "Epoch [9/50], Step [99/196], Loss: 0.0613\n",
      "Epoch [9/50], Step [149/196], Loss: 0.0205\n",
      "=> Train Accuracy : 98.372%  |  Valid Accuracy : 83.94%\n",
      "==================================================\n",
      "Epoch [10/50], Step [49/196], Loss: 0.0545\n",
      "Epoch [10/50], Step [99/196], Loss: 0.0357\n",
      "Epoch [10/50], Step [149/196], Loss: 0.0562\n",
      "=> Train Accuracy : 98.376%  |  Valid Accuracy : 83.79%\n",
      "==================================================\n",
      "Epoch [11/50], Step [49/196], Loss: 0.0703\n",
      "Epoch [11/50], Step [99/196], Loss: 0.0551\n",
      "Epoch [11/50], Step [149/196], Loss: 0.0510\n",
      "=> Train Accuracy : 98.308%  |  Valid Accuracy : 83.79%\n",
      "==================================================\n",
      "Epoch [12/50], Step [49/196], Loss: 0.0607\n",
      "Epoch [12/50], Step [99/196], Loss: 0.0476\n",
      "Epoch [12/50], Step [149/196], Loss: 0.0387\n",
      "=> Train Accuracy : 98.22%  |  Valid Accuracy : 83.93%\n",
      "==================================================\n",
      "Epoch [13/50], Step [49/196], Loss: 0.0346\n",
      "Epoch [13/50], Step [99/196], Loss: 0.0773\n",
      "Epoch [13/50], Step [149/196], Loss: 0.0507\n",
      "=> Train Accuracy : 98.194%  |  Valid Accuracy : 83.57%\n",
      "==================================================\n",
      "Epoch [14/50], Step [49/196], Loss: 0.0117\n",
      "Epoch [14/50], Step [99/196], Loss: 0.1501\n",
      "Epoch [14/50], Step [149/196], Loss: 0.1131\n",
      "=> Train Accuracy : 98.42%  |  Valid Accuracy : 83.77%\n",
      "==================================================\n",
      "Epoch [15/50], Step [49/196], Loss: 0.0276\n",
      "Epoch [15/50], Step [99/196], Loss: 0.0801\n",
      "Epoch [15/50], Step [149/196], Loss: 0.0338\n",
      "=> Train Accuracy : 98.298%  |  Valid Accuracy : 83.85%\n",
      "==================================================\n",
      "Epoch [16/50], Step [49/196], Loss: 0.3060\n",
      "Epoch [16/50], Step [99/196], Loss: 0.0482\n",
      "Epoch [16/50], Step [149/196], Loss: 0.0874\n",
      "=> Train Accuracy : 98.45%  |  Valid Accuracy : 84.1%\n",
      "==================================================\n",
      "Epoch [17/50], Step [49/196], Loss: 0.0310\n",
      "Epoch [17/50], Step [99/196], Loss: 0.0235\n",
      "Epoch [17/50], Step [149/196], Loss: 0.0438\n",
      "=> Train Accuracy : 98.348%  |  Valid Accuracy : 83.85%\n",
      "==================================================\n",
      "Epoch [18/50], Step [49/196], Loss: 0.0063\n",
      "Epoch [18/50], Step [99/196], Loss: 0.0256\n",
      "Epoch [18/50], Step [149/196], Loss: 0.0553\n",
      "=> Train Accuracy : 98.264%  |  Valid Accuracy : 83.6%\n",
      "==================================================\n",
      "Epoch [19/50], Step [49/196], Loss: 0.1339\n",
      "Epoch [19/50], Step [99/196], Loss: 0.1418\n",
      "Epoch [19/50], Step [149/196], Loss: 0.0303\n",
      "=> Train Accuracy : 98.186%  |  Valid Accuracy : 83.9%\n",
      "==================================================\n",
      "Epoch [20/50], Step [49/196], Loss: 0.0848\n",
      "Epoch [20/50], Step [99/196], Loss: 0.0502\n",
      "Epoch [20/50], Step [149/196], Loss: 0.0684\n",
      "=> Train Accuracy : 98.506%  |  Valid Accuracy : 83.3%\n",
      "==================================================\n",
      "Epoch [21/50], Step [49/196], Loss: 0.0423\n",
      "Epoch [21/50], Step [99/196], Loss: 0.0819\n",
      "Epoch [21/50], Step [149/196], Loss: 0.1301\n",
      "=> Train Accuracy : 98.372%  |  Valid Accuracy : 83.79%\n",
      "==================================================\n",
      "Epoch [22/50], Step [49/196], Loss: 0.0840\n",
      "Epoch [22/50], Step [99/196], Loss: 0.0473\n",
      "Epoch [22/50], Step [149/196], Loss: 0.0318\n",
      "=> Train Accuracy : 98.304%  |  Valid Accuracy : 83.63%\n",
      "==================================================\n",
      "Epoch [23/50], Step [49/196], Loss: 0.0656\n",
      "Epoch [23/50], Step [99/196], Loss: 0.0287\n",
      "Epoch [23/50], Step [149/196], Loss: 0.0272\n",
      "=> Train Accuracy : 98.294%  |  Valid Accuracy : 83.15%\n",
      "==================================================\n",
      "Epoch [24/50], Step [49/196], Loss: 0.0264\n",
      "Epoch [24/50], Step [99/196], Loss: 0.0897\n",
      "Epoch [24/50], Step [149/196], Loss: 0.0137\n",
      "=> Train Accuracy : 98.162%  |  Valid Accuracy : 83.32%\n",
      "==================================================\n",
      "Epoch [25/50], Step [49/196], Loss: 0.1012\n",
      "Epoch [25/50], Step [99/196], Loss: 0.0146\n",
      "Epoch [25/50], Step [149/196], Loss: 0.0667\n",
      "=> Train Accuracy : 98.186%  |  Valid Accuracy : 83.46%\n",
      "==================================================\n",
      "Epoch [26/50], Step [49/196], Loss: 0.1097\n",
      "Epoch [26/50], Step [99/196], Loss: 0.0149\n",
      "Epoch [26/50], Step [149/196], Loss: 0.0992\n",
      "=> Train Accuracy : 98.27%  |  Valid Accuracy : 83.76%\n",
      "==================================================\n",
      "Epoch [27/50], Step [49/196], Loss: 0.0883\n",
      "Epoch [27/50], Step [99/196], Loss: 0.0568\n",
      "Epoch [27/50], Step [149/196], Loss: 0.1095\n",
      "=> Train Accuracy : 98.248%  |  Valid Accuracy : 83.53%\n",
      "==================================================\n",
      "Epoch [28/50], Step [49/196], Loss: 0.0386\n",
      "Epoch [28/50], Step [99/196], Loss: 0.1349\n",
      "Epoch [28/50], Step [149/196], Loss: 0.0305\n",
      "=> Train Accuracy : 98.386%  |  Valid Accuracy : 83.58%\n",
      "==================================================\n",
      "Epoch [29/50], Step [49/196], Loss: 0.0137\n",
      "Epoch [29/50], Step [99/196], Loss: 0.1234\n",
      "Epoch [29/50], Step [149/196], Loss: 0.0756\n",
      "=> Train Accuracy : 98.338%  |  Valid Accuracy : 83.64%\n",
      "==================================================\n",
      "Epoch [30/50], Step [49/196], Loss: 0.0593\n",
      "Epoch [30/50], Step [99/196], Loss: 0.0376\n",
      "Epoch [30/50], Step [149/196], Loss: 0.1030\n",
      "=> Train Accuracy : 98.242%  |  Valid Accuracy : 83.87%\n",
      "==================================================\n",
      "Epoch [31/50], Step [49/196], Loss: 0.0984\n",
      "Epoch [31/50], Step [99/196], Loss: 0.0096\n",
      "Epoch [31/50], Step [149/196], Loss: 0.0741\n",
      "=> Train Accuracy : 98.402%  |  Valid Accuracy : 83.86%\n",
      "==================================================\n",
      "Epoch [32/50], Step [49/196], Loss: 0.0439\n",
      "Epoch [32/50], Step [99/196], Loss: 0.0395\n",
      "Epoch [32/50], Step [149/196], Loss: 0.0655\n",
      "=> Train Accuracy : 98.306%  |  Valid Accuracy : 83.35%\n",
      "==================================================\n",
      "Epoch [33/50], Step [49/196], Loss: 0.0934\n",
      "Epoch [33/50], Step [99/196], Loss: 0.0323\n",
      "Epoch [33/50], Step [149/196], Loss: 0.0217\n",
      "=> Train Accuracy : 98.298%  |  Valid Accuracy : 83.43%\n",
      "==================================================\n",
      "Epoch [34/50], Step [49/196], Loss: 0.0376\n",
      "Epoch [34/50], Step [99/196], Loss: 0.0115\n",
      "Epoch [34/50], Step [149/196], Loss: 0.1081\n",
      "=> Train Accuracy : 98.386%  |  Valid Accuracy : 83.89%\n",
      "==================================================\n",
      "Epoch [35/50], Step [49/196], Loss: 0.0960\n",
      "Epoch [35/50], Step [99/196], Loss: 0.0336\n",
      "Epoch [35/50], Step [149/196], Loss: 0.0232\n",
      "=> Train Accuracy : 98.34%  |  Valid Accuracy : 83.08%\n",
      "==================================================\n",
      "Epoch [36/50], Step [49/196], Loss: 0.0637\n",
      "Epoch [36/50], Step [99/196], Loss: 0.0588\n",
      "Epoch [36/50], Step [149/196], Loss: 0.0250\n",
      "=> Train Accuracy : 98.156%  |  Valid Accuracy : 83.65%\n",
      "==================================================\n",
      "Epoch [37/50], Step [49/196], Loss: 0.0169\n",
      "Epoch [37/50], Step [99/196], Loss: 0.0325\n",
      "Epoch [37/50], Step [149/196], Loss: 0.0103\n",
      "=> Train Accuracy : 98.274%  |  Valid Accuracy : 83.47%\n",
      "==================================================\n",
      "Epoch [38/50], Step [49/196], Loss: 0.1320\n",
      "Epoch [38/50], Step [99/196], Loss: 0.0382\n",
      "Epoch [38/50], Step [149/196], Loss: 0.1002\n",
      "=> Train Accuracy : 98.36%  |  Valid Accuracy : 83.73%\n",
      "==================================================\n",
      "Epoch [39/50], Step [49/196], Loss: 0.0252\n",
      "Epoch [39/50], Step [99/196], Loss: 0.0473\n",
      "Epoch [39/50], Step [149/196], Loss: 0.1149\n",
      "=> Train Accuracy : 98.406%  |  Valid Accuracy : 83.4%\n",
      "==================================================\n",
      "Epoch [40/50], Step [49/196], Loss: 0.0241\n",
      "Epoch [40/50], Step [99/196], Loss: 0.0899\n",
      "Epoch [40/50], Step [149/196], Loss: 0.0670\n",
      "=> Train Accuracy : 98.412%  |  Valid Accuracy : 83.94%\n",
      "==================================================\n",
      "Epoch [41/50], Step [49/196], Loss: 0.0316\n",
      "Epoch [41/50], Step [99/196], Loss: 0.1539\n",
      "Epoch [41/50], Step [149/196], Loss: 0.0413\n",
      "=> Train Accuracy : 98.316%  |  Valid Accuracy : 83.48%\n",
      "==================================================\n",
      "Epoch [42/50], Step [49/196], Loss: 0.0758\n",
      "Epoch [42/50], Step [99/196], Loss: 0.0929\n",
      "Epoch [42/50], Step [149/196], Loss: 0.1549\n",
      "=> Train Accuracy : 98.354%  |  Valid Accuracy : 83.62%\n",
      "==================================================\n",
      "Epoch [43/50], Step [49/196], Loss: 0.0357\n",
      "Epoch [43/50], Step [99/196], Loss: 0.1396\n",
      "Epoch [43/50], Step [149/196], Loss: 0.1644\n",
      "=> Train Accuracy : 98.418%  |  Valid Accuracy : 83.55%\n",
      "==================================================\n",
      "Epoch [44/50], Step [49/196], Loss: 0.0343\n",
      "Epoch [44/50], Step [99/196], Loss: 0.1024\n",
      "Epoch [44/50], Step [149/196], Loss: 0.1395\n",
      "=> Train Accuracy : 98.262%  |  Valid Accuracy : 83.35%\n",
      "==================================================\n",
      "Epoch [45/50], Step [49/196], Loss: 0.0492\n",
      "Epoch [45/50], Step [99/196], Loss: 0.0481\n",
      "Epoch [45/50], Step [149/196], Loss: 0.0183\n",
      "=> Train Accuracy : 98.344%  |  Valid Accuracy : 83.5%\n",
      "==================================================\n",
      "Epoch [46/50], Step [49/196], Loss: 0.0351\n",
      "Epoch [46/50], Step [99/196], Loss: 0.0659\n",
      "Epoch [46/50], Step [149/196], Loss: 0.0308\n",
      "=> Train Accuracy : 98.282%  |  Valid Accuracy : 83.84%\n",
      "==================================================\n",
      "Epoch [47/50], Step [49/196], Loss: 0.1364\n",
      "Epoch [47/50], Step [99/196], Loss: 0.0482\n",
      "Epoch [47/50], Step [149/196], Loss: 0.0176\n",
      "=> Train Accuracy : 98.224%  |  Valid Accuracy : 83.58%\n",
      "==================================================\n",
      "Epoch [48/50], Step [49/196], Loss: 0.0199\n",
      "Epoch [48/50], Step [99/196], Loss: 0.0741\n",
      "Epoch [48/50], Step [149/196], Loss: 0.0641\n",
      "=> Train Accuracy : 98.176%  |  Valid Accuracy : 83.69%\n",
      "==================================================\n",
      "Epoch [49/50], Step [49/196], Loss: 0.1580\n",
      "Epoch [49/50], Step [99/196], Loss: 0.0421\n",
      "Epoch [49/50], Step [149/196], Loss: 0.0400\n",
      "=> Train Accuracy : 98.312%  |  Valid Accuracy : 83.47%\n",
      "==================================================\n",
      "Epoch [50/50], Step [49/196], Loss: 0.0481\n",
      "Epoch [50/50], Step [99/196], Loss: 0.0956\n",
      "Epoch [50/50], Step [149/196], Loss: 0.0522\n",
      "=> Train Accuracy : 98.274%  |  Valid Accuracy : 83.53%\n",
      "==================================================\n",
      "\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Compile the mode with the necessary loss function and optimizer.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "print ('Current device : ', device)\n",
    "\n",
    "total_step = len(trainloader)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, classifier_scl.parameters()), lr=learning_rate)\n",
    "\n",
    "# Train the model.\n",
    "print(\"resnet classifier_scl model training...\")\n",
    "\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "valid_loss = []\n",
    "valid_acc = []\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    running_loss = 0\n",
    "    classifier_scl.train()\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = classifier_scl(inputs)  # torch.Size([256, 10])\n",
    "        predictions = torch.argmax(F.softmax(outputs, dim=-1), dim=-1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if (i+1)%50 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i, total_step, loss.item())) \n",
    "            \n",
    "    train_loss.append(loss)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_acc.append(train_accuracy)\n",
    "    \n",
    "    valid_correct = 0\n",
    "    valid_total = 0\n",
    "    for i, test_data in enumerate(testloader):\n",
    "        classifier_scl.eval()\n",
    "        \n",
    "        test_inputs, test_labels = test_data\n",
    "\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "        \n",
    "        test_outputs = classifier_scl(test_inputs)\n",
    "        val_loss = criterion(test_outputs, test_labels)\n",
    "        \n",
    "        test_predictions = torch.argmax(F.softmax(test_outputs, dim=-1), dim=-1)\n",
    "        valid_total += test_labels.size(0)\n",
    "        valid_correct += (test_predictions == test_labels).sum().item()\n",
    "    \n",
    "    valid_loss.append(val_loss)\n",
    "    valid_accuracy = 100 * valid_correct / valid_total\n",
    "    valid_acc.append(valid_accuracy)\n",
    "    print(f'=> Train Accuracy : {train_accuracy}%  |  Valid Accuracy : {valid_accuracy}%')\n",
    "    print(\"==================================================\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "inv_history = {'train_loss': train_loss,\n",
    "               'valid_loss': valid_loss,\n",
    "               'train_acc': train_acc,\n",
    "               'valid_acc': valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii4BdorgWuLe"
   },
   "source": [
    "We get to an improved test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ5KT626WuLf"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "As shown in the experiments, using the supervised contrastive learning technique\n",
    "outperformed the conventional technique in terms of the test accuracy. Note that\n",
    "the same training budget (i.e., number of epochs) was given to each technique.\n",
    "Supervised contrastive learning pays off when the encoder involves a complex\n",
    "architecture, like ResNet, and multi-class problems with many labels.\n",
    "In addition, large batch sizes and multi-layer projection heads\n",
    "improve its effectiveness. See the [Supervised Contrastive Learning](https://arxiv.org/abs/2004.11362)\n",
    "paper for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model : ResNet18\n",
    "\n",
    "#### Experiment 1 (End-to-end)\n",
    "\n",
    "* Training time : 07:04\n",
    "* Train Accuracy : 97.974%  \n",
    "* Valid Accuracy : 83.13%\n",
    "\n",
    "#### Experiment 2 (Two-phase)\n",
    "\n",
    "* Training time : 07:00 + 03:57 \n",
    "* Train Accuracy : 98.274% \n",
    "* Valid Accuracy : 83.53%\n",
    "\n",
    "##### >  최종 모델의 크기 및 다른 설정은 동일, Experiment 2 에서 약간의 성능 향상\n",
    "##### >  Contrastive loss로 feature representation을 잘 학습했다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "supervised-contrastive-learning",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
