{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-wellington",
   "metadata": {},
   "source": [
    "## Text generation with a miniature GPT\n",
    "\n",
    "### Introduction\n",
    "This example demonstrates how to implement an autoregressive language model using a miniature version of the GPT model. The model consists of a single Transformer block with causal masking in its attention layer. We use the text from the IMDB sentiment classification dataset for training and generate new movie reviews for a given prompt. When using this script with your own dataset, make sure it has at least 1 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-people",
   "metadata": {},
   "source": [
    "### Prepare the data for word-level language modelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sunset-damage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "# label -> pos : 1, neg : 0 \n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"datasets/aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"datasets/aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [1] * len(pos_texts) + [0] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    # sampling 후 reset_index : index 초기화 (https://yganalyst.github.io/data_handling/Pd_2/)\n",
    "    # 두 데이터 프레임을 합치면서 index를 0부터 초기화, drop=True : 기존 index를 버림\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "awful-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Last Hunt is one of the few westerns ever ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I usually try to construct reasonably well-arg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Preston Waters is off to a bad summer. Besides...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the single worst movie I have ever see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(some spoilers) - as if you wouldn't know how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  The Last Hunt is one of the few westerns ever ...          1\n",
       "1  I usually try to construct reasonably well-arg...          0\n",
       "2  Preston Waters is off to a bad summer. Besides...          1\n",
       "3  This is the single worst movie I have ever see...          0\n",
       "4  (some spoilers) - as if you wouldn't know how ...          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "settled-printing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train :  25000\n",
      "# of test :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of train : \", len(train_df))\n",
    "print(\"# of test : \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "military-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.sample(5000)\n",
    "# test_df = test_df.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-replication",
   "metadata": {},
   "source": [
    "### Build Vocab and Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "directed-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor -> bytes(from numpy) -> string\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\").numpy().decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clear-asbestos",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an interesting idea for a film , both show'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_standardization('An interesting idea for a film, both show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "republican-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for i in range(len(train_df.review.values)):\n",
    "    if (i+1) % (len(train_df.review.values)/10) == 0:\n",
    "        print(f'{(i+1)/(len(train_df)/100)} % Done')\n",
    "    tokens = custom_standardization((train_df.review.values[i])).split()\n",
    "    corpus.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "talented-affect",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'few',\n",
       " 'westerns',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'buffalo',\n",
       " 'hunting',\n",
       " ',',\n",
       " 'both',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sport',\n",
       " 'and',\n",
       " 'business',\n",
       " 'and',\n",
       " 'as',\n",
       " 'a',\n",
       " 'method',\n",
       " 'of',\n",
       " 'winning',\n",
       " 'the',\n",
       " 'plains',\n",
       " 'indian',\n",
       " 'wars',\n",
       " '.',\n",
       " 'before',\n",
       " 'the',\n",
       " 'white',\n",
       " 'man',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mississippi',\n",
       " ',',\n",
       " 'the',\n",
       " 'plains',\n",
       " 'used',\n",
       " 'to',\n",
       " 'have',\n",
       " 'herds',\n",
       " 'of',\n",
       " 'american',\n",
       " 'bison',\n",
       " 'as',\n",
       " 'large',\n",
       " 'as',\n",
       " 'some',\n",
       " 'of',\n",
       " 'our',\n",
       " 'largest',\n",
       " 'cities',\n",
       " '.',\n",
       " 'by',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'period',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'set',\n",
       " 'in',\n",
       " ',',\n",
       " 'the',\n",
       " 'buffalo',\n",
       " 'had',\n",
       " 'been',\n",
       " 'all',\n",
       " 'but',\n",
       " 'wiped',\n",
       " 'out',\n",
       " '.',\n",
       " 'the',\n",
       " '20th',\n",
       " 'century',\n",
       " ',',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'efforts',\n",
       " 'of',\n",
       " 'conservationists',\n",
       " ',',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'revival',\n",
       " 'in',\n",
       " 'population',\n",
       " 'of',\n",
       " 'the',\n",
       " 'species',\n",
       " ',',\n",
       " 'but',\n",
       " 'not',\n",
       " 'hardly',\n",
       " 'like',\n",
       " 'it',\n",
       " 'once',\n",
       " 'was',\n",
       " '.',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " 'and',\n",
       " 'stewart',\n",
       " 'granger',\n",
       " 'are',\n",
       " 'co',\n",
       " '-starring',\n",
       " 'in',\n",
       " 'a',\n",
       " 'second',\n",
       " 'film',\n",
       " 'together',\n",
       " 'and',\n",
       " 'this',\n",
       " 'one',\n",
       " 'is',\n",
       " 'far',\n",
       " 'superior',\n",
       " 'to',\n",
       " 'all',\n",
       " 'the',\n",
       " 'brothers',\n",
       " 'were',\n",
       " 'valiant',\n",
       " '.',\n",
       " 'here',\n",
       " 'stewart',\n",
       " 'granger',\n",
       " 'is',\n",
       " 'the',\n",
       " 'good',\n",
       " 'guy',\n",
       " ',',\n",
       " 'a',\n",
       " 'world',\n",
       " 'weary',\n",
       " 'buffalo',\n",
       " 'hunter',\n",
       " ',',\n",
       " 'who',\n",
       " 'has',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'a',\n",
       " 'job',\n",
       " 'he',\n",
       " 'hates',\n",
       " 'because',\n",
       " 'of',\n",
       " 'financial',\n",
       " 'considerations',\n",
       " '.',\n",
       " 'the',\n",
       " 'partner',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'chosen',\n",
       " 'to',\n",
       " 'throw',\n",
       " 'in',\n",
       " 'with',\n",
       " 'is',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " '.',\n",
       " 'forgetting',\n",
       " 'taylor',\n",
       " 'for',\n",
       " 'the',\n",
       " 'moment',\n",
       " ',',\n",
       " 'i',\n",
       " 'doubt',\n",
       " 'if',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'ever',\n",
       " 'been',\n",
       " 'a',\n",
       " 'meaner',\n",
       " ',',\n",
       " 'nastier',\n",
       " 'soul',\n",
       " 'than',\n",
       " 'charlie',\n",
       " 'gilsen',\n",
       " 'who',\n",
       " 'taylor',\n",
       " 'portrays',\n",
       " '.',\n",
       " 'in',\n",
       " 'devil',\n",
       " \"'s\",\n",
       " 'doorway',\n",
       " 'he',\n",
       " 'was',\n",
       " 'an',\n",
       " 'american',\n",
       " 'indian',\n",
       " 'fighting',\n",
       " 'against',\n",
       " 'the',\n",
       " 'prejudice',\n",
       " 'stirred',\n",
       " 'up',\n",
       " 'by',\n",
       " 'a',\n",
       " 'racist',\n",
       " 'played',\n",
       " 'by',\n",
       " 'louis',\n",
       " 'calhern',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " ',',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'racist',\n",
       " 'here',\n",
       " '.',\n",
       " 'he',\n",
       " 'kills',\n",
       " 'both',\n",
       " 'buffalo',\n",
       " 'and',\n",
       " 'indians',\n",
       " 'for',\n",
       " 'pure',\n",
       " 'pleasure',\n",
       " '.',\n",
       " 'he',\n",
       " 'kills',\n",
       " 'one',\n",
       " 'indian',\n",
       " 'family',\n",
       " 'when',\n",
       " 'they',\n",
       " 'steal',\n",
       " 'his',\n",
       " 'mules',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'widow',\n",
       " 'of',\n",
       " 'one',\n",
       " 'captive',\n",
       " '.',\n",
       " 'like',\n",
       " 'some',\n",
       " 'barbarian',\n",
       " 'conqueror',\n",
       " 'he',\n",
       " 'expects',\n",
       " 'the',\n",
       " 'pleasure',\n",
       " 'of',\n",
       " 'debra',\n",
       " 'paget',\n",
       " \"'s\",\n",
       " 'sexual',\n",
       " 'favors',\n",
       " '.',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'actually',\n",
       " 'mad',\n",
       " 'when',\n",
       " 'paget',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'see',\n",
       " 'it',\n",
       " 'that',\n",
       " 'way',\n",
       " '.',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'how',\n",
       " 'often',\n",
       " 'they',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'russ',\n",
       " 'tamblyn',\n",
       " 'as',\n",
       " 'a',\n",
       " 'halfbreed',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'never',\n",
       " 'really',\n",
       " 'convinced',\n",
       " 'he',\n",
       " 'was',\n",
       " 'any',\n",
       " 'part',\n",
       " 'indian',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'only',\n",
       " 'weakness',\n",
       " 'i',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " '.',\n",
       " 'however',\n",
       " 'lloyd',\n",
       " 'nolan',\n",
       " ',',\n",
       " 'the',\n",
       " 'grizzled',\n",
       " 'old',\n",
       " 'buffalo',\n",
       " 'skinner',\n",
       " 'taylor',\n",
       " 'and',\n",
       " 'granger',\n",
       " 'bring',\n",
       " 'along',\n",
       " 'is',\n",
       " 'just',\n",
       " 'great',\n",
       " '.',\n",
       " 'nolan',\n",
       " 'steals',\n",
       " 'every',\n",
       " 'scene',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'in',\n",
       " 'with',\n",
       " 'the',\n",
       " 'cast',\n",
       " '.',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'like',\n",
       " 'their',\n",
       " 'westerns',\n",
       " 'real',\n",
       " ',',\n",
       " 'who',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " 'side',\n",
       " 'of',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'on',\n",
       " 'screen',\n",
       " ',',\n",
       " 'and',\n",
       " 'who',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'like',\n",
       " 'cheap',\n",
       " 'heroics',\n",
       " ',',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'hunt',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "healthy-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "corpus_flat=list(chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_flat = sum(corpus, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hindu-military",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'few',\n",
       " 'westerns',\n",
       " 'ever',\n",
       " 'made',\n",
       " 'to',\n",
       " 'deal',\n",
       " 'with',\n",
       " 'buffalo',\n",
       " 'hunting',\n",
       " ',',\n",
       " 'both',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sport',\n",
       " 'and',\n",
       " 'business',\n",
       " 'and',\n",
       " 'as',\n",
       " 'a',\n",
       " 'method',\n",
       " 'of',\n",
       " 'winning',\n",
       " 'the',\n",
       " 'plains',\n",
       " 'indian',\n",
       " 'wars',\n",
       " '.',\n",
       " 'before',\n",
       " 'the',\n",
       " 'white',\n",
       " 'man',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mississippi',\n",
       " ',',\n",
       " 'the',\n",
       " 'plains',\n",
       " 'used',\n",
       " 'to',\n",
       " 'have',\n",
       " 'herds',\n",
       " 'of',\n",
       " 'american',\n",
       " 'bison',\n",
       " 'as',\n",
       " 'large',\n",
       " 'as',\n",
       " 'some',\n",
       " 'of',\n",
       " 'our',\n",
       " 'largest',\n",
       " 'cities',\n",
       " '.',\n",
       " 'by',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'the',\n",
       " 'period',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'set',\n",
       " 'in',\n",
       " ',',\n",
       " 'the',\n",
       " 'buffalo',\n",
       " 'had',\n",
       " 'been',\n",
       " 'all',\n",
       " 'but',\n",
       " 'wiped',\n",
       " 'out',\n",
       " '.',\n",
       " 'the',\n",
       " '20th',\n",
       " 'century',\n",
       " ',',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'efforts',\n",
       " 'of',\n",
       " 'conservationists',\n",
       " ',',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'revival',\n",
       " 'in',\n",
       " 'population',\n",
       " 'of',\n",
       " 'the',\n",
       " 'species',\n",
       " ',',\n",
       " 'but',\n",
       " 'not',\n",
       " 'hardly',\n",
       " 'like',\n",
       " 'it',\n",
       " 'once',\n",
       " 'was',\n",
       " '.',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " 'and',\n",
       " 'stewart',\n",
       " 'granger',\n",
       " 'are',\n",
       " 'co',\n",
       " '-starring',\n",
       " 'in',\n",
       " 'a',\n",
       " 'second',\n",
       " 'film',\n",
       " 'together',\n",
       " 'and',\n",
       " 'this',\n",
       " 'one',\n",
       " 'is',\n",
       " 'far',\n",
       " 'superior',\n",
       " 'to',\n",
       " 'all',\n",
       " 'the',\n",
       " 'brothers',\n",
       " 'were',\n",
       " 'valiant',\n",
       " '.',\n",
       " 'here',\n",
       " 'stewart',\n",
       " 'granger',\n",
       " 'is',\n",
       " 'the',\n",
       " 'good',\n",
       " 'guy',\n",
       " ',',\n",
       " 'a',\n",
       " 'world',\n",
       " 'weary',\n",
       " 'buffalo',\n",
       " 'hunter',\n",
       " ',',\n",
       " 'who',\n",
       " 'has',\n",
       " 'to',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'a',\n",
       " 'job',\n",
       " 'he',\n",
       " 'hates',\n",
       " 'because',\n",
       " 'of',\n",
       " 'financial',\n",
       " 'considerations',\n",
       " '.',\n",
       " 'the',\n",
       " 'partner',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'chosen',\n",
       " 'to',\n",
       " 'throw',\n",
       " 'in',\n",
       " 'with',\n",
       " 'is',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " '.',\n",
       " 'forgetting',\n",
       " 'taylor',\n",
       " 'for',\n",
       " 'the',\n",
       " 'moment',\n",
       " ',',\n",
       " 'i',\n",
       " 'doubt',\n",
       " 'if',\n",
       " 'there',\n",
       " \"'s\",\n",
       " 'ever',\n",
       " 'been',\n",
       " 'a',\n",
       " 'meaner',\n",
       " ',',\n",
       " 'nastier',\n",
       " 'soul',\n",
       " 'than',\n",
       " 'charlie',\n",
       " 'gilsen',\n",
       " 'who',\n",
       " 'taylor',\n",
       " 'portrays',\n",
       " '.',\n",
       " 'in',\n",
       " 'devil',\n",
       " \"'s\",\n",
       " 'doorway',\n",
       " 'he',\n",
       " 'was',\n",
       " 'an',\n",
       " 'american',\n",
       " 'indian',\n",
       " 'fighting',\n",
       " 'against',\n",
       " 'the',\n",
       " 'prejudice',\n",
       " 'stirred',\n",
       " 'up',\n",
       " 'by',\n",
       " 'a',\n",
       " 'racist',\n",
       " 'played',\n",
       " 'by',\n",
       " 'louis',\n",
       " 'calhern',\n",
       " '.',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " ',',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'racist',\n",
       " 'here',\n",
       " '.',\n",
       " 'he',\n",
       " 'kills',\n",
       " 'both',\n",
       " 'buffalo',\n",
       " 'and',\n",
       " 'indians',\n",
       " 'for',\n",
       " 'pure',\n",
       " 'pleasure',\n",
       " '.',\n",
       " 'he',\n",
       " 'kills',\n",
       " 'one',\n",
       " 'indian',\n",
       " 'family',\n",
       " 'when',\n",
       " 'they',\n",
       " 'steal',\n",
       " 'his',\n",
       " 'mules',\n",
       " 'and',\n",
       " 'takes',\n",
       " 'the',\n",
       " 'widow',\n",
       " 'of',\n",
       " 'one',\n",
       " 'captive',\n",
       " '.',\n",
       " 'like',\n",
       " 'some',\n",
       " 'barbarian',\n",
       " 'conqueror',\n",
       " 'he',\n",
       " 'expects',\n",
       " 'the',\n",
       " 'pleasure',\n",
       " 'of',\n",
       " 'debra',\n",
       " 'paget',\n",
       " \"'s\",\n",
       " 'sexual',\n",
       " 'favors',\n",
       " '.',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'actually',\n",
       " 'mad',\n",
       " 'when',\n",
       " 'paget',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'see',\n",
       " 'it',\n",
       " 'that',\n",
       " 'way',\n",
       " '.',\n",
       " 'no',\n",
       " 'matter',\n",
       " 'how',\n",
       " 'often',\n",
       " 'they',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'russ',\n",
       " 'tamblyn',\n",
       " 'as',\n",
       " 'a',\n",
       " 'halfbreed',\n",
       " ',',\n",
       " 'i',\n",
       " 'was',\n",
       " 'never',\n",
       " 'really',\n",
       " 'convinced',\n",
       " 'he',\n",
       " 'was',\n",
       " 'any',\n",
       " 'part',\n",
       " 'indian',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'only',\n",
       " 'weakness',\n",
       " 'i',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " '.',\n",
       " 'however',\n",
       " 'lloyd',\n",
       " 'nolan',\n",
       " ',',\n",
       " 'the',\n",
       " 'grizzled',\n",
       " 'old',\n",
       " 'buffalo',\n",
       " 'skinner',\n",
       " 'taylor',\n",
       " 'and',\n",
       " 'granger',\n",
       " 'bring',\n",
       " 'along',\n",
       " 'is',\n",
       " 'just',\n",
       " 'great',\n",
       " '.',\n",
       " 'nolan',\n",
       " 'steals',\n",
       " 'every',\n",
       " 'scene',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'in',\n",
       " 'with',\n",
       " 'the',\n",
       " 'cast',\n",
       " '.',\n",
       " 'for',\n",
       " 'those',\n",
       " 'who',\n",
       " 'like',\n",
       " 'their',\n",
       " 'westerns',\n",
       " 'real',\n",
       " ',',\n",
       " 'who',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'a',\n",
       " 'side',\n",
       " 'of',\n",
       " 'robert',\n",
       " 'taylor',\n",
       " 'never',\n",
       " 'seen',\n",
       " 'on',\n",
       " 'screen',\n",
       " ',',\n",
       " 'and',\n",
       " 'who',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'like',\n",
       " 'cheap',\n",
       " 'heroics',\n",
       " ',',\n",
       " 'the',\n",
       " 'last',\n",
       " 'hunt',\n",
       " 'is',\n",
       " 'the',\n",
       " 'ideal',\n",
       " 'hunt',\n",
       " '.',\n",
       " 'i',\n",
       " 'usually',\n",
       " 'try',\n",
       " 'to',\n",
       " 'construct',\n",
       " 'reasonably',\n",
       " 'well',\n",
       " '-argued',\n",
       " 'critiques',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'can',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'this',\n",
       " 'got',\n",
       " 'past',\n",
       " 'the',\n",
       " 'script',\n",
       " 'stage',\n",
       " '.',\n",
       " 'the',\n",
       " 'dialogue',\n",
       " 'is',\n",
       " 'appalling',\n",
       " ',',\n",
       " 'the',\n",
       " 'acting',\n",
       " 'very',\n",
       " 'dodgy',\n",
       " ',',\n",
       " 'the',\n",
       " 'accents',\n",
       " 'just',\n",
       " 'awful',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'direction',\n",
       " 'and',\n",
       " 'pacing',\n",
       " 'is',\n",
       " 'scrappy',\n",
       " 'at',\n",
       " 'best',\n",
       " '.',\n",
       " 'i',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'remember',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'a',\n",
       " 'film',\n",
       " 'quite',\n",
       " 'this',\n",
       " 'bad',\n",
       " '.',\n",
       " 'joseph',\n",
       " 'fiennes',\n",
       " ',',\n",
       " 'pretty',\n",
       " 'as',\n",
       " 'he',\n",
       " 'is',\n",
       " ',',\n",
       " 'might',\n",
       " 'just',\n",
       " 'have',\n",
       " 'killed',\n",
       " 'his',\n",
       " 'career',\n",
       " 'as',\n",
       " 'quickly',\n",
       " 'as',\n",
       " 'it',\n",
       " 'started',\n",
       " '.',\n",
       " 'the',\n",
       " 'island',\n",
       " 'of',\n",
       " 'doctor',\n",
       " 'moreau',\n",
       " 'was',\n",
       " 'no',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'this',\n",
       " 'garbage',\n",
       " '.',\n",
       " 'preston',\n",
       " 'waters',\n",
       " 'is',\n",
       " 'off',\n",
       " 'to',\n",
       " 'a',\n",
       " 'bad',\n",
       " 'summer',\n",
       " '.',\n",
       " 'besides',\n",
       " 'his',\n",
       " 'birthday',\n",
       " 'coming',\n",
       " 'up',\n",
       " ',',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'looks',\n",
       " 'promising',\n",
       " '.',\n",
       " 'first',\n",
       " 'he',\n",
       " 'has',\n",
       " 'to',\n",
       " 'share',\n",
       " 'his',\n",
       " 'own',\n",
       " 'room',\n",
       " 'with',\n",
       " 'his',\n",
       " 'brothers',\n",
       " 'who',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'run',\n",
       " 'a',\n",
       " 'business',\n",
       " '.',\n",
       " 'they',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'do',\n",
       " 'it',\n",
       " 'in',\n",
       " 'their',\n",
       " 'rooms',\n",
       " 'because',\n",
       " 'they',\n",
       " 'don',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'enough',\n",
       " 'space',\n",
       " '.',\n",
       " 'off',\n",
       " 'to',\n",
       " 'a',\n",
       " 'birthday',\n",
       " 'party',\n",
       " 'he',\n",
       " 'only',\n",
       " 'gets',\n",
       " '$6',\n",
       " 'tokens',\n",
       " 'while',\n",
       " 'others',\n",
       " 'get',\n",
       " '$32',\n",
       " ',',\n",
       " '$35',\n",
       " ',',\n",
       " 'and',\n",
       " 'even',\n",
       " '$50',\n",
       " '.',\n",
       " 'when',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'birthday',\n",
       " 'cards',\n",
       " 'comes',\n",
       " 'early',\n",
       " ',',\n",
       " 'he',\n",
       " 'only',\n",
       " 'gets',\n",
       " 'a',\n",
       " 'check',\n",
       " 'made',\n",
       " 'out',\n",
       " 'for',\n",
       " '$11',\n",
       " '.',\n",
       " 'going',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'he',\n",
       " 'learns',\n",
       " 'he',\n",
       " 'needs',\n",
       " '$200',\n",
       " 'at',\n",
       " 'least',\n",
       " 'to',\n",
       " 'start',\n",
       " 'an',\n",
       " 'account',\n",
       " '.',\n",
       " 'leaving',\n",
       " 'the',\n",
       " 'bank',\n",
       " ',',\n",
       " 'a',\n",
       " 'bully',\n",
       " 'steals',\n",
       " 'the',\n",
       " 'check',\n",
       " '.',\n",
       " 'pursuing',\n",
       " 'after',\n",
       " 'the',\n",
       " 'kid',\n",
       " 'nearly',\n",
       " 'gets',\n",
       " 'him',\n",
       " 'run',\n",
       " 'over',\n",
       " '(definately',\n",
       " 'his',\n",
       " 'bike',\n",
       " 'gets',\n",
       " 'ruined',\n",
       " ')',\n",
       " 'by',\n",
       " 'a',\n",
       " 'criminal',\n",
       " 'named',\n",
       " 'quigley',\n",
       " '(played',\n",
       " 'by',\n",
       " 'miguel',\n",
       " 'ferrer',\n",
       " ')',\n",
       " '.',\n",
       " 'quigley',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'come',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'too',\n",
       " 'from',\n",
       " 'giving',\n",
       " 'the',\n",
       " 'owner',\n",
       " '$1',\n",
       " ',000',\n",
       " ',000',\n",
       " 'to',\n",
       " 'give',\n",
       " 'to',\n",
       " 'this',\n",
       " 'guy',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'quigley',\n",
       " 'starts',\n",
       " 'to',\n",
       " 'write',\n",
       " 'a',\n",
       " 'check',\n",
       " 'for',\n",
       " 'the',\n",
       " 'damage',\n",
       " ',',\n",
       " 'but',\n",
       " 'only',\n",
       " 'succeeds',\n",
       " 'in',\n",
       " 'writing',\n",
       " 'his',\n",
       " 'name',\n",
       " 'before',\n",
       " 'a',\n",
       " 'police',\n",
       " 'car',\n",
       " 'circles',\n",
       " 'the',\n",
       " 'area',\n",
       " '.',\n",
       " 'afraid',\n",
       " ',',\n",
       " 'he',\n",
       " 'gives',\n",
       " 'preston',\n",
       " 'the',\n",
       " 'check',\n",
       " ',',\n",
       " 'informs',\n",
       " 'him',\n",
       " 'to',\n",
       " 'give',\n",
       " 'it',\n",
       " 'to',\n",
       " 'his',\n",
       " 'dad',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'out',\n",
       " '.',\n",
       " 'that',\n",
       " 'evening',\n",
       " 'though',\n",
       " ',',\n",
       " 'preston',\n",
       " 'tells',\n",
       " 'his',\n",
       " 'dad',\n",
       " 'that',\n",
       " 'he',\n",
       " 'doesn',\n",
       " \"'t\",\n",
       " 'want',\n",
       " 'a',\n",
       " 'new',\n",
       " 'bike',\n",
       " ',',\n",
       " 'he',\n",
       " 'wants',\n",
       " 'his',\n",
       " 'own',\n",
       " 'room',\n",
       " 'back',\n",
       " ',',\n",
       " 'better',\n",
       " 'yet',\n",
       " 'his',\n",
       " 'own',\n",
       " 'house',\n",
       " '.',\n",
       " 'his',\n",
       " 'father',\n",
       " 'confines',\n",
       " 'him',\n",
       " 'to',\n",
       " 'his',\n",
       " 'room',\n",
       " 'for',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'evening',\n",
       " '.',\n",
       " 'moping',\n",
       " 'in',\n",
       " 'his',\n",
       " 'room',\n",
       " 'preston',\n",
       " 'figures',\n",
       " 'things',\n",
       " 'can',\n",
       " \"'t\",\n",
       " 'get',\n",
       " 'any',\n",
       " 'worse',\n",
       " 'he',\n",
       " 'realizes',\n",
       " 'that',\n",
       " 'he',\n",
       " 'forgot',\n",
       " 'about',\n",
       " 'the',\n",
       " 'check',\n",
       " 'as',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'still',\n",
       " 'in',\n",
       " 'his',\n",
       " 'shirt',\n",
       " 'pocket',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'blank',\n",
       " '.',\n",
       " 'using',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'and',\n",
       " 'after',\n",
       " 'careful',\n",
       " 'consideration',\n",
       " ',',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'it',\n",
       " 'out',\n",
       " 'for',\n",
       " '$1',\n",
       " ',000',\n",
       " ',000',\n",
       " '.',\n",
       " 'the',\n",
       " 'next',\n",
       " 'day',\n",
       " ',',\n",
       " 'while',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'cash',\n",
       " 'it',\n",
       " 'at',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'taken',\n",
       " 'to',\n",
       " 'the',\n",
       " 'owner',\n",
       " 'who',\n",
       " 'thinks',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'person',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'give',\n",
       " 'the',\n",
       " '$1',\n",
       " ',000',\n",
       " ',000',\n",
       " 'and',\n",
       " 'does',\n",
       " '.',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'not',\n",
       " 'though',\n",
       " 'as',\n",
       " 'the',\n",
       " 'real',\n",
       " 'person',\n",
       " 'named',\n",
       " 'juice',\n",
       " 'comes',\n",
       " 'in',\n",
       " 'a',\n",
       " 'moment',\n",
       " 'later',\n",
       " '.',\n",
       " 'now',\n",
       " 'the',\n",
       " 'three',\n",
       " 'have',\n",
       " 'to',\n",
       " 'track',\n",
       " 'preston',\n",
       " 'down',\n",
       " '.',\n",
       " 'meanwhile',\n",
       " ',',\n",
       " 'preston',\n",
       " 'has',\n",
       " 'fun',\n",
       " 'buying',\n",
       " 'all',\n",
       " 'sorts',\n",
       " 'of',\n",
       " 'stuff',\n",
       " 'including',\n",
       " 'his',\n",
       " 'own',\n",
       " 'house',\n",
       " 'and',\n",
       " 'even',\n",
       " 'going',\n",
       " 'on',\n",
       " 'dates',\n",
       " 'with',\n",
       " 'a',\n",
       " 'disguised',\n",
       " 'bank',\n",
       " 'lady',\n",
       " 'who',\n",
       " \"'s\",\n",
       " 'really',\n",
       " 'and',\n",
       " 'fbi',\n",
       " 'agent',\n",
       " '(who',\n",
       " \"'s\",\n",
       " 'trying',\n",
       " 'to',\n",
       " 'track',\n",
       " 'the',\n",
       " '3',\n",
       " 'bad',\n",
       " 'guys',\n",
       " 'down',\n",
       " ')',\n",
       " '.',\n",
       " 'he',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'person',\n",
       " 'up',\n",
       " 'named',\n",
       " 'mr',\n",
       " '.',\n",
       " 'macintosh',\n",
       " 'who',\n",
       " 'he',\n",
       " 'works',\n",
       " 'for',\n",
       " 'and',\n",
       " 'even',\n",
       " 'plans',\n",
       " 'a',\n",
       " 'party',\n",
       " 'for',\n",
       " 'him',\n",
       " 'on',\n",
       " 'his',\n",
       " 'birthday',\n",
       " '.',\n",
       " 'but',\n",
       " 'eventually',\n",
       " 'things',\n",
       " 'catch',\n",
       " 'up',\n",
       " '(the',\n",
       " 'money',\n",
       " 'runs',\n",
       " 'out',\n",
       " ',',\n",
       " 'the',\n",
       " 'bad',\n",
       " 'guys',\n",
       " 'get',\n",
       " 'him',\n",
       " ')',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'funny',\n",
       " 'flick',\n",
       " '.',\n",
       " 'miguel',\n",
       " 'ferrer',\n",
       " 'plays',\n",
       " 'his',\n",
       " 'role',\n",
       " 'very',\n",
       " 'good',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'enjoyed',\n",
       " 'him',\n",
       " 'in',\n",
       " 'another',\n",
       " 'stakeout',\n",
       " ',',\n",
       " 'you',\n",
       " \"'ll\",\n",
       " 'love',\n",
       " 'him',\n",
       " 'here',\n",
       " '.',\n",
       " 'he',\n",
       " 'does',\n",
       " 'all',\n",
       " 'sorts',\n",
       " 'of',\n",
       " 'wicked',\n",
       " 'crazy',\n",
       " 'things',\n",
       " '.',\n",
       " 'rick',\n",
       " 'ducommun',\n",
       " '(the',\n",
       " 'stressed',\n",
       " 'out',\n",
       " 'boss',\n",
       " 'from',\n",
       " 'ghost',\n",
       " 'in',\n",
       " 'the',\n",
       " 'machine',\n",
       " ')',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "opposed-individual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104815\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_tokens_array = np.array(corpus_flat)\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "vocab_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "funny-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only take (vocab_size - 2) most commons words from the training data since\n",
    "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "random-journalism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "mounted-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('datasets/pickle_data/vocab_large_text_tg_gpt.pickle', 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-suggestion",
   "metadata": {},
   "source": [
    "* 사실상 padding 거의 사용 안 됨, 긴 문장에서 80 만큼만 잘라서 input, target 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "becoming-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('datasets/pickle_data/vocab_large_text_tg_gpt.pickle', 'rb') as f:\n",
    "    vocabulary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "automatic-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {}\n",
    "token_to_id['[UNK]'] = 0\n",
    "token_to_id['[PAD]'] = 1\n",
    "for i, token in enumerate(vocabulary):\n",
    "    token_to_id[token] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "extraordinary-essence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[UNK]': 0,\n",
       " '[PAD]': 1,\n",
       " 'the': 2,\n",
       " '.': 3,\n",
       " ',': 4,\n",
       " 'a': 5,\n",
       " 'and': 6,\n",
       " 'of': 7,\n",
       " 'to': 8,\n",
       " 'is': 9,\n",
       " 'it': 10,\n",
       " 'in': 11,\n",
       " 'i': 12,\n",
       " 'this': 13,\n",
       " 'that': 14,\n",
       " \"'s\": 15,\n",
       " 'was': 16,\n",
       " 'as': 17,\n",
       " 'for': 18,\n",
       " 'with': 19,\n",
       " 'movie': 20,\n",
       " 'but': 21,\n",
       " 'film': 22,\n",
       " ')': 23,\n",
       " 'on': 24,\n",
       " 'you': 25,\n",
       " \"'t\": 26,\n",
       " '\"': 27,\n",
       " 'not': 28,\n",
       " 'he': 29,\n",
       " 'are': 30,\n",
       " 'his': 31,\n",
       " 'have': 32,\n",
       " 'be': 33,\n",
       " 'one': 34,\n",
       " '!': 35,\n",
       " 'all': 36,\n",
       " 'at': 37,\n",
       " 'they': 38,\n",
       " 'by': 39,\n",
       " 'an': 40,\n",
       " 'who': 41,\n",
       " 'from': 42,\n",
       " 'so': 43,\n",
       " 'like': 44,\n",
       " '-': 45,\n",
       " 'there': 46,\n",
       " 'her': 47,\n",
       " 'just': 48,\n",
       " 'about': 49,\n",
       " 'or': 50,\n",
       " 'has': 51,\n",
       " 'out': 52,\n",
       " 'if': 53,\n",
       " '?': 54,\n",
       " 'what': 55,\n",
       " 'some': 56,\n",
       " 'good': 57,\n",
       " 'can': 58,\n",
       " 'more': 59,\n",
       " 'when': 60,\n",
       " 'very': 61,\n",
       " 'she': 62,\n",
       " 'would': 63,\n",
       " 'up': 64,\n",
       " 'time': 65,\n",
       " 'even': 66,\n",
       " 'no': 67,\n",
       " 'my': 68,\n",
       " 'story': 69,\n",
       " 'only': 70,\n",
       " 'really': 71,\n",
       " 'their': 72,\n",
       " 'had': 73,\n",
       " 'see': 74,\n",
       " 'which': 75,\n",
       " 'were': 76,\n",
       " 'me': 77,\n",
       " 'we': 78,\n",
       " 'well': 79,\n",
       " \"'\": 80,\n",
       " 'than': 81,\n",
       " ':': 82,\n",
       " 'much': 83,\n",
       " 'been': 84,\n",
       " 'people': 85,\n",
       " 'get': 86,\n",
       " 'will': 87,\n",
       " 'other': 88,\n",
       " 'into': 89,\n",
       " 'bad': 90,\n",
       " 'also': 91,\n",
       " 'do': 92,\n",
       " 'first': 93,\n",
       " 'great': 94,\n",
       " 'because': 95,\n",
       " 'him': 96,\n",
       " 'most': 97,\n",
       " 'how': 98,\n",
       " 'don': 99,\n",
       " 'made': 100,\n",
       " 'its': 101,\n",
       " 'then': 102,\n",
       " 'them': 103,\n",
       " 'make': 104,\n",
       " 'way': 105,\n",
       " 'could': 106,\n",
       " 'too': 107,\n",
       " 'any': 108,\n",
       " 'movies': 109,\n",
       " 'after': 110,\n",
       " 'think': 111,\n",
       " 'characters': 112,\n",
       " 'character': 113,\n",
       " 'watch': 114,\n",
       " 'films': 115,\n",
       " 'two': 116,\n",
       " 'seen': 117,\n",
       " 'many': 118,\n",
       " ';': 119,\n",
       " 'being': 120,\n",
       " 'plot': 121,\n",
       " 'never': 122,\n",
       " 'acting': 123,\n",
       " 'little': 124,\n",
       " 'best': 125,\n",
       " 'life': 126,\n",
       " 'love': 127,\n",
       " 'show': 128,\n",
       " 'where': 129,\n",
       " 'did': 130,\n",
       " 'over': 131,\n",
       " 'know': 132,\n",
       " 'ever': 133,\n",
       " 'does': 134,\n",
       " 'off': 135,\n",
       " 'here': 136,\n",
       " 'better': 137,\n",
       " '*': 138,\n",
       " 'man': 139,\n",
       " 'your': 140,\n",
       " 'end': 141,\n",
       " 'still': 142,\n",
       " 'these': 143,\n",
       " 'say': 144,\n",
       " 'scene': 145,\n",
       " 'while': 146,\n",
       " 'scenes': 147,\n",
       " \"'ve\": 148,\n",
       " 'why': 149,\n",
       " 'go': 150,\n",
       " 'should': 151,\n",
       " 'such': 152,\n",
       " 'through': 153,\n",
       " 'something': 154,\n",
       " 'back': 155,\n",
       " \"'m\": 156,\n",
       " 'those': 157,\n",
       " 'real': 158,\n",
       " 'watching': 159,\n",
       " 'doesn': 160,\n",
       " 'thing': 161,\n",
       " 'now': 162,\n",
       " 'years': 163,\n",
       " 'actors': 164,\n",
       " 'didn': 165,\n",
       " 'though': 166,\n",
       " 'another': 167,\n",
       " 'work': 168,\n",
       " 'before': 169,\n",
       " 'new': 170,\n",
       " 'nothing': 171,\n",
       " 'funny': 172,\n",
       " 'actually': 173,\n",
       " 'makes': 174,\n",
       " 'director': 175,\n",
       " 'old': 176,\n",
       " 'find': 177,\n",
       " 'few': 178,\n",
       " 'look': 179,\n",
       " 'same': 180,\n",
       " 'going': 181,\n",
       " 'lot': 182,\n",
       " 'part': 183,\n",
       " 'every': 184,\n",
       " 'again': 185,\n",
       " 'cast': 186,\n",
       " 'us': 187,\n",
       " 'world': 188,\n",
       " \"'re\": 189,\n",
       " 'quite': 190,\n",
       " 'want': 191,\n",
       " 'things': 192,\n",
       " 'pretty': 193,\n",
       " 'seems': 194,\n",
       " 'young': 195,\n",
       " 'down': 196,\n",
       " 'around': 197,\n",
       " 'got': 198,\n",
       " '&': 199,\n",
       " 'fact': 200,\n",
       " 'however': 201,\n",
       " 'take': 202,\n",
       " 'enough': 203,\n",
       " 'horror': 204,\n",
       " 'thought': 205,\n",
       " 'long': 206,\n",
       " 'between': 207,\n",
       " 'may': 208,\n",
       " 'big': 209,\n",
       " 'give': 210,\n",
       " 'original': 211,\n",
       " 'own': 212,\n",
       " 'both': 213,\n",
       " 'series': 214,\n",
       " 'right': 215,\n",
       " 'times': 216,\n",
       " 'always': 217,\n",
       " 'without': 218,\n",
       " 'action': 219,\n",
       " 'must': 220,\n",
       " 'gets': 221,\n",
       " 'role': 222,\n",
       " 'point': 223,\n",
       " 'isn': 224,\n",
       " 'saw': 225,\n",
       " 'come': 226,\n",
       " 'family': 227,\n",
       " 'least': 228,\n",
       " 'interesting': 229,\n",
       " 'comedy': 230,\n",
       " 'almost': 231,\n",
       " 'whole': 232,\n",
       " 'bit': 233,\n",
       " 'music': 234,\n",
       " 'script': 235,\n",
       " 'done': 236,\n",
       " 'far': 237,\n",
       " 'guy': 238,\n",
       " 'minutes': 239,\n",
       " 'anything': 240,\n",
       " 'feel': 241,\n",
       " 'might': 242,\n",
       " 'performance': 243,\n",
       " '\"the': 244,\n",
       " 'last': 245,\n",
       " \"'ll\": 246,\n",
       " 'since': 247,\n",
       " 'probably': 248,\n",
       " 'am': 249,\n",
       " 'woman': 250,\n",
       " 'girl': 251,\n",
       " 'kind': 252,\n",
       " 'away': 253,\n",
       " 'rather': 254,\n",
       " 'yet': 255,\n",
       " 'worst': 256,\n",
       " 'sure': 257,\n",
       " 'fun': 258,\n",
       " 'tv': 259,\n",
       " 'day': 260,\n",
       " 'anyone': 261,\n",
       " 'hard': 262,\n",
       " 'each': 263,\n",
       " 'making': 264,\n",
       " 'found': 265,\n",
       " 'having': 266,\n",
       " 'course': 267,\n",
       " 'believe': 268,\n",
       " 'comes': 269,\n",
       " 'trying': 270,\n",
       " 'our': 271,\n",
       " '10': 272,\n",
       " 'goes': 273,\n",
       " '2': 274,\n",
       " 'set': 275,\n",
       " 'looks': 276,\n",
       " 'place': 277,\n",
       " 'book': 278,\n",
       " 'put': 279,\n",
       " 'different': 280,\n",
       " 'actor': 281,\n",
       " \"'d\": 282,\n",
       " 'money': 283,\n",
       " 'ending': 284,\n",
       " 'reason': 285,\n",
       " 'screen': 286,\n",
       " 'someone': 287,\n",
       " 'played': 288,\n",
       " 'dvd': 289,\n",
       " 'wasn': 290,\n",
       " 'sense': 291,\n",
       " 'shows': 292,\n",
       " 'everything': 293,\n",
       " 'once': 294,\n",
       " 'true': 295,\n",
       " 'job': 296,\n",
       " 'although': 297,\n",
       " 'worth': 298,\n",
       " 'main': 299,\n",
       " 'three': 300,\n",
       " 'especially': 301,\n",
       " 'looking': 302,\n",
       " 'watched': 303,\n",
       " 'together': 304,\n",
       " 'plays': 305,\n",
       " 'maybe': 306,\n",
       " 'let': 307,\n",
       " 'everyone': 308,\n",
       " 'said': 309,\n",
       " 'audience': 310,\n",
       " 'play': 311,\n",
       " 'seem': 312,\n",
       " 'later': 313,\n",
       " 'effects': 314,\n",
       " 'takes': 315,\n",
       " 'instead': 316,\n",
       " 'himself': 317,\n",
       " 'version': 318,\n",
       " 'beautiful': 319,\n",
       " 'during': 320,\n",
       " 'left': 321,\n",
       " 'high': 322,\n",
       " 'seeing': 323,\n",
       " 'father': 324,\n",
       " 'wife': 325,\n",
       " 'half': 326,\n",
       " 'night': 327,\n",
       " 'year': 328,\n",
       " 'special': 329,\n",
       " 'excellent': 330,\n",
       " 'john': 331,\n",
       " 'idea': 332,\n",
       " 'house': 333,\n",
       " 'else': 334,\n",
       " 'shot': 335,\n",
       " 'mind': 336,\n",
       " 'american': 337,\n",
       " 'nice': 338,\n",
       " 'simply': 339,\n",
       " 'less': 340,\n",
       " 'black': 341,\n",
       " 'second': 342,\n",
       " 'war': 343,\n",
       " 'fan': 344,\n",
       " 'read': 345,\n",
       " 'help': 346,\n",
       " 'completely': 347,\n",
       " 'poor': 348,\n",
       " 'either': 349,\n",
       " 'short': 350,\n",
       " 'used': 351,\n",
       " 'death': 352,\n",
       " 'hollywood': 353,\n",
       " 'given': 354,\n",
       " 'men': 355,\n",
       " 'home': 356,\n",
       " 'try': 357,\n",
       " 'star': 358,\n",
       " 'performances': 359,\n",
       " 'kids': 360,\n",
       " 'wrong': 361,\n",
       " 'enjoy': 362,\n",
       " 'rest': 363,\n",
       " 'need': 364,\n",
       " 'women': 365,\n",
       " 'use': 366,\n",
       " 'classic': 367,\n",
       " 'boring': 368,\n",
       " 'dead': 369,\n",
       " 'line': 370,\n",
       " 'low': 371,\n",
       " 'camera': 372,\n",
       " 'friends': 373,\n",
       " 'until': 374,\n",
       " 'truly': 375,\n",
       " 'full': 376,\n",
       " 'along': 377,\n",
       " 'production': 378,\n",
       " '/10': 379,\n",
       " 'couple': 380,\n",
       " 'tell': 381,\n",
       " 'start': 382,\n",
       " 'next': 383,\n",
       " 'mean': 384,\n",
       " '3': 385,\n",
       " 'came': 386,\n",
       " 'stupid': 387,\n",
       " 'won': 388,\n",
       " 'recommend': 389,\n",
       " '(and': 390,\n",
       " 'remember': 391,\n",
       " 'moments': 392,\n",
       " '1': 393,\n",
       " 'wonderful': 394,\n",
       " 'episode': 395,\n",
       " 'awful': 396,\n",
       " 'understand': 397,\n",
       " 'terrible': 398,\n",
       " 'sex': 399,\n",
       " 'top': 400,\n",
       " 'video': 401,\n",
       " 'small': 402,\n",
       " 'getting': 403,\n",
       " 'perhaps': 404,\n",
       " 'stars': 405,\n",
       " 'others': 406,\n",
       " 'playing': 407,\n",
       " 'keep': 408,\n",
       " 'face': 409,\n",
       " 'doing': 410,\n",
       " 'school': 411,\n",
       " 'often': 412,\n",
       " 'gives': 413,\n",
       " 'definitely': 414,\n",
       " 'person': 415,\n",
       " 'early': 416,\n",
       " 'name': 417,\n",
       " 'itself': 418,\n",
       " '(': 419,\n",
       " 'become': 420,\n",
       " 'human': 421,\n",
       " 'perfect': 422,\n",
       " 'dialogue': 423,\n",
       " 'lines': 424,\n",
       " 'case': 425,\n",
       " 'felt': 426,\n",
       " 'live': 427,\n",
       " 'finally': 428,\n",
       " 'supposed': 429,\n",
       " 'head': 430,\n",
       " 'piece': 431,\n",
       " 'liked': 432,\n",
       " 'couldn': 433,\n",
       " 'mother': 434,\n",
       " 'children': 435,\n",
       " 'absolutely': 436,\n",
       " 'budget': 437,\n",
       " 'title': 438,\n",
       " 'boy': 439,\n",
       " 'picture': 440,\n",
       " 'cinema': 441,\n",
       " '(the': 442,\n",
       " 'went': 443,\n",
       " 'against': 444,\n",
       " 'worse': 445,\n",
       " 'entire': 446,\n",
       " 'lost': 447,\n",
       " 'waste': 448,\n",
       " 'certainly': 449,\n",
       " 'style': 450,\n",
       " 'sort': 451,\n",
       " 'written': 452,\n",
       " 'problem': 453,\n",
       " 'hope': 454,\n",
       " 'entertaining': 455,\n",
       " 'friend': 456,\n",
       " 'several': 457,\n",
       " 'overall': 458,\n",
       " 'loved': 459,\n",
       " 'fans': 460,\n",
       " 'killer': 461,\n",
       " 'beginning': 462,\n",
       " 'lives': 463,\n",
       " 'care': 464,\n",
       " 'evil': 465,\n",
       " 'becomes': 466,\n",
       " 'direction': 467,\n",
       " 'already': 468,\n",
       " 'example': 469,\n",
       " 'laugh': 470,\n",
       " 'seemed': 471,\n",
       " 'throughout': 472,\n",
       " 'white': 473,\n",
       " 'wanted': 474,\n",
       " 'turn': 475,\n",
       " '\\x96': 476,\n",
       " 'under': 477,\n",
       " 'based': 478,\n",
       " 'drama': 479,\n",
       " 'mr': 480,\n",
       " 'yes': 481,\n",
       " 'unfortunately': 482,\n",
       " 'fine': 483,\n",
       " 'history': 484,\n",
       " 'dark': 485,\n",
       " 'despite': 486,\n",
       " 'amazing': 487,\n",
       " 'totally': 488,\n",
       " 'sound': 489,\n",
       " 'lead': 490,\n",
       " 'heart': 491,\n",
       " 'son': 492,\n",
       " 'guess': 493,\n",
       " 'final': 494,\n",
       " 'guys': 495,\n",
       " 'humor': 496,\n",
       " 'child': 497,\n",
       " 'writing': 498,\n",
       " 'wants': 499,\n",
       " 'close': 500,\n",
       " 'works': 501,\n",
       " 'tries': 502,\n",
       " 'past': 503,\n",
       " 'called': 504,\n",
       " 'viewer': 505,\n",
       " 'behind': 506,\n",
       " 'quality': 507,\n",
       " 'days': 508,\n",
       " 'game': 509,\n",
       " 'turns': 510,\n",
       " 'able': 511,\n",
       " 'enjoyed': 512,\n",
       " 'side': 513,\n",
       " 'today': 514,\n",
       " 'town': 515,\n",
       " 'favorite': 516,\n",
       " 'flick': 517,\n",
       " 'hand': 518,\n",
       " 'gave': 519,\n",
       " 'starts': 520,\n",
       " 'act': 521,\n",
       " 'soon': 522,\n",
       " 'genre': 523,\n",
       " 'car': 524,\n",
       " 'michael': 525,\n",
       " 'kill': 526,\n",
       " 'eyes': 527,\n",
       " 'oh': 528,\n",
       " 'girls': 529,\n",
       " 'art': 530,\n",
       " 'horrible': 531,\n",
       " 'late': 532,\n",
       " 'parts': 533,\n",
       " 'actress': 534,\n",
       " '4': 535,\n",
       " 'themselves': 536,\n",
       " 'kid': 537,\n",
       " 'expect': 538,\n",
       " 'sometimes': 539,\n",
       " 'brilliant': 540,\n",
       " 'run': 541,\n",
       " 'stuff': 542,\n",
       " 'self': 543,\n",
       " 'stories': 544,\n",
       " 'thinking': 545,\n",
       " 'etc': 546,\n",
       " 'city': 547,\n",
       " 'directed': 548,\n",
       " 'myself': 549,\n",
       " 'decent': 550,\n",
       " 'god': 551,\n",
       " 'obviously': 552,\n",
       " 'blood': 553,\n",
       " 'feeling': 554,\n",
       " 'voice': 555,\n",
       " 'highly': 556,\n",
       " 'matter': 557,\n",
       " 'roles': 558,\n",
       " 'fight': 559,\n",
       " 'moment': 560,\n",
       " 'says': 561,\n",
       " 'killed': 562,\n",
       " 'heard': 563,\n",
       " 'daughter': 564,\n",
       " 'slow': 565,\n",
       " 'b': 566,\n",
       " 'took': 567,\n",
       " 'cannot': 568,\n",
       " 'anyway': 569,\n",
       " 'strong': 570,\n",
       " 'hour': 571,\n",
       " 'leave': 572,\n",
       " 'happens': 573,\n",
       " 'brother': 574,\n",
       " 'happened': 575,\n",
       " 'violence': 576,\n",
       " 'involved': 577,\n",
       " 'chance': 578,\n",
       " 'extremely': 579,\n",
       " '(i': 580,\n",
       " 'hit': 581,\n",
       " 'police': 582,\n",
       " 'age': 583,\n",
       " 'wouldn': 584,\n",
       " 'lack': 585,\n",
       " 'obvious': 586,\n",
       " 'experience': 587,\n",
       " 'told': 588,\n",
       " 'attempt': 589,\n",
       " 'happen': 590,\n",
       " 'living': 591,\n",
       " 'alone': 592,\n",
       " 'writer': 593,\n",
       " 'coming': 594,\n",
       " 'wonder': 595,\n",
       " 'ago': 596,\n",
       " 'group': 597,\n",
       " 'crap': 598,\n",
       " '5': 599,\n",
       " 'score': 600,\n",
       " 'type': 601,\n",
       " 'interest': 602,\n",
       " 'simple': 603,\n",
       " 'murder': 604,\n",
       " 'gore': 605,\n",
       " 'looked': 606,\n",
       " 'none': 607,\n",
       " 'complete': 608,\n",
       " 'particularly': 609,\n",
       " 'husband': 610,\n",
       " 'career': 611,\n",
       " 'number': 612,\n",
       " 'please': 613,\n",
       " 'song': 614,\n",
       " 'james': 615,\n",
       " 'exactly': 616,\n",
       " 'shown': 617,\n",
       " 'save': 618,\n",
       " 'hell': 619,\n",
       " 'stop': 620,\n",
       " 'annoying': 621,\n",
       " 'yourself': 622,\n",
       " 'taken': 623,\n",
       " 'sad': 624,\n",
       " 'cinematography': 625,\n",
       " 'musical': 626,\n",
       " 'ends': 627,\n",
       " 'hours': 628,\n",
       " 'except': 629,\n",
       " 'across': 630,\n",
       " 'seriously': 631,\n",
       " 'possible': 632,\n",
       " 'released': 633,\n",
       " 'running': 634,\n",
       " 'opinion': 635,\n",
       " 'relationship': 636,\n",
       " 'usual': 637,\n",
       " 'ridiculous': 638,\n",
       " 'body': 639,\n",
       " 'novel': 640,\n",
       " 'started': 641,\n",
       " 'opening': 642,\n",
       " 'light': 643,\n",
       " 'somewhat': 644,\n",
       " 'serious': 645,\n",
       " 'hilarious': 646,\n",
       " 'english': 647,\n",
       " 'usually': 648,\n",
       " 'known': 649,\n",
       " 'cut': 650,\n",
       " 'wish': 651,\n",
       " 'ok': 652,\n",
       " 'king': 653,\n",
       " 'finds': 654,\n",
       " 'ones': 655,\n",
       " 'change': 656,\n",
       " 'reality': 657,\n",
       " 'middle': 658,\n",
       " 'order': 659,\n",
       " 'whose': 660,\n",
       " 'huge': 661,\n",
       " 'shots': 662,\n",
       " 'saying': 663,\n",
       " 'scary': 664,\n",
       " 'hero': 665,\n",
       " 'david': 666,\n",
       " 'jokes': 667,\n",
       " 'episodes': 668,\n",
       " 'view': 669,\n",
       " 'talking': 670,\n",
       " 'female': 671,\n",
       " 'rating': 672,\n",
       " 'level': 673,\n",
       " 'taking': 674,\n",
       " 'disappointed': 675,\n",
       " 'power': 676,\n",
       " 'room': 677,\n",
       " 'single': 678,\n",
       " 'country': 679,\n",
       " 'major': 680,\n",
       " 'call': 681,\n",
       " 'events': 682,\n",
       " 'documentary': 683,\n",
       " 'talent': 684,\n",
       " 'cool': 685,\n",
       " 'strange': 686,\n",
       " 'songs': 687,\n",
       " 'important': 688,\n",
       " 'knows': 689,\n",
       " 'basically': 690,\n",
       " 'knew': 691,\n",
       " 'attention': 692,\n",
       " 'word': 693,\n",
       " 'clearly': 694,\n",
       " 'supporting': 695,\n",
       " 'happy': 696,\n",
       " 'due': 697,\n",
       " 'future': 698,\n",
       " 'turned': 699,\n",
       " 'easily': 700,\n",
       " 'television': 701,\n",
       " 'problems': 702,\n",
       " 'four': 703,\n",
       " 'british': 704,\n",
       " 'cheap': 705,\n",
       " 'mostly': 706,\n",
       " 'tells': 707,\n",
       " 'fast': 708,\n",
       " 'apparently': 709,\n",
       " 'comic': 710,\n",
       " 'aren': 711,\n",
       " 'local': 712,\n",
       " 'words': 713,\n",
       " 'earth': 714,\n",
       " 'silly': 715,\n",
       " 'sequence': 716,\n",
       " 'five': 717,\n",
       " 'non': 718,\n",
       " 'modern': 719,\n",
       " '-up': 720,\n",
       " 'robert': 721,\n",
       " 'bring': 722,\n",
       " 'including': 723,\n",
       " 'entertainment': 724,\n",
       " 'jack': 725,\n",
       " '.i': 726,\n",
       " 'falls': 727,\n",
       " 'predictable': 728,\n",
       " 'beyond': 729,\n",
       " 'sets': 730,\n",
       " 'review': 731,\n",
       " 'similar': 732,\n",
       " '.the': 733,\n",
       " 'whether': 734,\n",
       " 'miss': 735,\n",
       " 'enjoyable': 736,\n",
       " 'oscar': 737,\n",
       " 'upon': 738,\n",
       " '-the': 739,\n",
       " 'appears': 740,\n",
       " 'needs': 741,\n",
       " 'romantic': 742,\n",
       " 'straight': 743,\n",
       " 'lady': 744,\n",
       " 'eye': 745,\n",
       " 'giving': 746,\n",
       " 'rock': 747,\n",
       " 'talk': 748,\n",
       " 'animation': 749,\n",
       " 'theater': 750,\n",
       " 'within': 751,\n",
       " 'bunch': 752,\n",
       " 'near': 753,\n",
       " 'dull': 754,\n",
       " 'mention': 755,\n",
       " 'sequel': 756,\n",
       " 'points': 757,\n",
       " 'above': 758,\n",
       " 'ten': 759,\n",
       " 'york': 760,\n",
       " 'haven': 761,\n",
       " 'add': 762,\n",
       " 'feels': 763,\n",
       " 'stand': 764,\n",
       " 'nearly': 765,\n",
       " 'message': 766,\n",
       " 'george': 767,\n",
       " 'paul': 768,\n",
       " 'storyline': 769,\n",
       " 'surprised': 770,\n",
       " 'ways': 771,\n",
       " 'theme': 772,\n",
       " 'herself': 773,\n",
       " 'begins': 774,\n",
       " 'moving': 775,\n",
       " 'team': 776,\n",
       " '7': 777,\n",
       " 'lee': 778,\n",
       " 'actual': 779,\n",
       " 'sister': 780,\n",
       " 'effort': 781,\n",
       " 'easy': 782,\n",
       " 'thriller': 783,\n",
       " 'fantastic': 784,\n",
       " 'using': 785,\n",
       " 'viewers': 786,\n",
       " 'clear': 787,\n",
       " 'mystery': 788,\n",
       " '8': 789,\n",
       " 'elements': 790,\n",
       " 'feature': 791,\n",
       " 'richard': 792,\n",
       " 'named': 793,\n",
       " 'lots': 794,\n",
       " 'release': 795,\n",
       " 'follow': 796,\n",
       " 'comments': 797,\n",
       " 'showing': 798,\n",
       " 'die': 799,\n",
       " 'tried': 800,\n",
       " 'french': 801,\n",
       " 'tale': 802,\n",
       " 'dialog': 803,\n",
       " 'certain': 804,\n",
       " 'working': 805,\n",
       " 'soundtrack': 806,\n",
       " 'typical': 807,\n",
       " 'among': 808,\n",
       " 'avoid': 809,\n",
       " 'stay': 810,\n",
       " 'means': 811,\n",
       " 'fall': 812,\n",
       " 'material': 813,\n",
       " 'form': 814,\n",
       " 'season': 815,\n",
       " 'hate': 816,\n",
       " 'editing': 817,\n",
       " 'weak': 818,\n",
       " 'parents': 819,\n",
       " 'famous': 820,\n",
       " 'doubt': 821,\n",
       " 'buy': 822,\n",
       " 'general': 823,\n",
       " 'red': 824,\n",
       " 'leads': 825,\n",
       " 'period': 826,\n",
       " 'kept': 827,\n",
       " 'somehow': 828,\n",
       " 'filmed': 829,\n",
       " 'figure': 830,\n",
       " 'viewing': 831,\n",
       " 'greatest': 832,\n",
       " 'class': 833,\n",
       " '(which': 834,\n",
       " 're': 835,\n",
       " 'disney': 836,\n",
       " 'lame': 837,\n",
       " 'brought': 838,\n",
       " 'hear': 839,\n",
       " 'crime': 840,\n",
       " 'third': 841,\n",
       " 'particular': 842,\n",
       " 'atmosphere': 843,\n",
       " 'space': 844,\n",
       " 'realistic': 845,\n",
       " 'sequences': 846,\n",
       " '(as': 847,\n",
       " 'tom': 848,\n",
       " 'move': 849,\n",
       " 'gone': 850,\n",
       " 'imagine': 851,\n",
       " 'suspense': 852,\n",
       " 'america': 853,\n",
       " 'peter': 854,\n",
       " 'learn': 855,\n",
       " 'deal': 856,\n",
       " 'reviews': 857,\n",
       " 'check': 858,\n",
       " 'eventually': 859,\n",
       " 'indeed': 860,\n",
       " 'premise': 861,\n",
       " 'poorly': 862,\n",
       " 'forget': 863,\n",
       " 'subject': 864,\n",
       " 'dance': 865,\n",
       " 'decided': 866,\n",
       " 'believable': 867,\n",
       " 'nature': 868,\n",
       " 'sit': 869,\n",
       " 'expected': 870,\n",
       " 'sorry': 871,\n",
       " 'wait': 872,\n",
       " 'became': 873,\n",
       " 'stage': 874,\n",
       " 'japanese': 875,\n",
       " 'difficult': 876,\n",
       " 'truth': 877,\n",
       " 'surprise': 878,\n",
       " 'average': 879,\n",
       " 'sexual': 880,\n",
       " 'rent': 881,\n",
       " 'whatever': 882,\n",
       " 'de': 883,\n",
       " 'question': 884,\n",
       " '\"i': 885,\n",
       " 'screenplay': 886,\n",
       " 'possibly': 887,\n",
       " 'street': 888,\n",
       " 'zombie': 889,\n",
       " 'leaves': 890,\n",
       " 'needed': 891,\n",
       " 'begin': 892,\n",
       " 'nor': 893,\n",
       " 'imdb': 894,\n",
       " 'okay': 895,\n",
       " 'reading': 896,\n",
       " 'killing': 897,\n",
       " 'romance': 898,\n",
       " 'society': 899,\n",
       " '(or': 900,\n",
       " 'superb': 901,\n",
       " '9': 902,\n",
       " 'situation': 903,\n",
       " 'shame': 904,\n",
       " 'directors': 905,\n",
       " 'meets': 906,\n",
       " 'otherwise': 907,\n",
       " 'credits': 908,\n",
       " 'earlier': 909,\n",
       " 'memorable': 910,\n",
       " 'write': 911,\n",
       " 'forced': 912,\n",
       " 'minute': 913,\n",
       " 'o': 914,\n",
       " 'realize': 915,\n",
       " 'badly': 916,\n",
       " 'joe': 917,\n",
       " 'baby': 918,\n",
       " 'emotional': 919,\n",
       " 'footage': 920,\n",
       " 'dog': 921,\n",
       " 'comment': 922,\n",
       " 'laughs': 923,\n",
       " 'interested': 924,\n",
       " 'meet': 925,\n",
       " 'older': 926,\n",
       " 'weird': 927,\n",
       " 'note': 928,\n",
       " 'ask': 929,\n",
       " 'keeps': 930,\n",
       " '20': 931,\n",
       " 'male': 932,\n",
       " 'dr': 933,\n",
       " 'open': 934,\n",
       " 'features': 935,\n",
       " 'mess': 936,\n",
       " 'free': 937,\n",
       " 'quickly': 938,\n",
       " 'development': 939,\n",
       " 'writers': 940,\n",
       " 'beauty': 941,\n",
       " 'towards': 942,\n",
       " 'dramatic': 943,\n",
       " 'dream': 944,\n",
       " 'directing': 945,\n",
       " 'previous': 946,\n",
       " 'result': 947,\n",
       " 'perfectly': 948,\n",
       " 'worked': 949,\n",
       " 'brings': 950,\n",
       " 'sci': 951,\n",
       " 'hands': 952,\n",
       " 'effect': 953,\n",
       " 'creepy': 954,\n",
       " 'hot': 955,\n",
       " 'cheesy': 956,\n",
       " 'incredibly': 957,\n",
       " 'crazy': 958,\n",
       " 'sounds': 959,\n",
       " 'deep': 960,\n",
       " 'plenty': 961,\n",
       " 'admit': 962,\n",
       " 'unique': 963,\n",
       " 'setting': 964,\n",
       " 'personal': 965,\n",
       " 'background': 966,\n",
       " 'fantasy': 967,\n",
       " 'appear': 968,\n",
       " 'total': 969,\n",
       " 'box': 970,\n",
       " 'plus': 971,\n",
       " '(a': 972,\n",
       " 'casting': 973,\n",
       " 'meant': 974,\n",
       " 'leading': 975,\n",
       " 'powerful': 976,\n",
       " 'business': 977,\n",
       " '(who': 978,\n",
       " '-and': 979,\n",
       " 'fails': 980,\n",
       " 'potential': 981,\n",
       " 'whom': 982,\n",
       " 'forward': 983,\n",
       " 'monster': 984,\n",
       " 'unless': 985,\n",
       " 'create': 986,\n",
       " 'various': 987,\n",
       " 'christmas': 988,\n",
       " 'hardly': 989,\n",
       " 'pay': 990,\n",
       " 'break': 991,\n",
       " 'mark': 992,\n",
       " 'joke': 993,\n",
       " 'boys': 994,\n",
       " 'battle': 995,\n",
       " 'reasons': 996,\n",
       " 'air': 997,\n",
       " 'apart': 998,\n",
       " 'ideas': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "vocational-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_token = {v: k for k, v in token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "available-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input : list, output : list\n",
    "max_seq_len = 80\n",
    "\n",
    "def encode_tokens(tokens, max_seq_len=max_seq_len):\n",
    "    \n",
    "    encoded = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in list(token_to_id.keys()):\n",
    "            encoded.append(token_to_id[token])\n",
    "        else:\n",
    "            encoded.append(token_to_id['[UNK]']) # unknown token\n",
    "    # padding\n",
    "    if len(tokens) < max_seq_len:\n",
    "        encoded = encoded + [0] * (max_seq_len - len(tokens))\n",
    "    # truncate\n",
    "    elif len(tokens) >= max_seq_len:\n",
    "        encoded = encoded[:max_seq_len]\n",
    "        \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "arabic-southwest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 245,\n",
       " 2394,\n",
       " 9,\n",
       " 34,\n",
       " 7,\n",
       " 2,\n",
       " 178,\n",
       " 2961,\n",
       " 133,\n",
       " 100,\n",
       " 8,\n",
       " 856,\n",
       " 19,\n",
       " 5365,\n",
       " 3391,\n",
       " 4,\n",
       " 213,\n",
       " 17,\n",
       " 5,\n",
       " 4049,\n",
       " 6,\n",
       " 977,\n",
       " 6,\n",
       " 17,\n",
       " 5,\n",
       " 4791,\n",
       " 7,\n",
       " 1884,\n",
       " 2,\n",
       " 16896,\n",
       " 1422,\n",
       " 1681,\n",
       " 3,\n",
       " 169,\n",
       " 2,\n",
       " 473,\n",
       " 139,\n",
       " 275,\n",
       " 2175,\n",
       " 24,\n",
       " 2,\n",
       " 88,\n",
       " 513,\n",
       " 7,\n",
       " 2,\n",
       " 0,\n",
       " 4,\n",
       " 2,\n",
       " 16896,\n",
       " 351,\n",
       " 8,\n",
       " 32,\n",
       " 0,\n",
       " 7,\n",
       " 337,\n",
       " 0,\n",
       " 17,\n",
       " 1062,\n",
       " 17,\n",
       " 56,\n",
       " 7,\n",
       " 271,\n",
       " 9388,\n",
       " 4792,\n",
       " 3,\n",
       " 39,\n",
       " 2,\n",
       " 65,\n",
       " 7,\n",
       " 2,\n",
       " 826,\n",
       " 2,\n",
       " 245,\n",
       " 2394,\n",
       " 9,\n",
       " 275,\n",
       " 11,\n",
       " 4,\n",
       " 2,\n",
       " 5365]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_tokens(corpus[0], 81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-sally",
   "metadata": {},
   "source": [
    "* `max_seq_len + 1` 만큼 encoding 하는 이유\n",
    "* time step 1씩 차이가 나도록\n",
    "\n",
    "```\n",
    "tokenized_sentences=vectorize_layer(tf.expand_dims('text is good',-1))\n",
    "x = tokenized_sentences[:, :-1]\n",
    "y = tokenized_sentences[:, 1:]\n",
    "\n",
    ">> tokenized_sentences\n",
    "... <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[1, 1, 1, 0, 0]])>\n",
    "\n",
    ">> x\n",
    "... <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[1, 1, 1, 0]])>\n",
    "\n",
    ">> y\n",
    "... <tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[1, 1, 0, 0]])>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "better-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n",
      "train_text shape : (25000, 81)\n"
     ]
    }
   ],
   "source": [
    "train_text=[]\n",
    "for i in range(len(corpus)):\n",
    "    if (i+1) % (len(corpus)/10) == 0:\n",
    "        print(f'{(i+1)/(len(corpus)/100)} % Done')\n",
    "    train_text.append(encode_tokens(corpus[i], max_seq_len = max_seq_len + 1))  # encode reviews with vectorizer\n",
    "train_text = np.array(train_text)\n",
    "print(\"train_text shape :\", train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "broke-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('datasets/pickle_data/train_large_text_tg_gpt.pickle', 'wb') as f:\n",
    "    pickle.dump(train_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "exterior-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('datasets/pickle_data/train_large_text_tg_gpt.pickle', 'rb') as f:\n",
    "    train_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "colonial-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape : (25000, 80)\n",
      "train_y shape : (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "train_x = train_text[:, :-1]\n",
    "train_y = train_text[:, 1:]\n",
    "\n",
    "print('train_x shape :', train_x.shape)\n",
    "print('train_y shape :', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-infection",
   "metadata": {},
   "source": [
    "* position_(i) 에 해당하는 input token의 target이 position_(i+1)의 token이 되도록 input target 구성\n",
    "\n",
    "```Python\n",
    "# from keras\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "applied-alaska",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  13,   20,  221,   10,  215,    3,   17,    5, 1135,    0,    0,\n",
       "           0,    4,   12,   58,  381,   25,   13,   20,   51,   10,   36,\n",
       "           3,    2, 9157,    7,    2,    0, 1771,    3,    2, 1072,   18,\n",
       "         693,    3,    2, 1845,    7, 5825,    3,    2, 3266,   49,  264,\n",
       "          10,  153,    2,  267,    3,    2, 9102,    7,  159,   34,   15,\n",
       "        4066, 6364,    2,  125,    7,  103,    0,   52,    3,    2, 8665,\n",
       "           7, 2720,    2, 2123, 1833,   45,    2,  352,    7,    0, 1450,\n",
       "           0,    3,    2]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "presidential-woman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  20,  221,   10,  215,    3,   17,    5, 1135,    0,    0,    0,\n",
       "           4,   12,   58,  381,   25,   13,   20,   51,   10,   36,    3,\n",
       "           2, 9157,    7,    2,    0, 1771,    3,    2, 1072,   18,  693,\n",
       "           3,    2, 1845,    7, 5825,    3,    2, 3266,   49,  264,   10,\n",
       "         153,    2,  267,    3,    2, 9102,    7,  159,   34,   15, 4066,\n",
       "        6364,    2,  125,    7,  103,    0,   52,    3,    2, 8665,    7,\n",
       "        2720,    2, 2123, 1833,   45,    2,  352,    7,    0, 1450,    0,\n",
       "           3,    2, 3312]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "noticed-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextGenDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_tokens, target_tokens):\n",
    "        self.input_tokens = input_tokens\n",
    "        self.target_tokens = target_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inputs = self.input_tokens[idx]\n",
    "        targets = self.target_tokens[idx]\n",
    "        \n",
    "        output = {\"inputs\": inputs,\n",
    "                  \"targets\": targets}\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "seeing-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_train_x = train_x[:5000]\n",
    "# sample_train_y = train_y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dated-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = TextGenDataset(train_x, train_y)\n",
    "# train_dataset = TextGenDataset(sample_train_x, sample_train_y)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "lovely-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'inputs': tensor([[   17,     5,   710,  ..., 10601,  3124,  1747],\n",
      "        [  528,    55,     5,  ...,  1210,     4,    40],\n",
      "        [   12,   249,  1215,  ...,    15,    43,    90],\n",
      "        ...,\n",
      "        [ 1531,     3,    13,  ...,   420,  7132,     6],\n",
      "        [   10,    15,   398,  ...,    20,     4,     2],\n",
      "        [   13,     9,    40,  ...,    21,  1842,     6]]), 'targets': tensor([[    5,   710,   278,  ...,  3124,  1747,  1182],\n",
      "        [   55,     5,  9659,  ...,     4,    40,  2666],\n",
      "        [  249,  1215,     6,  ...,    43,    90,    12],\n",
      "        ...,\n",
      "        [    3,    13,    20,  ...,  7132,     6,   357],\n",
      "        [   15,   398,    98,  ...,     4,     2,   214],\n",
      "        [    9,    40, 10010,  ...,  1842,     6,  3164]])}\n"
     ]
    }
   ],
   "source": [
    "print(iter(train_loader).next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "mysterious-westminster",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-copying",
   "metadata": {},
   "source": [
    "### Implement an embedding layer\n",
    "\n",
    "Create two seperate embedding layers: one for tokens and one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "electrical-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embed_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param max_len: max length of seqeunce\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(num_embeddings=max_len, embedding_dim=embed_size)\n",
    "\n",
    "    def forward(self, sequence, device): # [128, 80]\n",
    "\n",
    "        max_seq_len = sequence.shape[-1]\n",
    "\n",
    "        positions = torch.tensor(range(max_seq_len)).to(device)\n",
    "        positions = self.pos_emb(positions)\n",
    "        emb_out = self.token_emb(sequence) + positions # torch.Size([128, 80, 256])\n",
    "        return emb_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-hometown",
   "metadata": {},
   "source": [
    "### Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "wireless-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SamLynnEvans/Transformer/tree/e06ae2810f119c75aa34585442872026875e6462\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, q, k, v, d_k, mask=None, dropout=None):\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "                \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "\n",
    "       # calculate attention using function we will define next\n",
    "        scores = self.attention(q, k, v, self.d_k, mask)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout = 0.0):\n",
    "        super().__init__() \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-uniform",
   "metadata": {},
   "source": [
    "#### Decoder의 Masked Self-attention 을 위한 Causal mask 생성\n",
    "https://machinereads.wordpress.com/2020/05/10/xlnet-generalized-autoregressive-pretraining-for-language-understanding-1-3/\n",
    "\n",
    "**질문?** query, key의 길이가 달라지는 경우가 있는지?\n",
    "- Keras 구현에 의하면, query, key의 길이를 각각 받아서 mask 생성\n",
    "- Self-attention이면 query 길이 하나만 받아도 되지 않나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "desperate-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#  Keras Implementation\n",
    "#####\n",
    "\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    \n",
    "    return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "spiritual-dairy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 5), dtype=bool, numpy=\n",
       "array([[[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]],\n",
       "\n",
       "       [[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masked self attention\n",
    "\n",
    "causal_mask = causal_attention_mask(batch_size=2, n_dest=5, n_src=5, dtype=tf.bool)\n",
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "hawaiian-status",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 5), dtype=bool, numpy=\n",
       "array([[[ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,3,5, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "visible-stone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 3), dtype=bool, numpy=\n",
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [ True, False, False],\n",
       "        [ True,  True, False],\n",
       "        [ True,  True,  True]]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,5,3, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "confident-locking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 5), dtype=bool, numpy=\n",
       "array([[[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]]])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_mask(1,5,5, dtype=tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dependent-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#  Simple Implementation\n",
    "#####\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.tril.html\n",
    "\n",
    "def simple_causal_attention_mask(batch_size, n_dest, n_src):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "#     print(np.ones((batch_size, n_dest, n_src)))\n",
    "#     print(np.tril(np.ones((batch_size, n_dest, n_src))))\n",
    "    mask = np.tril(np.ones((batch_size, n_dest, n_src)), n_src-n_dest)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "initial-landscape",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-5=-2, 음수 값 만큼 내려가서 diagonal 시작\n",
    "simple_causal_attention_mask(1,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "contained-kennedy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-3=2, 양수 값 만큼 올라간 후 diagonal 시작\n",
    "simple_causal_attention_mask(1,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "comparative-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! No Encoder-decoder attention\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, seq_len, d_model, heads, d_ff, dropout = 0.0):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, batch_size, x, device, mask=None):\n",
    "        \n",
    "        causal_mask = simple_causal_attention_mask(batch_size, self.seq_len, self.seq_len)\n",
    "        causal_mask = torch.tensor(np.array(causal_mask)).to(device)\n",
    "        \n",
    "        # self attention + add&norm\n",
    "        attention_output = self.attn(x, x, x, mask=causal_mask) # Masked self attention\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        masked_self_att_out = self.norm_1(x + attention_output)\n",
    "\n",
    "        # ffn + add&norm\n",
    "        ffn_output = self.ff(masked_self_att_out)\n",
    "        ffn_output = self.dropout_2(ffn_output)\n",
    "        out = self.norm_2(masked_self_att_out + ffn_output)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-moment",
   "metadata": {},
   "source": [
    "### Implement the miniature GPT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "sized-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, device, vocab_size, max_seq_len, d_model, heads, d_ff):\n",
    "        super(MiniGPT, self).__init__()\n",
    "        self.device = device\n",
    "        self.token_pos_embedding = TokenAndPositionEmbedding(vocab_size=vocab_size, max_len=max_seq_len, embed_size=d_model)\n",
    "        self.transformer_block = TransformerDecoderLayer(seq_len=max_seq_len, d_model=d_model, heads=heads, d_ff=d_ff, dropout=0.1)\n",
    "        self.ff = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, batch_size, inputs):\n",
    "        x = self.token_pos_embedding(inputs, self.device) \n",
    "        x = self.transformer_block(batch_size, x, self.device)\n",
    "        x = self.ff(x) # torch.Size([batch_size, max_seq_len, vocab_size])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-bradley",
   "metadata": {},
   "source": [
    "### Implement a callback for generating text -> PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "variable-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : reimp not using tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "class TextGenerator:\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, device, max_seq_len, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        \n",
    "    # random choince from top_k predicted tokens\n",
    "    # logits : (20000,)\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = self.max_seq_len - len(start_tokens)\n",
    "            # current token index\n",
    "            current_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:self.max_seq_len]\n",
    "                current_index = self.max_seq_len - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = torch.tensor([x]).to(self.device)\n",
    "            y = self.model(x.shape[0], x)\n",
    "            y = y.to('cpu').detach().numpy() # torch.Size([128, 80, 20000])\n",
    "            \n",
    "            sample_token = self.sample_from(y[0][current_index]) # token id\n",
    "            \n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "            \n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-denmark",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "lesbian-beads",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device : cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "max_seq_len = 80  # Max sequence size\n",
    "d_model = 256  # Embedding size for each token\n",
    "heads = 2  # Number of attention heads\n",
    "d_ff = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "# https://githubmemory.com/repo/arogozhnikov/einops/issues\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print ('Current device :', device)\n",
    "model = MiniGPT(device=device, vocab_size=vocab_size, max_seq_len=max_seq_len, d_model=d_model, heads=heads, d_ff=d_ff)\n",
    "\n",
    "learning_rate = 0.001 \n",
    "\n",
    "criterion =  nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "applicable-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-northern",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282f210e5fb643388d905ca6307b47ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [4/196], Loss: 9.4184\n",
      "Epoch [1/25], Step [9/196], Loss: 7.7152\n",
      "Epoch [1/25], Step [14/196], Loss: 6.8842\n",
      "Epoch [1/25], Step [19/196], Loss: 6.5720\n",
      "Epoch [1/25], Step [24/196], Loss: 6.6629\n",
      "Epoch [1/25], Step [29/196], Loss: 6.7069\n",
      "Epoch [1/25], Step [34/196], Loss: 6.5952\n",
      "Epoch [1/25], Step [39/196], Loss: 6.6938\n",
      "Epoch [1/25], Step [44/196], Loss: 6.5765\n",
      "Epoch [1/25], Step [49/196], Loss: 6.5991\n",
      "Epoch [1/25], Step [54/196], Loss: 6.6001\n",
      "Epoch [1/25], Step [59/196], Loss: 6.5826\n",
      "Epoch [1/25], Step [64/196], Loss: 6.5045\n",
      "Epoch [1/25], Step [69/196], Loss: 6.5399\n",
      "Epoch [1/25], Step [74/196], Loss: 6.5148\n",
      "Epoch [1/25], Step [79/196], Loss: 6.4928\n",
      "Epoch [1/25], Step [84/196], Loss: 6.4652\n",
      "Epoch [1/25], Step [89/196], Loss: 6.4218\n",
      "Epoch [1/25], Step [94/196], Loss: 6.3699\n",
      "Epoch [1/25], Step [99/196], Loss: 6.4072\n",
      "Epoch [1/25], Step [104/196], Loss: 6.2384\n",
      "Epoch [1/25], Step [109/196], Loss: 6.2437\n",
      "Epoch [1/25], Step [114/196], Loss: 6.2327\n",
      "Epoch [1/25], Step [119/196], Loss: 6.1221\n",
      "Epoch [1/25], Step [124/196], Loss: 6.0892\n",
      "Epoch [1/25], Step [129/196], Loss: 6.0782\n",
      "Epoch [1/25], Step [134/196], Loss: 6.0676\n",
      "Epoch [1/25], Step [139/196], Loss: 6.1763\n",
      "Epoch [1/25], Step [144/196], Loss: 6.0658\n",
      "Epoch [1/25], Step [149/196], Loss: 6.1480\n",
      "Epoch [1/25], Step [154/196], Loss: 6.0549\n",
      "Epoch [1/25], Step [159/196], Loss: 5.9970\n",
      "Epoch [1/25], Step [164/196], Loss: 6.0521\n",
      "Epoch [1/25], Step [169/196], Loss: 5.9888\n",
      "Epoch [1/25], Step [174/196], Loss: 5.9182\n",
      "Epoch [1/25], Step [179/196], Loss: 5.9577\n",
      "Epoch [1/25], Step [184/196], Loss: 5.7915\n",
      "Epoch [1/25], Step [189/196], Loss: 5.9589\n",
      "Epoch [1/25], Step [194/196], Loss: 5.9201\n",
      ">> Epoch 1\n",
      "generated text:\n",
      "this movie is just . . this is a very to have just a film . i had i love , i saw i have been to see the movie is one of a bad as they think i thought that it 's one\n",
      "\n",
      "None\n",
      "Epoch [2/25], Step [4/196], Loss: 5.8840\n",
      "Epoch [2/25], Step [9/196], Loss: 5.7398\n",
      "Epoch [2/25], Step [14/196], Loss: 5.8023\n",
      "Epoch [2/25], Step [19/196], Loss: 5.8163\n",
      "Epoch [2/25], Step [24/196], Loss: 5.7820\n",
      "Epoch [2/25], Step [29/196], Loss: 5.6901\n",
      "Epoch [2/25], Step [34/196], Loss: 5.6341\n",
      "Epoch [2/25], Step [39/196], Loss: 5.6866\n",
      "Epoch [2/25], Step [44/196], Loss: 5.6026\n",
      "Epoch [2/25], Step [49/196], Loss: 5.6476\n",
      "Epoch [2/25], Step [54/196], Loss: 5.6678\n",
      "Epoch [2/25], Step [59/196], Loss: 5.7582\n",
      "Epoch [2/25], Step [64/196], Loss: 5.6288\n",
      "Epoch [2/25], Step [69/196], Loss: 5.6218\n",
      "Epoch [2/25], Step [74/196], Loss: 5.5870\n",
      "Epoch [2/25], Step [79/196], Loss: 5.6372\n",
      "Epoch [2/25], Step [84/196], Loss: 5.6177\n",
      "Epoch [2/25], Step [89/196], Loss: 5.6044\n",
      "Epoch [2/25], Step [94/196], Loss: 5.5915\n",
      "Epoch [2/25], Step [99/196], Loss: 5.6542\n",
      "Epoch [2/25], Step [104/196], Loss: 5.6357\n",
      "Epoch [2/25], Step [109/196], Loss: 5.6452\n",
      "Epoch [2/25], Step [114/196], Loss: 5.5770\n",
      "Epoch [2/25], Step [119/196], Loss: 5.4972\n",
      "Epoch [2/25], Step [124/196], Loss: 5.5186\n",
      "Epoch [2/25], Step [129/196], Loss: 5.6086\n",
      "Epoch [2/25], Step [134/196], Loss: 5.5108\n",
      "Epoch [2/25], Step [139/196], Loss: 5.4909\n",
      "Epoch [2/25], Step [144/196], Loss: 5.5426\n",
      "Epoch [2/25], Step [149/196], Loss: 5.5050\n",
      "Epoch [2/25], Step [154/196], Loss: 5.4616\n",
      "Epoch [2/25], Step [159/196], Loss: 5.4644\n",
      "Epoch [2/25], Step [164/196], Loss: 5.3892\n",
      "Epoch [2/25], Step [169/196], Loss: 5.4351\n",
      "Epoch [2/25], Step [174/196], Loss: 5.4303\n",
      "Epoch [2/25], Step [179/196], Loss: 5.3877\n",
      "Epoch [2/25], Step [184/196], Loss: 5.4154\n",
      "Epoch [2/25], Step [189/196], Loss: 5.5157\n",
      "Epoch [2/25], Step [194/196], Loss: 5.4255\n",
      ">> Epoch 2\n",
      "generated text:\n",
      "this movie is the most of a film . i have to be the movie of the original story , and a very little girl to get a lot of the movie that it 's very good . but the film is a bit\n",
      "\n",
      "None\n",
      "Epoch [3/25], Step [4/196], Loss: 5.2716\n",
      "Epoch [3/25], Step [9/196], Loss: 5.2495\n",
      "Epoch [3/25], Step [14/196], Loss: 5.1400\n",
      "Epoch [3/25], Step [19/196], Loss: 5.2849\n",
      "Epoch [3/25], Step [24/196], Loss: 5.2880\n",
      "Epoch [3/25], Step [29/196], Loss: 5.2890\n",
      "Epoch [3/25], Step [34/196], Loss: 5.2709\n",
      "Epoch [3/25], Step [39/196], Loss: 5.3182\n",
      "Epoch [3/25], Step [44/196], Loss: 5.2314\n",
      "Epoch [3/25], Step [49/196], Loss: 5.2784\n",
      "Epoch [3/25], Step [54/196], Loss: 5.1408\n",
      "Epoch [3/25], Step [59/196], Loss: 5.2520\n",
      "Epoch [3/25], Step [64/196], Loss: 5.3000\n",
      "Epoch [3/25], Step [69/196], Loss: 5.2765\n",
      "Epoch [3/25], Step [74/196], Loss: 5.2352\n",
      "Epoch [3/25], Step [79/196], Loss: 5.2493\n",
      "Epoch [3/25], Step [84/196], Loss: 5.1889\n",
      "Epoch [3/25], Step [89/196], Loss: 5.2167\n",
      "Epoch [3/25], Step [94/196], Loss: 5.2174\n",
      "Epoch [3/25], Step [99/196], Loss: 5.2077\n",
      "Epoch [3/25], Step [104/196], Loss: 5.1571\n",
      "Epoch [3/25], Step [109/196], Loss: 5.1275\n",
      "Epoch [3/25], Step [114/196], Loss: 5.2715\n",
      "Epoch [3/25], Step [119/196], Loss: 5.1425\n",
      "Epoch [3/25], Step [124/196], Loss: 5.2510\n",
      "Epoch [3/25], Step [129/196], Loss: 5.1768\n",
      "Epoch [3/25], Step [134/196], Loss: 5.0758\n",
      "Epoch [3/25], Step [139/196], Loss: 5.1405\n",
      "Epoch [3/25], Step [144/196], Loss: 5.1959\n",
      "Epoch [3/25], Step [149/196], Loss: 5.1998\n",
      "Epoch [3/25], Step [154/196], Loss: 5.1357\n",
      "Epoch [3/25], Step [159/196], Loss: 5.1727\n",
      "Epoch [3/25], Step [164/196], Loss: 5.2089\n",
      "Epoch [3/25], Step [169/196], Loss: 5.1295\n",
      "Epoch [3/25], Step [174/196], Loss: 5.0388\n",
      "Epoch [3/25], Step [179/196], Loss: 5.1941\n",
      "Epoch [3/25], Step [184/196], Loss: 5.1659\n",
      "Epoch [3/25], Step [189/196], Loss: 5.2054\n",
      "Epoch [3/25], Step [194/196], Loss: 5.1158\n",
      ">> Epoch 3\n",
      "generated text:\n",
      "this movie is the only the original film . and one of my favorite films of all time ! i thought that movie was going to be a little kid that a very funny . i was so bad i like that . it\n",
      "\n",
      "None\n",
      "Epoch [4/25], Step [4/196], Loss: 4.9444\n",
      "Epoch [4/25], Step [9/196], Loss: 4.9600\n",
      "Epoch [4/25], Step [14/196], Loss: 4.9953\n",
      "Epoch [4/25], Step [19/196], Loss: 4.8766\n",
      "Epoch [4/25], Step [24/196], Loss: 5.0371\n",
      "Epoch [4/25], Step [29/196], Loss: 4.9451\n",
      "Epoch [4/25], Step [34/196], Loss: 4.9754\n",
      "Epoch [4/25], Step [39/196], Loss: 4.9468\n",
      "Epoch [4/25], Step [44/196], Loss: 4.9914\n",
      "Epoch [4/25], Step [49/196], Loss: 5.0056\n",
      "Epoch [4/25], Step [54/196], Loss: 4.9664\n",
      "Epoch [4/25], Step [59/196], Loss: 4.9422\n",
      "Epoch [4/25], Step [64/196], Loss: 4.9660\n",
      "Epoch [4/25], Step [69/196], Loss: 4.9986\n",
      "Epoch [4/25], Step [74/196], Loss: 5.0005\n",
      "Epoch [4/25], Step [79/196], Loss: 4.9199\n",
      "Epoch [4/25], Step [84/196], Loss: 5.0496\n",
      "Epoch [4/25], Step [89/196], Loss: 5.0179\n",
      "Epoch [4/25], Step [94/196], Loss: 4.9428\n",
      "Epoch [4/25], Step [99/196], Loss: 4.9302\n",
      "Epoch [4/25], Step [104/196], Loss: 5.0399\n",
      "Epoch [4/25], Step [109/196], Loss: 4.9317\n",
      "Epoch [4/25], Step [114/196], Loss: 4.9496\n",
      "Epoch [4/25], Step [119/196], Loss: 4.9200\n",
      "Epoch [4/25], Step [124/196], Loss: 5.0099\n",
      "Epoch [4/25], Step [129/196], Loss: 4.9624\n",
      "Epoch [4/25], Step [134/196], Loss: 4.8931\n",
      "Epoch [4/25], Step [139/196], Loss: 4.9496\n",
      "Epoch [4/25], Step [144/196], Loss: 5.0258\n",
      "Epoch [4/25], Step [149/196], Loss: 4.9755\n",
      "Epoch [4/25], Step [154/196], Loss: 4.9837\n",
      "Epoch [4/25], Step [159/196], Loss: 4.9459\n",
      "Epoch [4/25], Step [164/196], Loss: 4.9509\n",
      "Epoch [4/25], Step [169/196], Loss: 4.9930\n",
      "Epoch [4/25], Step [174/196], Loss: 5.0005\n",
      "Epoch [4/25], Step [179/196], Loss: 5.0346\n",
      "Epoch [4/25], Step [184/196], Loss: 4.9913\n",
      "Epoch [4/25], Step [189/196], Loss: 4.9403\n",
      "Epoch [4/25], Step [194/196], Loss: 4.9330\n",
      ">> Epoch 4\n",
      "generated text:\n",
      "this movie is a movie , and the acting was bad ! the plot that makes it really good . it was a good way they didn 't make this film to me think it was the worst and i have ever seen .\n",
      "\n",
      "None\n",
      "Epoch [5/25], Step [4/196], Loss: 4.8167\n",
      "Epoch [5/25], Step [9/196], Loss: 4.8271\n",
      "Epoch [5/25], Step [14/196], Loss: 4.7830\n",
      "Epoch [5/25], Step [19/196], Loss: 4.7949\n",
      "Epoch [5/25], Step [24/196], Loss: 4.7107\n",
      "Epoch [5/25], Step [29/196], Loss: 4.7919\n",
      "Epoch [5/25], Step [34/196], Loss: 4.7910\n",
      "Epoch [5/25], Step [39/196], Loss: 4.8423\n",
      "Epoch [5/25], Step [44/196], Loss: 4.7616\n",
      "Epoch [5/25], Step [49/196], Loss: 4.8045\n",
      "Epoch [5/25], Step [54/196], Loss: 4.7443\n",
      "Epoch [5/25], Step [59/196], Loss: 4.7384\n",
      "Epoch [5/25], Step [64/196], Loss: 4.7367\n",
      "Epoch [5/25], Step [69/196], Loss: 4.8042\n",
      "Epoch [5/25], Step [74/196], Loss: 4.7966\n",
      "Epoch [5/25], Step [79/196], Loss: 4.8472\n",
      "Epoch [5/25], Step [84/196], Loss: 4.8716\n",
      "Epoch [5/25], Step [89/196], Loss: 4.8247\n",
      "Epoch [5/25], Step [94/196], Loss: 4.8122\n",
      "Epoch [5/25], Step [99/196], Loss: 4.7958\n",
      "Epoch [5/25], Step [104/196], Loss: 4.7525\n",
      "Epoch [5/25], Step [109/196], Loss: 4.7827\n",
      "Epoch [5/25], Step [114/196], Loss: 4.8075\n",
      "Epoch [5/25], Step [119/196], Loss: 4.8239\n",
      "Epoch [5/25], Step [124/196], Loss: 4.7991\n",
      "Epoch [5/25], Step [129/196], Loss: 4.8203\n",
      "Epoch [5/25], Step [134/196], Loss: 4.8057\n",
      "Epoch [5/25], Step [139/196], Loss: 4.6859\n",
      "Epoch [5/25], Step [144/196], Loss: 4.8569\n",
      "Epoch [5/25], Step [149/196], Loss: 4.9564\n",
      "Epoch [5/25], Step [154/196], Loss: 4.8233\n",
      "Epoch [5/25], Step [159/196], Loss: 4.7462\n",
      "Epoch [5/25], Step [164/196], Loss: 4.8092\n",
      "Epoch [5/25], Step [169/196], Loss: 4.8437\n",
      "Epoch [5/25], Step [174/196], Loss: 4.8175\n",
      "Epoch [5/25], Step [179/196], Loss: 4.7728\n",
      "Epoch [5/25], Step [184/196], Loss: 4.8502\n",
      "Epoch [5/25], Step [189/196], Loss: 4.7945\n",
      "Epoch [5/25], Step [194/196], Loss: 4.7990\n",
      ">> Epoch 5\n",
      "generated text:\n",
      "this movie is a great movie i 've seen the movie . there is an amazing movie that 's only reason why the movie was a very good film . it was just about the story was a nice , but it 's not\n",
      "\n",
      "None\n",
      "Epoch [6/25], Step [4/196], Loss: 4.6195\n",
      "Epoch [6/25], Step [9/196], Loss: 4.6774\n",
      "Epoch [6/25], Step [14/196], Loss: 4.6830\n",
      "Epoch [6/25], Step [19/196], Loss: 4.6134\n",
      "Epoch [6/25], Step [24/196], Loss: 4.5991\n",
      "Epoch [6/25], Step [29/196], Loss: 4.6866\n",
      "Epoch [6/25], Step [34/196], Loss: 4.6494\n",
      "Epoch [6/25], Step [39/196], Loss: 4.6324\n",
      "Epoch [6/25], Step [44/196], Loss: 4.6699\n",
      "Epoch [6/25], Step [49/196], Loss: 4.6205\n",
      "Epoch [6/25], Step [54/196], Loss: 4.7441\n",
      "Epoch [6/25], Step [59/196], Loss: 4.6409\n",
      "Epoch [6/25], Step [64/196], Loss: 4.6502\n",
      "Epoch [6/25], Step [69/196], Loss: 4.6154\n",
      "Epoch [6/25], Step [74/196], Loss: 4.6902\n",
      "Epoch [6/25], Step [79/196], Loss: 4.6590\n",
      "Epoch [6/25], Step [84/196], Loss: 4.5995\n",
      "Epoch [6/25], Step [89/196], Loss: 4.6467\n",
      "Epoch [6/25], Step [94/196], Loss: 4.6203\n",
      "Epoch [6/25], Step [99/196], Loss: 4.6811\n",
      "Epoch [6/25], Step [104/196], Loss: 4.6583\n",
      "Epoch [6/25], Step [109/196], Loss: 4.6426\n",
      "Epoch [6/25], Step [114/196], Loss: 4.6861\n",
      "Epoch [6/25], Step [119/196], Loss: 4.7539\n",
      "Epoch [6/25], Step [124/196], Loss: 4.6226\n",
      "Epoch [6/25], Step [129/196], Loss: 4.6794\n",
      "Epoch [6/25], Step [134/196], Loss: 4.7280\n",
      "Epoch [6/25], Step [139/196], Loss: 4.7169\n",
      "Epoch [6/25], Step [144/196], Loss: 4.6947\n",
      "Epoch [6/25], Step [149/196], Loss: 4.7143\n",
      "Epoch [6/25], Step [154/196], Loss: 4.6728\n",
      "Epoch [6/25], Step [159/196], Loss: 4.7053\n",
      "Epoch [6/25], Step [164/196], Loss: 4.7130\n",
      "Epoch [6/25], Step [169/196], Loss: 4.6867\n",
      "Epoch [6/25], Step [174/196], Loss: 4.7589\n",
      "Epoch [6/25], Step [179/196], Loss: 4.6166\n",
      "Epoch [6/25], Step [184/196], Loss: 4.6747\n",
      "Epoch [6/25], Step [189/196], Loss: 4.7134\n",
      "Epoch [6/25], Step [194/196], Loss: 4.6761\n",
      ">> Epoch 6\n",
      "generated text:\n",
      "this movie is very very bad movie and funny . it 's about how it makes me wonder how bad this movie could have been a movie with some people . i think i had seen it . first came out to be a\n",
      "\n",
      "None\n",
      "Epoch [7/25], Step [4/196], Loss: 4.4911\n",
      "Epoch [7/25], Step [9/196], Loss: 4.5289\n",
      "Epoch [7/25], Step [14/196], Loss: 4.5406\n",
      "Epoch [7/25], Step [19/196], Loss: 4.5975\n",
      "Epoch [7/25], Step [24/196], Loss: 4.4824\n",
      "Epoch [7/25], Step [29/196], Loss: 4.4981\n",
      "Epoch [7/25], Step [34/196], Loss: 4.5310\n",
      "Epoch [7/25], Step [39/196], Loss: 4.5392\n",
      "Epoch [7/25], Step [44/196], Loss: 4.5032\n",
      "Epoch [7/25], Step [49/196], Loss: 4.5371\n",
      "Epoch [7/25], Step [54/196], Loss: 4.5902\n",
      "Epoch [7/25], Step [59/196], Loss: 4.5441\n",
      "Epoch [7/25], Step [64/196], Loss: 4.5165\n",
      "Epoch [7/25], Step [69/196], Loss: 4.5618\n",
      "Epoch [7/25], Step [74/196], Loss: 4.5389\n",
      "Epoch [7/25], Step [79/196], Loss: 4.5745\n",
      "Epoch [7/25], Step [84/196], Loss: 4.6029\n",
      "Epoch [7/25], Step [89/196], Loss: 4.5408\n",
      "Epoch [7/25], Step [94/196], Loss: 4.5720\n",
      "Epoch [7/25], Step [99/196], Loss: 4.5785\n",
      "Epoch [7/25], Step [104/196], Loss: 4.5907\n",
      "Epoch [7/25], Step [109/196], Loss: 4.5170\n",
      "Epoch [7/25], Step [114/196], Loss: 4.5327\n",
      "Epoch [7/25], Step [119/196], Loss: 4.6055\n",
      "Epoch [7/25], Step [124/196], Loss: 4.4911\n",
      "Epoch [7/25], Step [129/196], Loss: 4.5843\n",
      "Epoch [7/25], Step [134/196], Loss: 4.5568\n",
      "Epoch [7/25], Step [139/196], Loss: 4.5498\n",
      "Epoch [7/25], Step [144/196], Loss: 4.5983\n",
      "Epoch [7/25], Step [149/196], Loss: 4.6209\n",
      "Epoch [7/25], Step [154/196], Loss: 4.5492\n",
      "Epoch [7/25], Step [159/196], Loss: 4.5763\n",
      "Epoch [7/25], Step [164/196], Loss: 4.6243\n",
      "Epoch [7/25], Step [169/196], Loss: 4.6132\n",
      "Epoch [7/25], Step [174/196], Loss: 4.5495\n",
      "Epoch [7/25], Step [179/196], Loss: 4.5449\n",
      "Epoch [7/25], Step [184/196], Loss: 4.5381\n",
      "Epoch [7/25], Step [189/196], Loss: 4.6118\n",
      "Epoch [7/25], Step [194/196], Loss: 4.5504\n",
      ">> Epoch 7\n",
      "generated text:\n",
      "this movie is a very bad movie . it 's just a movie . i loved it . and i don 't remember it when it 's just a bad movie . the acting , and bad acting , bad writing . this story\n",
      "\n",
      "None\n",
      "Epoch [8/25], Step [4/196], Loss: 4.4207\n",
      "Epoch [8/25], Step [9/196], Loss: 4.3277\n",
      "Epoch [8/25], Step [14/196], Loss: 4.4270\n",
      "Epoch [8/25], Step [19/196], Loss: 4.3347\n",
      "Epoch [8/25], Step [24/196], Loss: 4.3857\n",
      "Epoch [8/25], Step [29/196], Loss: 4.4905\n",
      "Epoch [8/25], Step [34/196], Loss: 4.4298\n",
      "Epoch [8/25], Step [39/196], Loss: 4.4250\n",
      "Epoch [8/25], Step [44/196], Loss: 4.4341\n",
      "Epoch [8/25], Step [49/196], Loss: 4.4606\n",
      "Epoch [8/25], Step [54/196], Loss: 4.4553\n",
      "Epoch [8/25], Step [59/196], Loss: 4.4557\n",
      "Epoch [8/25], Step [64/196], Loss: 4.4293\n",
      "Epoch [8/25], Step [69/196], Loss: 4.4758\n",
      "Epoch [8/25], Step [74/196], Loss: 4.4150\n",
      "Epoch [8/25], Step [79/196], Loss: 4.4509\n",
      "Epoch [8/25], Step [84/196], Loss: 4.4377\n",
      "Epoch [8/25], Step [89/196], Loss: 4.4835\n",
      "Epoch [8/25], Step [94/196], Loss: 4.4179\n",
      "Epoch [8/25], Step [99/196], Loss: 4.4636\n",
      "Epoch [8/25], Step [104/196], Loss: 4.4782\n",
      "Epoch [8/25], Step [109/196], Loss: 4.5454\n",
      "Epoch [8/25], Step [114/196], Loss: 4.4644\n",
      "Epoch [8/25], Step [119/196], Loss: 4.4501\n",
      "Epoch [8/25], Step [124/196], Loss: 4.4709\n",
      "Epoch [8/25], Step [129/196], Loss: 4.4207\n",
      "Epoch [8/25], Step [134/196], Loss: 4.4389\n",
      "Epoch [8/25], Step [139/196], Loss: 4.5151\n",
      "Epoch [8/25], Step [144/196], Loss: 4.4336\n",
      "Epoch [8/25], Step [149/196], Loss: 4.5218\n",
      "Epoch [8/25], Step [154/196], Loss: 4.5464\n",
      "Epoch [8/25], Step [159/196], Loss: 4.4080\n",
      "Epoch [8/25], Step [164/196], Loss: 4.5956\n",
      "Epoch [8/25], Step [169/196], Loss: 4.5349\n",
      "Epoch [8/25], Step [174/196], Loss: 4.5682\n",
      "Epoch [8/25], Step [179/196], Loss: 4.4858\n",
      "Epoch [8/25], Step [184/196], Loss: 4.5202\n",
      "Epoch [8/25], Step [189/196], Loss: 4.5869\n",
      "Epoch [8/25], Step [194/196], Loss: 4.5272\n",
      ">> Epoch 8\n",
      "generated text:\n",
      "this movie is one of the worst films i have ever seen as a single worst film i have ever seen . there is a little girl , i 've never seen this film . it was made in my life . this time\n",
      "\n",
      "None\n",
      "Epoch [9/25], Step [4/196], Loss: 4.3003\n",
      "Epoch [9/25], Step [9/196], Loss: 4.2758\n",
      "Epoch [9/25], Step [14/196], Loss: 4.2987\n",
      "Epoch [9/25], Step [19/196], Loss: 4.3169\n",
      "Epoch [9/25], Step [24/196], Loss: 4.2924\n",
      "Epoch [9/25], Step [29/196], Loss: 4.3774\n",
      "Epoch [9/25], Step [34/196], Loss: 4.3537\n",
      "Epoch [9/25], Step [39/196], Loss: 4.3682\n",
      "Epoch [9/25], Step [44/196], Loss: 4.3669\n",
      "Epoch [9/25], Step [49/196], Loss: 4.3550\n",
      "Epoch [9/25], Step [54/196], Loss: 4.3441\n",
      "Epoch [9/25], Step [59/196], Loss: 4.3882\n",
      "Epoch [9/25], Step [64/196], Loss: 4.2769\n",
      "Epoch [9/25], Step [69/196], Loss: 4.3685\n",
      "Epoch [9/25], Step [74/196], Loss: 4.3197\n",
      "Epoch [9/25], Step [79/196], Loss: 4.3312\n",
      "Epoch [9/25], Step [84/196], Loss: 4.4565\n",
      "Epoch [9/25], Step [89/196], Loss: 4.3463\n",
      "Epoch [9/25], Step [94/196], Loss: 4.3007\n",
      "Epoch [9/25], Step [99/196], Loss: 4.3914\n",
      "Epoch [9/25], Step [104/196], Loss: 4.3995\n",
      "Epoch [9/25], Step [109/196], Loss: 4.3863\n",
      "Epoch [9/25], Step [114/196], Loss: 4.4111\n",
      "Epoch [9/25], Step [119/196], Loss: 4.3802\n",
      "Epoch [9/25], Step [124/196], Loss: 4.4555\n",
      "Epoch [9/25], Step [129/196], Loss: 4.3955\n",
      "Epoch [9/25], Step [134/196], Loss: 4.4036\n",
      "Epoch [9/25], Step [139/196], Loss: 4.4837\n",
      "Epoch [9/25], Step [144/196], Loss: 4.4026\n",
      "Epoch [9/25], Step [149/196], Loss: 4.4121\n",
      "Epoch [9/25], Step [154/196], Loss: 4.4795\n",
      "Epoch [9/25], Step [159/196], Loss: 4.4195\n",
      "Epoch [9/25], Step [164/196], Loss: 4.4298\n",
      "Epoch [9/25], Step [169/196], Loss: 4.4031\n",
      "Epoch [9/25], Step [174/196], Loss: 4.4477\n",
      "Epoch [9/25], Step [179/196], Loss: 4.4160\n",
      "Epoch [9/25], Step [184/196], Loss: 4.3807\n",
      "Epoch [9/25], Step [189/196], Loss: 4.4080\n",
      "Epoch [9/25], Step [194/196], Loss: 4.4203\n",
      ">> Epoch 9\n",
      "generated text:\n",
      "this movie is so bad it hurts it to the end , i didn 't have it to see it again . this is a bad movie . it has not to make it as it is , and it 's good . the\n",
      "\n",
      "None\n",
      "Epoch [10/25], Step [4/196], Loss: 4.2427\n",
      "Epoch [10/25], Step [9/196], Loss: 4.1903\n",
      "Epoch [10/25], Step [14/196], Loss: 4.2404\n",
      "Epoch [10/25], Step [19/196], Loss: 4.2775\n",
      "Epoch [10/25], Step [24/196], Loss: 4.2553\n",
      "Epoch [10/25], Step [29/196], Loss: 4.2604\n",
      "Epoch [10/25], Step [34/196], Loss: 4.2511\n",
      "Epoch [10/25], Step [39/196], Loss: 4.2533\n",
      "Epoch [10/25], Step [44/196], Loss: 4.3134\n",
      "Epoch [10/25], Step [49/196], Loss: 4.2506\n",
      "Epoch [10/25], Step [54/196], Loss: 4.2641\n",
      "Epoch [10/25], Step [59/196], Loss: 4.2748\n",
      "Epoch [10/25], Step [64/196], Loss: 4.3524\n",
      "Epoch [10/25], Step [69/196], Loss: 4.2287\n",
      "Epoch [10/25], Step [74/196], Loss: 4.3400\n",
      "Epoch [10/25], Step [79/196], Loss: 4.3087\n",
      "Epoch [10/25], Step [84/196], Loss: 4.3124\n",
      "Epoch [10/25], Step [89/196], Loss: 4.2989\n",
      "Epoch [10/25], Step [94/196], Loss: 4.3626\n",
      "Epoch [10/25], Step [99/196], Loss: 4.3013\n",
      "Epoch [10/25], Step [104/196], Loss: 4.3820\n",
      "Epoch [10/25], Step [109/196], Loss: 4.3268\n",
      "Epoch [10/25], Step [114/196], Loss: 4.3315\n",
      "Epoch [10/25], Step [119/196], Loss: 4.3121\n",
      "Epoch [10/25], Step [124/196], Loss: 4.3336\n",
      "Epoch [10/25], Step [129/196], Loss: 4.3674\n",
      "Epoch [10/25], Step [134/196], Loss: 4.3407\n",
      "Epoch [10/25], Step [139/196], Loss: 4.3697\n",
      "Epoch [10/25], Step [144/196], Loss: 4.3966\n",
      "Epoch [10/25], Step [149/196], Loss: 4.3423\n",
      "Epoch [10/25], Step [154/196], Loss: 4.3418\n",
      "Epoch [10/25], Step [159/196], Loss: 4.3674\n",
      "Epoch [10/25], Step [164/196], Loss: 4.4025\n",
      "Epoch [10/25], Step [169/196], Loss: 4.3351\n",
      "Epoch [10/25], Step [174/196], Loss: 4.3640\n",
      "Epoch [10/25], Step [179/196], Loss: 4.3197\n",
      "Epoch [10/25], Step [184/196], Loss: 4.3844\n",
      "Epoch [10/25], Step [189/196], Loss: 4.3482\n",
      "Epoch [10/25], Step [194/196], Loss: 4.4141\n",
      ">> Epoch 10\n",
      "generated text:\n",
      "this movie is so bad . the best of all time the year of all time . the movie has a lot of the plot . i can 't believe that it would have been better . it is so awful i have seen\n",
      "\n",
      "None\n",
      "Epoch [11/25], Step [4/196], Loss: 4.1093\n",
      "Epoch [11/25], Step [9/196], Loss: 4.0989\n",
      "Epoch [11/25], Step [14/196], Loss: 4.1930\n",
      "Epoch [11/25], Step [19/196], Loss: 4.2119\n",
      "Epoch [11/25], Step [24/196], Loss: 4.2271\n",
      "Epoch [11/25], Step [29/196], Loss: 4.1440\n",
      "Epoch [11/25], Step [34/196], Loss: 4.2509\n",
      "Epoch [11/25], Step [39/196], Loss: 4.1828\n",
      "Epoch [11/25], Step [44/196], Loss: 4.1709\n",
      "Epoch [11/25], Step [49/196], Loss: 4.2399\n",
      "Epoch [11/25], Step [54/196], Loss: 4.2263\n",
      "Epoch [11/25], Step [59/196], Loss: 4.2408\n",
      "Epoch [11/25], Step [64/196], Loss: 4.2319\n",
      "Epoch [11/25], Step [69/196], Loss: 4.2447\n",
      "Epoch [11/25], Step [74/196], Loss: 4.2218\n",
      "Epoch [11/25], Step [79/196], Loss: 4.2654\n",
      "Epoch [11/25], Step [84/196], Loss: 4.2709\n",
      "Epoch [11/25], Step [89/196], Loss: 4.2298\n",
      "Epoch [11/25], Step [94/196], Loss: 4.2247\n",
      "Epoch [11/25], Step [99/196], Loss: 4.2687\n",
      "Epoch [11/25], Step [104/196], Loss: 4.2695\n",
      "Epoch [11/25], Step [109/196], Loss: 4.3227\n",
      "Epoch [11/25], Step [114/196], Loss: 4.2673\n",
      "Epoch [11/25], Step [119/196], Loss: 4.2135\n",
      "Epoch [11/25], Step [124/196], Loss: 4.2525\n",
      "Epoch [11/25], Step [129/196], Loss: 4.3071\n",
      "Epoch [11/25], Step [134/196], Loss: 4.2446\n",
      "Epoch [11/25], Step [139/196], Loss: 4.3040\n",
      "Epoch [11/25], Step [144/196], Loss: 4.3204\n",
      "Epoch [11/25], Step [149/196], Loss: 4.2203\n",
      "Epoch [11/25], Step [154/196], Loss: 4.2889\n",
      "Epoch [11/25], Step [159/196], Loss: 4.2669\n",
      "Epoch [11/25], Step [164/196], Loss: 4.2481\n",
      "Epoch [11/25], Step [169/196], Loss: 4.3097\n",
      "Epoch [11/25], Step [174/196], Loss: 4.2518\n",
      "Epoch [11/25], Step [179/196], Loss: 4.3361\n",
      "Epoch [11/25], Step [184/196], Loss: 4.3270\n",
      "Epoch [11/25], Step [189/196], Loss: 4.2940\n",
      "Epoch [11/25], Step [194/196], Loss: 4.3974\n",
      ">> Epoch 11\n",
      "generated text:\n",
      "this movie is not really bad as it is good ! the actors in the movie and this movie is good , but the plot is bad ! the movie is a complete waste of time . it 's too . this movie is\n",
      "\n",
      "None\n",
      "Epoch [12/25], Step [4/196], Loss: 4.1611\n",
      "Epoch [12/25], Step [9/196], Loss: 4.1163\n",
      "Epoch [12/25], Step [14/196], Loss: 4.0789\n",
      "Epoch [12/25], Step [19/196], Loss: 4.1334\n",
      "Epoch [12/25], Step [24/196], Loss: 4.1625\n",
      "Epoch [12/25], Step [29/196], Loss: 4.1214\n",
      "Epoch [12/25], Step [34/196], Loss: 4.1464\n",
      "Epoch [12/25], Step [39/196], Loss: 4.0916\n",
      "Epoch [12/25], Step [44/196], Loss: 4.1279\n",
      "Epoch [12/25], Step [49/196], Loss: 4.1519\n",
      "Epoch [12/25], Step [54/196], Loss: 4.1461\n",
      "Epoch [12/25], Step [59/196], Loss: 4.1561\n",
      "Epoch [12/25], Step [64/196], Loss: 4.1540\n",
      "Epoch [12/25], Step [69/196], Loss: 4.1594\n",
      "Epoch [12/25], Step [74/196], Loss: 4.1332\n",
      "Epoch [12/25], Step [79/196], Loss: 4.1756\n",
      "Epoch [12/25], Step [84/196], Loss: 4.1842\n",
      "Epoch [12/25], Step [89/196], Loss: 4.1765\n",
      "Epoch [12/25], Step [94/196], Loss: 4.1973\n",
      "Epoch [12/25], Step [99/196], Loss: 4.1950\n",
      "Epoch [12/25], Step [104/196], Loss: 4.2096\n",
      "Epoch [12/25], Step [109/196], Loss: 4.2228\n",
      "Epoch [12/25], Step [114/196], Loss: 4.2535\n",
      "Epoch [12/25], Step [119/196], Loss: 4.2159\n",
      "Epoch [12/25], Step [124/196], Loss: 4.2186\n",
      "Epoch [12/25], Step [129/196], Loss: 4.1702\n",
      "Epoch [12/25], Step [134/196], Loss: 4.2703\n",
      "Epoch [12/25], Step [139/196], Loss: 4.2156\n",
      "Epoch [12/25], Step [144/196], Loss: 4.2234\n",
      "Epoch [12/25], Step [149/196], Loss: 4.2351\n",
      "Epoch [12/25], Step [154/196], Loss: 4.2341\n",
      "Epoch [12/25], Step [159/196], Loss: 4.1780\n",
      "Epoch [12/25], Step [164/196], Loss: 4.1954\n",
      "Epoch [12/25], Step [169/196], Loss: 4.1991\n",
      "Epoch [12/25], Step [174/196], Loss: 4.2598\n",
      "Epoch [12/25], Step [179/196], Loss: 4.2385\n",
      "Epoch [12/25], Step [184/196], Loss: 4.2123\n",
      "Epoch [12/25], Step [189/196], Loss: 4.2501\n",
      "Epoch [12/25], Step [194/196], Loss: 4.3044\n",
      ">> Epoch 12\n",
      "generated text:\n",
      "this movie is a must -see , because i love it ! ! ! ! ! ! ! ! you can 't help me ! the only movie the actors were that funny in this case ! ! ! ! ! ! !i can\n",
      "\n",
      "None\n",
      "Epoch [13/25], Step [4/196], Loss: 4.0876\n",
      "Epoch [13/25], Step [9/196], Loss: 4.0219\n",
      "Epoch [13/25], Step [14/196], Loss: 4.0577\n",
      "Epoch [13/25], Step [19/196], Loss: 4.0608\n",
      "Epoch [13/25], Step [24/196], Loss: 4.0338\n",
      "Epoch [13/25], Step [29/196], Loss: 4.0819\n",
      "Epoch [13/25], Step [34/196], Loss: 4.0403\n",
      "Epoch [13/25], Step [39/196], Loss: 4.1025\n",
      "Epoch [13/25], Step [44/196], Loss: 4.0420\n",
      "Epoch [13/25], Step [49/196], Loss: 4.1069\n",
      "Epoch [13/25], Step [54/196], Loss: 4.0767\n",
      "Epoch [13/25], Step [59/196], Loss: 4.1392\n",
      "Epoch [13/25], Step [64/196], Loss: 4.1192\n",
      "Epoch [13/25], Step [69/196], Loss: 4.0920\n",
      "Epoch [13/25], Step [74/196], Loss: 4.0708\n",
      "Epoch [13/25], Step [79/196], Loss: 4.1228\n",
      "Epoch [13/25], Step [84/196], Loss: 4.0995\n",
      "Epoch [13/25], Step [89/196], Loss: 4.1772\n",
      "Epoch [13/25], Step [94/196], Loss: 4.1278\n",
      "Epoch [13/25], Step [99/196], Loss: 4.1481\n",
      "Epoch [13/25], Step [104/196], Loss: 4.1416\n",
      "Epoch [13/25], Step [109/196], Loss: 4.1725\n",
      "Epoch [13/25], Step [114/196], Loss: 4.1692\n",
      "Epoch [13/25], Step [119/196], Loss: 4.1658\n",
      "Epoch [13/25], Step [124/196], Loss: 4.1963\n",
      "Epoch [13/25], Step [129/196], Loss: 4.2023\n",
      "Epoch [13/25], Step [134/196], Loss: 4.2118\n",
      "Epoch [13/25], Step [139/196], Loss: 4.1851\n",
      "Epoch [13/25], Step [144/196], Loss: 4.1996\n",
      "Epoch [13/25], Step [149/196], Loss: 4.2395\n",
      "Epoch [13/25], Step [154/196], Loss: 4.1433\n",
      "Epoch [13/25], Step [159/196], Loss: 4.2038\n",
      "Epoch [13/25], Step [164/196], Loss: 4.2229\n",
      "Epoch [13/25], Step [169/196], Loss: 4.2538\n",
      "Epoch [13/25], Step [174/196], Loss: 4.1979\n",
      "Epoch [13/25], Step [179/196], Loss: 4.1776\n",
      "Epoch [13/25], Step [184/196], Loss: 4.1849\n",
      "Epoch [13/25], Step [189/196], Loss: 4.1854\n",
      "Epoch [13/25], Step [194/196], Loss: 4.1828\n",
      ">> Epoch 13\n",
      "generated text:\n",
      "this movie is one of those rare movies you see , if it 's good . it 's very good for the movie , but it 's not funny . . . . . . . . . . . . . . but\n",
      "\n",
      "None\n",
      "Epoch [14/25], Step [4/196], Loss: 3.9776\n",
      "Epoch [14/25], Step [9/196], Loss: 3.9886\n",
      "Epoch [14/25], Step [14/196], Loss: 4.0088\n",
      "Epoch [14/25], Step [19/196], Loss: 3.9940\n",
      "Epoch [14/25], Step [24/196], Loss: 4.0232\n",
      "Epoch [14/25], Step [29/196], Loss: 4.0208\n",
      "Epoch [14/25], Step [34/196], Loss: 4.0656\n",
      "Epoch [14/25], Step [39/196], Loss: 4.0941\n",
      "Epoch [14/25], Step [44/196], Loss: 4.0910\n",
      "Epoch [14/25], Step [49/196], Loss: 4.0484\n",
      "Epoch [14/25], Step [54/196], Loss: 4.0073\n",
      "Epoch [14/25], Step [59/196], Loss: 4.1084\n",
      "Epoch [14/25], Step [64/196], Loss: 4.1149\n",
      "Epoch [14/25], Step [69/196], Loss: 4.0908\n",
      "Epoch [14/25], Step [74/196], Loss: 4.0627\n",
      "Epoch [14/25], Step [79/196], Loss: 4.0742\n",
      "Epoch [14/25], Step [84/196], Loss: 4.0623\n",
      "Epoch [14/25], Step [89/196], Loss: 4.0655\n",
      "Epoch [14/25], Step [94/196], Loss: 4.1039\n",
      "Epoch [14/25], Step [99/196], Loss: 4.0637\n",
      "Epoch [14/25], Step [104/196], Loss: 4.0821\n",
      "Epoch [14/25], Step [109/196], Loss: 4.0961\n",
      "Epoch [14/25], Step [114/196], Loss: 4.1842\n",
      "Epoch [14/25], Step [119/196], Loss: 4.1354\n",
      "Epoch [14/25], Step [124/196], Loss: 4.1128\n",
      "Epoch [14/25], Step [129/196], Loss: 4.0922\n",
      "Epoch [14/25], Step [134/196], Loss: 4.1111\n",
      "Epoch [14/25], Step [139/196], Loss: 4.0896\n",
      "Epoch [14/25], Step [144/196], Loss: 4.1836\n",
      "Epoch [14/25], Step [149/196], Loss: 4.1840\n",
      "Epoch [14/25], Step [154/196], Loss: 4.1397\n",
      "Epoch [14/25], Step [159/196], Loss: 4.1142\n",
      "Epoch [14/25], Step [164/196], Loss: 4.0834\n",
      "Epoch [14/25], Step [169/196], Loss: 4.2150\n",
      "Epoch [14/25], Step [174/196], Loss: 4.1617\n",
      "Epoch [14/25], Step [179/196], Loss: 4.1687\n",
      "Epoch [14/25], Step [184/196], Loss: 4.1618\n",
      "Epoch [14/25], Step [189/196], Loss: 4.2816\n",
      "Epoch [14/25], Step [194/196], Loss: 4.2225\n",
      ">> Epoch 14\n",
      "generated text:\n",
      "this movie is a very strange movie about a young who was cured . i was not sure my family values and we went to a group of people who are not very bad enough of everyman . the plot is so awful ,\n",
      "\n",
      "None\n",
      "Epoch [15/25], Step [4/196], Loss: 3.9023\n",
      "Epoch [15/25], Step [9/196], Loss: 3.9454\n",
      "Epoch [15/25], Step [14/196], Loss: 3.9526\n",
      "Epoch [15/25], Step [19/196], Loss: 3.9149\n",
      "Epoch [15/25], Step [24/196], Loss: 3.9648\n",
      "Epoch [15/25], Step [29/196], Loss: 4.0658\n",
      "Epoch [15/25], Step [34/196], Loss: 3.9606\n",
      "Epoch [15/25], Step [39/196], Loss: 3.9965\n",
      "Epoch [15/25], Step [44/196], Loss: 3.9938\n",
      "Epoch [15/25], Step [49/196], Loss: 3.9746\n",
      "Epoch [15/25], Step [54/196], Loss: 4.0559\n",
      "Epoch [15/25], Step [59/196], Loss: 4.0395\n",
      "Epoch [15/25], Step [64/196], Loss: 4.0252\n",
      "Epoch [15/25], Step [69/196], Loss: 4.0116\n",
      "Epoch [15/25], Step [74/196], Loss: 4.0581\n",
      "Epoch [15/25], Step [79/196], Loss: 4.0630\n",
      "Epoch [15/25], Step [84/196], Loss: 4.0022\n",
      "Epoch [15/25], Step [89/196], Loss: 4.0963\n",
      "Epoch [15/25], Step [94/196], Loss: 4.0374\n",
      "Epoch [15/25], Step [99/196], Loss: 4.0754\n",
      "Epoch [15/25], Step [104/196], Loss: 4.0512\n",
      "Epoch [15/25], Step [109/196], Loss: 4.0359\n",
      "Epoch [15/25], Step [114/196], Loss: 4.0183\n",
      "Epoch [15/25], Step [119/196], Loss: 4.0518\n",
      "Epoch [15/25], Step [124/196], Loss: 4.1031\n",
      "Epoch [15/25], Step [129/196], Loss: 4.1173\n",
      "Epoch [15/25], Step [134/196], Loss: 4.0721\n",
      "Epoch [15/25], Step [139/196], Loss: 4.0583\n",
      "Epoch [15/25], Step [144/196], Loss: 4.1030\n",
      "Epoch [15/25], Step [149/196], Loss: 4.0697\n",
      "Epoch [15/25], Step [154/196], Loss: 4.1461\n",
      "Epoch [15/25], Step [159/196], Loss: 4.0747\n",
      "Epoch [15/25], Step [164/196], Loss: 4.1351\n",
      "Epoch [15/25], Step [169/196], Loss: 4.1131\n",
      "Epoch [15/25], Step [174/196], Loss: 4.1671\n",
      "Epoch [15/25], Step [179/196], Loss: 4.0668\n",
      "Epoch [15/25], Step [184/196], Loss: 4.0875\n",
      "Epoch [15/25], Step [189/196], Loss: 4.1382\n",
      "Epoch [15/25], Step [194/196], Loss: 4.1671\n",
      ">> Epoch 15\n",
      "generated text:\n",
      "this movie is the first half of the movie . it doesn 't take the story and it was just that in which the film is about it . this is a comedy . the film is the effectiveness of this film is a\n",
      "\n",
      "None\n",
      "Epoch [16/25], Step [4/196], Loss: 3.9142\n",
      "Epoch [16/25], Step [9/196], Loss: 3.9177\n",
      "Epoch [16/25], Step [14/196], Loss: 4.0030\n",
      "Epoch [16/25], Step [19/196], Loss: 3.9941\n",
      "Epoch [16/25], Step [24/196], Loss: 3.9526\n",
      "Epoch [16/25], Step [29/196], Loss: 3.9580\n",
      "Epoch [16/25], Step [34/196], Loss: 3.9314\n",
      "Epoch [16/25], Step [39/196], Loss: 3.9563\n",
      "Epoch [16/25], Step [44/196], Loss: 3.9250\n",
      "Epoch [16/25], Step [49/196], Loss: 3.9031\n",
      "Epoch [16/25], Step [54/196], Loss: 4.0164\n",
      "Epoch [16/25], Step [59/196], Loss: 4.0468\n",
      "Epoch [16/25], Step [64/196], Loss: 3.9783\n",
      "Epoch [16/25], Step [69/196], Loss: 3.9970\n",
      "Epoch [16/25], Step [74/196], Loss: 3.9855\n",
      "Epoch [16/25], Step [79/196], Loss: 3.9522\n",
      "Epoch [16/25], Step [84/196], Loss: 4.0362\n",
      "Epoch [16/25], Step [89/196], Loss: 3.9917\n",
      "Epoch [16/25], Step [94/196], Loss: 3.9856\n",
      "Epoch [16/25], Step [99/196], Loss: 3.9547\n",
      "Epoch [16/25], Step [104/196], Loss: 4.0399\n",
      "Epoch [16/25], Step [109/196], Loss: 3.9925\n",
      "Epoch [16/25], Step [114/196], Loss: 4.0604\n",
      "Epoch [16/25], Step [119/196], Loss: 4.0924\n",
      "Epoch [16/25], Step [124/196], Loss: 4.0541\n",
      "Epoch [16/25], Step [129/196], Loss: 4.0389\n",
      "Epoch [16/25], Step [134/196], Loss: 4.0449\n",
      "Epoch [16/25], Step [139/196], Loss: 4.0107\n",
      "Epoch [16/25], Step [144/196], Loss: 4.0285\n",
      "Epoch [16/25], Step [149/196], Loss: 4.0759\n",
      "Epoch [16/25], Step [154/196], Loss: 4.1106\n",
      "Epoch [16/25], Step [159/196], Loss: 4.1114\n",
      "Epoch [16/25], Step [164/196], Loss: 4.0703\n",
      "Epoch [16/25], Step [169/196], Loss: 4.0629\n",
      "Epoch [16/25], Step [174/196], Loss: 4.0904\n",
      "Epoch [16/25], Step [179/196], Loss: 4.1149\n",
      "Epoch [16/25], Step [184/196], Loss: 4.0613\n",
      "Epoch [16/25], Step [189/196], Loss: 4.0541\n",
      "Epoch [16/25], Step [194/196], Loss: 4.0562\n",
      ">> Epoch 16\n",
      "generated text:\n",
      "this movie is one that you 'll have to see this film in the simplest terms of a film . you can think of gardner 's work and the film is not a film , but not the best of the most underrated films\n",
      "\n",
      "None\n",
      "Epoch [17/25], Step [4/196], Loss: 3.8954\n",
      "Epoch [17/25], Step [9/196], Loss: 3.8400\n",
      "Epoch [17/25], Step [14/196], Loss: 3.9008\n",
      "Epoch [17/25], Step [19/196], Loss: 3.8962\n",
      "Epoch [17/25], Step [24/196], Loss: 3.8266\n",
      "Epoch [17/25], Step [29/196], Loss: 3.9256\n",
      "Epoch [17/25], Step [34/196], Loss: 3.8580\n",
      "Epoch [17/25], Step [39/196], Loss: 3.9446\n",
      "Epoch [17/25], Step [44/196], Loss: 3.9800\n",
      "Epoch [17/25], Step [49/196], Loss: 3.9071\n",
      "Epoch [17/25], Step [54/196], Loss: 3.9134\n",
      "Epoch [17/25], Step [59/196], Loss: 3.9397\n",
      "Epoch [17/25], Step [64/196], Loss: 3.9358\n",
      "Epoch [17/25], Step [69/196], Loss: 3.9728\n",
      "Epoch [17/25], Step [74/196], Loss: 3.9051\n",
      "Epoch [17/25], Step [79/196], Loss: 3.9385\n",
      "Epoch [17/25], Step [84/196], Loss: 3.9627\n",
      "Epoch [17/25], Step [89/196], Loss: 3.9832\n",
      "Epoch [17/25], Step [94/196], Loss: 3.9811\n",
      "Epoch [17/25], Step [99/196], Loss: 3.9701\n",
      "Epoch [17/25], Step [104/196], Loss: 4.0130\n",
      "Epoch [17/25], Step [109/196], Loss: 3.9851\n",
      "Epoch [17/25], Step [114/196], Loss: 4.0055\n",
      "Epoch [17/25], Step [119/196], Loss: 4.0950\n",
      "Epoch [17/25], Step [124/196], Loss: 4.0796\n",
      "Epoch [17/25], Step [129/196], Loss: 3.9549\n",
      "Epoch [17/25], Step [134/196], Loss: 3.9979\n",
      "Epoch [17/25], Step [139/196], Loss: 4.0693\n",
      "Epoch [17/25], Step [144/196], Loss: 4.0088\n",
      "Epoch [17/25], Step [149/196], Loss: 4.0046\n",
      "Epoch [17/25], Step [154/196], Loss: 4.0128\n",
      "Epoch [17/25], Step [159/196], Loss: 4.0151\n",
      "Epoch [17/25], Step [164/196], Loss: 4.0525\n",
      "Epoch [17/25], Step [169/196], Loss: 3.9792\n",
      "Epoch [17/25], Step [174/196], Loss: 4.0736\n",
      "Epoch [17/25], Step [179/196], Loss: 4.0752\n",
      "Epoch [17/25], Step [184/196], Loss: 4.0759\n",
      "Epoch [17/25], Step [189/196], Loss: 4.0223\n",
      "Epoch [17/25], Step [194/196], Loss: 4.0148\n",
      ">> Epoch 17\n",
      "generated text:\n",
      "this movie is about a young man living in a new york . it celebrates to it . i 'm not sure if it 's just plain awful and i did enjoy it . it 's a very funny -looking twisted and you will\n",
      "\n",
      "None\n",
      "Epoch [18/25], Step [4/196], Loss: 3.8100\n",
      "Epoch [18/25], Step [9/196], Loss: 3.8937\n",
      "Epoch [18/25], Step [14/196], Loss: 3.7741\n",
      "Epoch [18/25], Step [19/196], Loss: 3.8426\n",
      "Epoch [18/25], Step [24/196], Loss: 3.8503\n",
      "Epoch [18/25], Step [29/196], Loss: 3.8591\n",
      "Epoch [18/25], Step [34/196], Loss: 3.8494\n",
      "Epoch [18/25], Step [39/196], Loss: 3.8938\n",
      "Epoch [18/25], Step [44/196], Loss: 3.9117\n",
      "Epoch [18/25], Step [49/196], Loss: 3.9383\n",
      "Epoch [18/25], Step [54/196], Loss: 3.9192\n",
      "Epoch [18/25], Step [59/196], Loss: 4.0060\n",
      "Epoch [18/25], Step [64/196], Loss: 3.8650\n",
      "Epoch [18/25], Step [69/196], Loss: 3.8620\n",
      "Epoch [18/25], Step [74/196], Loss: 3.9809\n",
      "Epoch [18/25], Step [79/196], Loss: 3.9598\n",
      "Epoch [18/25], Step [84/196], Loss: 3.8848\n",
      "Epoch [18/25], Step [89/196], Loss: 3.9670\n",
      "Epoch [18/25], Step [94/196], Loss: 3.9552\n",
      "Epoch [18/25], Step [99/196], Loss: 3.9462\n",
      "Epoch [18/25], Step [104/196], Loss: 3.9734\n",
      "Epoch [18/25], Step [109/196], Loss: 3.9489\n",
      "Epoch [18/25], Step [114/196], Loss: 3.9742\n",
      "Epoch [18/25], Step [119/196], Loss: 3.9522\n",
      "Epoch [18/25], Step [124/196], Loss: 4.0041\n",
      "Epoch [18/25], Step [129/196], Loss: 3.9627\n",
      "Epoch [18/25], Step [134/196], Loss: 3.9809\n",
      "Epoch [18/25], Step [139/196], Loss: 4.0481\n",
      "Epoch [18/25], Step [144/196], Loss: 3.9900\n",
      "Epoch [18/25], Step [149/196], Loss: 3.9460\n",
      "Epoch [18/25], Step [154/196], Loss: 3.9556\n",
      "Epoch [18/25], Step [159/196], Loss: 4.0518\n",
      "Epoch [18/25], Step [164/196], Loss: 4.0529\n",
      "Epoch [18/25], Step [169/196], Loss: 4.0858\n",
      "Epoch [18/25], Step [174/196], Loss: 4.0192\n",
      "Epoch [18/25], Step [179/196], Loss: 4.0394\n",
      "Epoch [18/25], Step [184/196], Loss: 3.9824\n",
      "Epoch [18/25], Step [189/196], Loss: 3.9836\n",
      "Epoch [18/25], Step [194/196], Loss: 4.0368\n",
      ">> Epoch 18\n",
      "generated text:\n",
      "this movie is one of the first movies that i have ever heard . but it 's so bad it really . the movie starts out , and there was a bit of a story . it was bad , i was very surprised\n",
      "\n",
      "None\n",
      "Epoch [19/25], Step [4/196], Loss: 3.7758\n",
      "Epoch [19/25], Step [9/196], Loss: 3.7886\n",
      "Epoch [19/25], Step [14/196], Loss: 3.8715\n",
      "Epoch [19/25], Step [19/196], Loss: 3.8304\n",
      "Epoch [19/25], Step [24/196], Loss: 3.8358\n",
      "Epoch [19/25], Step [29/196], Loss: 3.8243\n",
      "Epoch [19/25], Step [34/196], Loss: 3.8367\n",
      "Epoch [19/25], Step [39/196], Loss: 3.8674\n",
      "Epoch [19/25], Step [44/196], Loss: 3.8524\n",
      "Epoch [19/25], Step [49/196], Loss: 3.8469\n",
      "Epoch [19/25], Step [54/196], Loss: 3.8815\n",
      "Epoch [19/25], Step [59/196], Loss: 3.8723\n",
      "Epoch [19/25], Step [64/196], Loss: 3.8327\n",
      "Epoch [19/25], Step [69/196], Loss: 3.8301\n",
      "Epoch [19/25], Step [74/196], Loss: 3.8675\n",
      "Epoch [19/25], Step [79/196], Loss: 3.8820\n",
      "Epoch [19/25], Step [84/196], Loss: 3.8645\n",
      "Epoch [19/25], Step [89/196], Loss: 3.9302\n",
      "Epoch [19/25], Step [94/196], Loss: 3.9732\n",
      "Epoch [19/25], Step [99/196], Loss: 3.9543\n",
      "Epoch [19/25], Step [104/196], Loss: 3.9530\n",
      "Epoch [19/25], Step [109/196], Loss: 3.8996\n",
      "Epoch [19/25], Step [114/196], Loss: 3.9032\n",
      "Epoch [19/25], Step [119/196], Loss: 3.9583\n",
      "Epoch [19/25], Step [124/196], Loss: 3.9437\n",
      "Epoch [19/25], Step [129/196], Loss: 4.0371\n",
      "Epoch [19/25], Step [134/196], Loss: 3.9047\n",
      "Epoch [19/25], Step [139/196], Loss: 3.9097\n",
      "Epoch [19/25], Step [144/196], Loss: 3.9456\n",
      "Epoch [19/25], Step [149/196], Loss: 3.9250\n",
      "Epoch [19/25], Step [154/196], Loss: 3.9210\n",
      "Epoch [19/25], Step [159/196], Loss: 3.9430\n",
      "Epoch [19/25], Step [164/196], Loss: 3.9842\n",
      "Epoch [19/25], Step [169/196], Loss: 3.9759\n",
      "Epoch [19/25], Step [174/196], Loss: 3.9673\n",
      "Epoch [19/25], Step [179/196], Loss: 4.0390\n",
      "Epoch [19/25], Step [184/196], Loss: 3.9586\n",
      "Epoch [19/25], Step [189/196], Loss: 3.9794\n",
      "Epoch [19/25], Step [194/196], Loss: 4.0008\n",
      ">> Epoch 19\n",
      "generated text:\n",
      "this movie is not even funny , and there are some funny situations . i don 't watch this film , and there is not one shred . it 's not a good way it is so well , but you can 't understand\n",
      "\n",
      "None\n",
      "Epoch [20/25], Step [4/196], Loss: 3.7585\n",
      "Epoch [20/25], Step [9/196], Loss: 3.7180\n",
      "Epoch [20/25], Step [14/196], Loss: 3.8104\n",
      "Epoch [20/25], Step [19/196], Loss: 3.8106\n",
      "Epoch [20/25], Step [24/196], Loss: 3.8015\n",
      "Epoch [20/25], Step [29/196], Loss: 3.8007\n",
      "Epoch [20/25], Step [34/196], Loss: 3.7628\n",
      "Epoch [20/25], Step [39/196], Loss: 3.8267\n",
      "Epoch [20/25], Step [44/196], Loss: 3.8492\n",
      "Epoch [20/25], Step [49/196], Loss: 3.8516\n",
      "Epoch [20/25], Step [54/196], Loss: 3.7894\n",
      "Epoch [20/25], Step [59/196], Loss: 3.9088\n",
      "Epoch [20/25], Step [64/196], Loss: 3.8488\n",
      "Epoch [20/25], Step [69/196], Loss: 3.8336\n",
      "Epoch [20/25], Step [74/196], Loss: 3.8706\n",
      "Epoch [20/25], Step [79/196], Loss: 3.9159\n",
      "Epoch [20/25], Step [84/196], Loss: 3.8824\n",
      "Epoch [20/25], Step [89/196], Loss: 3.9282\n",
      "Epoch [20/25], Step [94/196], Loss: 3.8710\n",
      "Epoch [20/25], Step [99/196], Loss: 3.8879\n",
      "Epoch [20/25], Step [104/196], Loss: 3.8800\n",
      "Epoch [20/25], Step [109/196], Loss: 3.8641\n",
      "Epoch [20/25], Step [114/196], Loss: 3.9232\n",
      "Epoch [20/25], Step [119/196], Loss: 3.8870\n",
      "Epoch [20/25], Step [124/196], Loss: 3.8803\n",
      "Epoch [20/25], Step [129/196], Loss: 3.8947\n",
      "Epoch [20/25], Step [134/196], Loss: 3.9410\n",
      "Epoch [20/25], Step [139/196], Loss: 3.9621\n",
      "Epoch [20/25], Step [144/196], Loss: 3.9489\n",
      "Epoch [20/25], Step [149/196], Loss: 3.8604\n",
      "Epoch [20/25], Step [154/196], Loss: 3.9157\n",
      "Epoch [20/25], Step [159/196], Loss: 3.9619\n",
      "Epoch [20/25], Step [164/196], Loss: 3.8910\n",
      "Epoch [20/25], Step [169/196], Loss: 3.9590\n",
      "Epoch [20/25], Step [174/196], Loss: 3.9313\n",
      "Epoch [20/25], Step [179/196], Loss: 3.9488\n",
      "Epoch [20/25], Step [184/196], Loss: 3.9482\n",
      "Epoch [20/25], Step [189/196], Loss: 3.9563\n",
      "Epoch [20/25], Step [194/196], Loss: 4.0437\n",
      ">> Epoch 20\n",
      "generated text:\n",
      "this movie is the kind of a great movie . it is not funny to watch . it is a great story of a bunch of aimless ideals on his way to waste the movie . a movie about 5 stars on a sofa\n",
      "\n",
      "None\n",
      "Epoch [21/25], Step [4/196], Loss: 3.7114\n",
      "Epoch [21/25], Step [9/196], Loss: 3.7677\n",
      "Epoch [21/25], Step [14/196], Loss: 3.7810\n",
      "Epoch [21/25], Step [19/196], Loss: 3.7259\n",
      "Epoch [21/25], Step [24/196], Loss: 3.7612\n",
      "Epoch [21/25], Step [29/196], Loss: 3.7626\n",
      "Epoch [21/25], Step [34/196], Loss: 3.7579\n",
      "Epoch [21/25], Step [39/196], Loss: 3.7561\n",
      "Epoch [21/25], Step [44/196], Loss: 3.8119\n",
      "Epoch [21/25], Step [49/196], Loss: 3.7562\n",
      "Epoch [21/25], Step [54/196], Loss: 3.8252\n",
      "Epoch [21/25], Step [59/196], Loss: 3.8231\n",
      "Epoch [21/25], Step [64/196], Loss: 3.8292\n",
      "Epoch [21/25], Step [69/196], Loss: 3.8523\n",
      "Epoch [21/25], Step [74/196], Loss: 3.7750\n",
      "Epoch [21/25], Step [79/196], Loss: 3.8303\n",
      "Epoch [21/25], Step [84/196], Loss: 3.8538\n",
      "Epoch [21/25], Step [89/196], Loss: 3.8861\n",
      "Epoch [21/25], Step [94/196], Loss: 3.8149\n",
      "Epoch [21/25], Step [99/196], Loss: 3.8328\n",
      "Epoch [21/25], Step [104/196], Loss: 3.8400\n"
     ]
    }
   ],
   "source": [
    "num_epochs=25\n",
    "total_step = len(train_loader)\n",
    "\n",
    "model = model.to(device)\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [token_to_id.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "\n",
    "for epoch in tqdm(range(0, num_epochs)):    \n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "\n",
    "        batch_inputs = sample_batched['inputs'].to(device) # torch.Size([128, 80])\n",
    "        batch_targets = sample_batched['targets'].to(device)\n",
    "        \n",
    "        batch_size = batch_targets.size(0)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(batch_size, batch_inputs) # torch.Size([128, 80, 20000])\n",
    "        \n",
    "        # Compute loss\n",
    "        batch_predicts = rearrange(outputs, 'b c l -> b l c') # torch.Size([128, 20000, 80])\n",
    "        loss = criterion(batch_predicts, batch_targets) # torch.Size([128, 80])\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i_batch+1) % 5 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i_batch, total_step, loss.item())) \n",
    "    \n",
    "    # Save the model checkpoints\n",
    "    torch.save(model.state_dict(), './models/text_gen_gpt-{}.ckpt'.format(epoch+1))\n",
    "    text_gen_callback = TextGenerator(model, device, max_seq_len, num_tokens_generated, start_tokens, id_to_token)\n",
    "    print(f'>> Epoch {epoch + 1}')\n",
    "    print(text_gen_callback.on_epoch_end(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-venue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-section",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-melissa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
