{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPde18ra0fb7"
   },
   "source": [
    "# End-to-end Masked Language Modeling with BERT\n",
    "\n",
    "**Author:** [Ankur Singh](https://twitter.com/ankur310794)<br>\n",
    "**Date created:** 2020/09/18<br>\n",
    "**Last modified:** 2020/09/18<br>\n",
    "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRBcTVyQ0fcA"
   },
   "source": [
    "## Introduction (Edited)\n",
    "\n",
    "Masked Language Modeling is a fill-in-the-blank task,\n",
    "where a model uses the context words surrounding a mask token to try to predict what the\n",
    "masked word should be.\n",
    "\n",
    "For an input that contains one or more mask tokens,\n",
    "the model will generate the most likely substitution for each.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
    "- Output: \"I have watched this movie and it was awesome.\"\n",
    "\n",
    "Masked language modeling is a great way to train a language\n",
    "model in a self-supervised setting (without human-annotated labels).\n",
    "Such a model can then be fine-tuned to accomplish various supervised\n",
    "NLP tasks.\n",
    "\n",
    "This example teaches you how to build a BERT model from scratch,\n",
    "train it with the masked language modeling task,\n",
    "and then fine-tune this model on a sentiment classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytpq_WxN0fcB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ziwhjNNr0fcB"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p0gx5BS0fcC"
   },
   "source": [
    "## Set-up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CZU_1apI0fcC"
   },
   "outputs": [],
   "source": [
    "# Python decorator 설명 : https://choice-life.tistory.com/42\n",
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 1e-04\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "    NUM_CLASS = 2\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR58FAct0fcC"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will first download the IMDB data and load into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JakIeO_30fcC",
    "outputId": "8c79c840-f873-4c64-f0d2-227cd8023ae5"
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `<br />  <br>` : html tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28Etor4I2tt_",
    "outputId": "e9b7a30f-3616-45a9-cc9e-e3e94e467991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i, too, loved this series when i was a kid. In 1952 i was 5 and my family always watched this show. My favorite character was the one played by Marion Lorne as a rather stuttering, bumbling and very lovable \"aunt\" type person. i can still recall her \"ubba bubba um um\" type comments as she would try and say something important. And then when she came back and played Aunt Clara in Bewitched it was great casting! <br /><br />It was the first time that i can remember seeing Walter Matthau whose career i followed as a fan for many many years.<br /><br />i have a question if anyone can verify: was the title or end credits music the \"Swedish Rhapsody\" by Hugo Alfven? Every time i hear it played on my classical radio station here in Southern California it brings back memories of the image of Mr. Peepers walking away with his back to the camera. i'm not even certain if this image in my mind's eye is correct."
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/2203_8.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0CT-f6J29ua",
    "outputId": "6dddef61-a23e-402b-efa0-bb8e2b33c071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
      "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "!ls aclImdb/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zc7vntwo0fcC"
   },
   "outputs": [],
   "source": [
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "# label -> pos : 1, neg : 0 \n",
    "def get_data_from_text_files(folder_name):\n",
    "\n",
    "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
    "    pos_texts = get_text_list_from_files(pos_files)\n",
    "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
    "    neg_texts = get_text_list_from_files(neg_files)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"review\": pos_texts + neg_texts,\n",
    "            \"sentiment\": [1] * len(pos_texts) + [0] * len(neg_texts),\n",
    "        }\n",
    "    )\n",
    "    # sampling 후 reset_index : index 초기화 (https://yganalyst.github.io/data_handling/Pd_2/)\n",
    "    # 두 데이터 프레임을 합치면서 index를 0부터 초기화, drop=True : 기존 index를 버림\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = get_data_from_text_files(\"train\")\n",
    "test_df = get_data_from_text_files(\"test\")\n",
    "\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "loALp11HdQKA",
    "outputId": "07531a7e-ae0d-49cd-bd57-9cfa3d1a1c32"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The major fault in this film is that it is imp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This had a good story...it had a nice pace and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don't say I didn't warn you, but your gonna la...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm not sure why this little film has been ban...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Serious HOME ALONE/KARATE KID knock off with e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  The major fault in this film is that it is imp...          0\n",
       "1  This had a good story...it had a nice pace and...          1\n",
       "2  Don't say I didn't warn you, but your gonna la...          1\n",
       "3  I'm not sure why this little film has been ban...          1\n",
       "4  Serious HOME ALONE/KARATE KID knock off with e...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "dDvEv-Ve2jYA",
    "outputId": "d605798d-bc62-4a05-974b-7e0ee2844380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The major fault in this film is that it is impossible to believe any of these people would ever be cast in a professional production of Macbeth. Hearing David Lansbury\\'s soft voice struggling laboriously with the famous \"Tomorrow, Tomorrow, and Tomorrow\" speech made it impossible to believe anyone would ever consider him for the role. I kept believing therefore that he didn\\'t get the part because he was a lousy actor; not because a bigger name was available. Then when we see portions of the play in rehearsal it is difficult to believe the director is not parodying things with a hopelessly miscast, misdirected travesty of actors who are unable to articulate or even understand the verse and directors who see the play through their own screwball interpretations. Sometimes directors are so anxious to have their films done (and writers think they have the ability to direct their own works)that they settle for less. This appears to be such an example.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.iloc[0,]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjU2Ma9nyCt_",
    "outputId": "ac22ff82-524b-4a72-a94f-b80b8f71abf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train :  25000\n",
      "# of test :  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"# of train : \", len(train_df))\n",
    "print(\"# of test : \", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* keras에서는 train, test를 각각 25000개씩 사용했지만, 여기서는 일단 구현 및 디버깅 용이하게 하기 위해 5000개만 사용 (추후 25000개 학습 예정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IYdisUQyfA3c"
   },
   "outputs": [],
   "source": [
    "# train_df_sample = train_df.sample(5000)\n",
    "# test_df_sample = test_df.sample(5000)\n",
    "\n",
    "train_df_sample = train_df\n",
    "test_df_sample = test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr2RHdkG0fcD"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "**1. Keras Implementation**\n",
    "\n",
    "   [Keras `TextVectorization layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization#used-in-the-notebooks_1) : vectorize the text into integer token ids.\n",
    "\n",
    "    It transforms a batch of strings into either\n",
    "    a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
    "    or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
    "\n",
    "        vectorize_layer = TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode=\"int\",    -> more option : \"binary\", \"count\" or \"tf-idf\"\n",
    "            standardize=custom_standardization,\n",
    "            output_sequence_length=max_seq,\n",
    "        )\n",
    "\n",
    "    Below, we define 3 preprocessing functions.\n",
    "\n",
    "    1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
    "    2.  The `encode` function encodes raw text into integer token ids.\n",
    "    3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
    "    It masks 15% of all input tokens in each sequence at random.\n",
    "\n",
    "\n",
    "\n",
    "**2. PyTorch Implementation**\n",
    "  Huggingface 제공 `BertTokenizer`\n",
    " 1) [`BertTokenizer` source code](https://github.com/huggingface/transformers/blob/86d5fb0b360e68de46d40265e7c707fe68c8015b/src/transformers/models/bert/tokenization_bert.py#L117)\n",
    "    \n",
    " 2) [`BertTokenizer` document description](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wFaDnJ5AhfN5"
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hJhAqGgqiK9b"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Pretrained model on English language using a masked language modeling (MLM) objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eic2sGsYimw6"
   },
   "outputs": [],
   "source": [
    "text = \" [CLS] [MASK] [SEP] The capital of France, paris, contains the Eiffel Tower.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWi048Ogst32",
    "outputId": "d62c4f41-c0ea-443c-8d6d-11318960be09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[MASK]',\n",
       " '[SEP]',\n",
       " 'the',\n",
       " 'capital',\n",
       " 'of',\n",
       " 'france',\n",
       " ',',\n",
       " 'paris',\n",
       " ',',\n",
       " 'contains',\n",
       " 'the',\n",
       " 'e',\n",
       " '##iff',\n",
       " '##el',\n",
       " 'tower',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(text) # list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-Enppe8DikBG"
   },
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(text, add_special_tokens = True, max_length=256, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* BERT can only accept/take as input only `fixed length` tokens at a time, we must specify the truncation parameter to True. \n",
    "* The add special tokens parameter is just for BERT to add tokens like the start, end, [SEP], and [CLS] tokens.\n",
    "      - start token :101\n",
    "      - end token : 102\n",
    "      - [CLS]: 101, [SEP] : 102, [MASK] : 103\n",
    "      \n",
    "* Return_tensors = “pt” is just for the tokenizer to return PyTorch tensors. \n",
    "     If you don’t want this to happen(maybe you want it to return a list), then you can remove the parameter and it will return lists.\n",
    "* max_length : default값은 512이나\n",
    "\n",
    "* tokenizer.encode는 단순 encoding 결과 tensor 만 제공\n",
    "* tokenizer.encode_plus는 encoding 결과 tensor, token_type_ids, attention_mask 까지 dictionary로 제공\n",
    "\n",
    "    cf. token_type_ids : https://huggingface.co/transformers/glossary.html#token-type-ids\n",
    "        -> token_type_ids는 next sentence prediction 같은 task에서 문장 구분할때 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09ngHx6Riwz2",
    "outputId": "78ceea65-2e61-451a-b8ab-b64c360ca1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  torch.Size([1, 256])\n",
      "token_type_ids :  torch.Size([1, 256])\n",
      "attention_mask :  torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "{'input_ids': tensor([[  101,   101,   103,   102,  1996,  3007,  1997,  2605,  1010,  3000,\n",
    "          1010,  3397,  1996,  1041, 13355,  2884,  3578,  1012,   102,     0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "                                          ...\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0]]), \n",
    "            'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                                          ...\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]]), \n",
    "         'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                                          ...\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
    "\"\"\"\n",
    "\n",
    "print(\"input_ids : \", encoding['input_ids'].shape)\n",
    "print(\"token_type_ids : \", encoding['token_type_ids'].shape)\n",
    "print(\"attention_mask : \", encoding['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gu699u6tlF0e",
    "outputId": "9122e5fe-245d-4ebc-c34d-c559227a9c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 101\n",
      "[MASK] 103\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.mask_token, tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WjLL0uwbh7xj"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    # tf.strings.regex_replace(\"Text with tags.<br /><b>contains html</b>\", \"<[^>]+>\", \" \").numpy()\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    ).numpy().decode('UTF-8') # Tensor -> bytes(from numpy) -> string\n",
    "\n",
    "# 아래 데이터셋 구축 단계에서 쓰임\n",
    "def encode(text):\n",
    "    # tokenizer.encode 사용하여 Tensor 반환\n",
    "    encoded_text = tokenizer.encode(text, add_special_tokens = True, max_length=256, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n",
    "    encoded_text = tf.reshape(encoded_text, [-1]) # 2D->1D\n",
    "    return encoded_text.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2023, 102]\n",
      "[CLS] this [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode('this'))\n",
    "print(tokenizer.decode(torch.tensor([101,2023,102]))) # list도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 2023,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "class BERTMLMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    torch.utils.data.Dataset은 데이터셋을 나타내는 추상클래스\n",
    "    Custom Dataset은 Dataset에 상속하고 아래와 같이 override.\n",
    "\n",
    "    DataLoader에서,\n",
    "      __len__ 은 데이터셋의 크기를 반환 --> iterable \n",
    "      __getitem__ 은 i번째 샘플을 찾는데 사용\n",
    "      \n",
    "    * encoded_input_texts : encoded texts with random masking.\n",
    "    * encoded_labels : ground truth encoded labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoded_input_texts, encoded_labels):\n",
    "        self.encoded_input_texts = encoded_input_texts\n",
    "        self.encoded_labels = encoded_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        bert_input = self.encoded_input_texts[idx]\n",
    "        bert_label = self.encoded_labels[idx]\n",
    "        \n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label}\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtZNQCO_y70c",
    "outputId": "4563f2aa-c791-4ddf-beaa-b18b6d71398e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n",
      "x_train shape : (25000, 256)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "## Get Training Dataset ##\n",
    "##########################\n",
    "\n",
    "# We have 5000 examples for training\n",
    "\n",
    "# standardization & encoding\n",
    "# x_train -> array (num_samples, dim=256)\n",
    "\n",
    "x_train=[]\n",
    "for i in range(len(train_df_sample.review.values)):\n",
    "    if (i+1) % (len(train_df_sample.review.values)/10) == 0:\n",
    "        print(f'{(i+1)/250} % Done')\n",
    "    train_df_sample.review.values[i] = custom_standardization((train_df_sample.review.values[i]))\n",
    "    x_train.append(encode(train_df_sample.review.values[i]))  # encode reviews with vectorizer\n",
    "x_train = np.array(x_train)\n",
    "print(\"x_train shape :\", x_train.shape)\n",
    "\n",
    "y_train = train_df_sample.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BERTMLMDataset(x_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_input': tensor([[  101, 11519,  7284,  ...,     0,     0,     0],\n",
      "        [  101,  2057,  2113,  ...,     0,     0,     0],\n",
      "        [  101,  2019, 20998,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  8923, 11531,  ...,     0,     0,     0],\n",
      "        [  101,  2625,  2969,  ...,     0,     0,     0],\n",
      "        [  101,  1000,  7367,  ...,     0,     0,     0]]), 'bert_label': tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "print(iter(train_loader).next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3AjplE1y1bL",
    "outputId": "a069ad35-3377-4be8-8dfb-18abdfc5fd68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n",
      "x_test shape : (25000, 256)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "##   Get Test Dataset   ##\n",
    "##########################\n",
    "\n",
    "\n",
    "# We have 5000 examples for testing\n",
    "\n",
    "x_test=[]\n",
    "for i in range(len(test_df_sample.review.values)):\n",
    "    if (i+1) % (len(test_df_sample.review.values)/10) == 0:\n",
    "        print(f'{(i+1)/250} % Done')\n",
    "    train_df_sample.review.values[i] = custom_standardization((test_df_sample.review.values[i]))\n",
    "    x_test.append(encode(test_df_sample.review.values[i]))  # encode reviews with vectorizer\n",
    "x_test = np.array(x_test)\n",
    "print(\"x_test shape :\", x_test.shape)\n",
    "\n",
    "y_test = test_df_sample.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = BERTMLMDataset(x_test, y_test)\n",
    "test_loader = DataLoader(dataset=train_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for Pre-train MLM & End-to-end MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AyRqA-NHYyv_"
   },
   "outputs": [],
   "source": [
    "def get_masked_input_and_labels(encoded_texts, mask_token_id):\n",
    "    \n",
    "    ####################\n",
    "    # 15% BERT masking #\n",
    "    ####################\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    special_tokens = [tokenizer.unk_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id,\n",
    "                      tokenizer.cls_token_id, tokenizer.mask_token_id]\n",
    "    # Get boolean array where original array contains the elements in values list above\n",
    "    masking_condition = np.isin(encoded_texts, special_tokens) \n",
    "    inp_mask[masking_condition] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "\n",
    "    ####################\n",
    "    #   10% Unchanged  #\n",
    "    ####################\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id # mask_token_id : 103\n",
    "\n",
    "    \n",
    "    ####################\n",
    "    #    10% Random    #\n",
    "    ####################\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "##   Get End-tp-ennd Test Dataset   ##\n",
    "######################################\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "# x_test 가 encoding된 벡터가 아니라 string 원문이 들어감\n",
    "\n",
    "test_raw_dataset = BERTMLMDataset(test_df_sample.review.values, y_test)\n",
    "test_raw_loader = DataLoader(dataset=test_raw_dataset, batch_size=config.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxAVZEC-0tsU",
    "outputId": "e860fca0-ff24-42e0-b3f7-be2c58d578fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 % Done\n",
      "20.0 % Done\n",
      "30.0 % Done\n",
      "40.0 % Done\n",
      "50.0 % Done\n",
      "60.0 % Done\n",
      "70.0 % Done\n",
      "80.0 % Done\n",
      "90.0 % Done\n",
      "100.0 % Done\n",
      "x_all_review shape : (50000, 256)\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "##   Get Masked language model Dataset   ##\n",
    "###########################################\n",
    "\n",
    "# Prepare data for masked language model\n",
    "# 기존 Train + Test 데이터 합쳐서 사용 -> 총 10000개\n",
    "all_data_sample = train_df_sample.append(test_df_sample)\n",
    "\n",
    "x_all_review=[]\n",
    "for i in range(len(all_data_sample.review.values)):\n",
    "    if (i+1) % (len(all_data_sample.review.values)/10) == 0:\n",
    "        print(f'{(i+1)/500} % Done')\n",
    "    x_all_review.append(encode(all_data_sample.review.values[i]))\n",
    "x_all_review = np.array(x_all_review)\n",
    "print(\"x_all_review shape :\", x_all_review.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L8B4kVyRTTvC"
   },
   "outputs": [],
   "source": [
    "x_masked_train, y_masked_labels = get_masked_input_and_labels(\n",
    "    x_all_review, tokenizer.mask_token_id\n",
    ")\n",
    "\n",
    "mlm_dataset = BERTMLMDataset(x_masked_train, y_masked_labels)\n",
    "mlm_loader = DataLoader(dataset=mlm_dataset, batch_size=config.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jHJxnatvTl0C",
    "outputId": "46a804b4-fe8e-46ef-90af-837e33c99f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_id :  103\n",
      "\n",
      "\n",
      "x_masked_train\n",
      "[[  101  1045   103 ...   103  2216   102]\n",
      " [  101  5292   103 ...     0     0     0]\n",
      " [  101  3575   103 ...     0     0     0]\n",
      " ...\n",
      " [  101  1045  2941 ... 22889 18163   102]\n",
      " [  101  1045  2293 ...     0     0     0]\n",
      " [  101  2045  1005 ...  2018  1037   102]]\n",
      "\n",
      "y_masked_labels\n",
      "[[  101  1045  2387 ...  2011  2216   102]\n",
      " [  101  5292 27172 ...     0     0     0]\n",
      " [  101  3575  3849 ...     0     0     0]\n",
      " ...\n",
      " [  101  1045  2941 ... 22889 18163   102]\n",
      " [  101  1045  2293 ...     0     0     0]\n",
      " [  101  2045  1005 ...  2018  1037   102]]\n"
     ]
    }
   ],
   "source": [
    "print(\"mask_token_id : \", tokenizer.mask_token_id)\n",
    "print(\"\\n\")\n",
    "print(\"x_masked_train\")\n",
    "print(x_masked_train)\n",
    "print(\"\\ny_masked_labels\")\n",
    "print(y_masked_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save variables using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "import pickle\n",
    "\n",
    "with open('pickle_data/train.pickle', 'wb') as f:\n",
    "    pickle.dump([x_train, y_train], f)\n",
    "\n",
    "with open('pickle_data/test.pickle', 'wb') as f:\n",
    "    pickle.dump([x_test, y_test], f)\n",
    "\n",
    "\n",
    "with open('pickle_data/x_all_review.pickle', 'wb') as f:\n",
    "    pickle.dump(x_all_review, f)\n",
    "\n",
    "with open('pickle_data/masked_train.pickle', 'wb') as f:\n",
    "    pickle.dump([x_masked_train, y_masked_labels], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 참고 : nn.Embedding\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LepG2utE0fcF"
   },
   "source": [
    "## Create BERT model (Pretraining Model) for masked language modeling\n",
    "\n",
    "We will create a BERT-like pretraining model architecture\n",
    "using the `MultiHeadAttention` layer.\n",
    "It will take token ids as inputs (including masked tokens)\n",
    "and it will predict the correct ids for the masked input tokens.\n",
    "\n",
    "PyTorch Implementation Reference Code : https://github.com/codertimo/BERT-pytorch/tree/master/bert_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "5JxgSyn10fcF",
    "outputId": "44bc5595-640b-4cf8-aaa3-3cfc61c37913"
   },
   "outputs": [],
   "source": [
    "# NOT TRAINED\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_len, d_emb):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_emb).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_emb, 2).float() * -(math.log(10000.0) / d_emb)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, max_len, embed_size):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size, padding_idx=0) # tokenizer.pad_token_id = 0\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_emb= embed_size)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence) + self.position(sequence)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-headed Attention & Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Attention\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) # masking value : negative infinity\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "    \n",
    "# Multi-Headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, embed_dim, dropout=0.1): # d_model : embed_dim\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0  # 나머지\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = embed_dim // num_heads  # 몫\n",
    "        self.h = num_heads\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from embed_dim => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # contiguous() 설명 \n",
    "        # contiguous한 텐서는 storage 상에서 점핑없이 순서대로 효율적이게 방문할 수 있기 때문에 메모리 접근 성능을 향상 시킬 수 있음\n",
    "        \n",
    "        # Blog Post : https://subinium.github.io/pytorch-Tensor-Variable/\n",
    "        # Stack Overflow : https://bit.ly/3uNnv2B\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "        \n",
    "        \n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "    \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size -> Keras Example에서는 4배 안 함\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(num_heads=attn_heads, embed_dim=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=0.0) # multi-head self-attention block의 layer normalization\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=0.0) # FFN block의 layer normalization\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT & MLMBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, max_len=256, hidden=128, n_layers=1, attn_heads=8, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.max_len= max_len\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # Keras example used hidden_size=128 for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, max_len=max_len, embed_size=hidden)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden, dropout) for _ in range(n_layers)]) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, seq_len]) -> torch.ByteTensor([batch_size, 1, seq_len, seq_len])\n",
    "        # TODO : attention 계산 시에 padding 부분에 대한 mask 생성\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1) \n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)  # 128 X 30000\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : torch.Size([32, 256, 128])\n",
    "        # return output : # torch.Size([32, 256, 30000])\n",
    "        return self.softmax(self.linear(x)) \n",
    "    \n",
    "    \n",
    "class BERTMLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bert(x)\n",
    "        return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPauXRAz0fcI"
   },
   "source": [
    "## Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "            \n",
    "\n",
    "class BERTMLMTrainer:\n",
    "    \"\"\"\n",
    "    BERTMLMTrainer make the pretrained BERT model with Masked Language Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                 with_cuda: bool = True, log_freq: int = 100):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        # CUDA out of memory.로 False 처리\n",
    "        cuda_condition = False # torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        self.bert = bert\n",
    "        # Initialize the BERT Masked Language Model, with BERT model\n",
    "        self.model = BERTMLM(bert, vocab_size).to(self.device)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.bert.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the masked_lm model\n",
    "            mask_lm_output = self.model.forward(data[\"bert_input\"]) #  torch.Size([32, 256, 30000])\n",
    "        \n",
    "            # NLLLoss of predicting masked token word\n",
    "            loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"bert_label\"]) # torch.Size([32, 30000, 256]), torch.Size([32, 256])\n",
    "\n",
    "            # 2. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "    \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter))\n",
    "\n",
    "\n",
    "    def save(self, epoch, file_path, mlm = True):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :param mlm: If True, save the full MLM BERT model. Otherwise, save only the BERT model.\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        if mlm:\n",
    "            output_path = file_path\n",
    "            # save full `mlm bert`\n",
    "            torch.save(self.model.cpu(), output_path)\n",
    "            self.model.to(self.device)\n",
    "            \n",
    "        else:\n",
    "            output_path = file_path \n",
    "            # save only `bert`\n",
    "            torch.save(self.bert.cpu(), output_path)\n",
    "            self.bert.to(self.device)\n",
    "\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n",
      "Creating BERTMLM Pre-Trainer\n",
      "Total Parameters: 7809584\n",
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 1/1563 [00:03<1:23:36,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 10.638824462890625, 'loss': 10.638824462890625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   6%|| 101/1563 [05:26<1:18:48,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 10.605322082443992, 'loss': 10.55669116973877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  13%|| 201/1563 [10:51<1:13:33,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 10.551837024404042, 'loss': 10.425453186035156}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  19%|| 301/1563 [16:17<1:09:08,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 10.463132956495317, 'loss': 10.140094757080078}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  26%|| 401/1563 [21:49<1:04:58,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 10.32018718338964, 'loss': 9.600696563720703}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  32%|| 501/1563 [27:22<59:29,  3.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 10.024912465832191, 'loss': 7.700746059417725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  38%|| 601/1563 [32:55<52:36,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 9.527322860406759, 'loss': 6.503056526184082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  45%|| 701/1563 [38:28<48:04,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 9.066506063376956, 'loss': 6.2775750160217285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  51%|| 801/1563 [44:03<42:35,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 800, 'avg_loss': 8.662292575717121, 'loss': 5.406287670135498}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  58%|| 901/1563 [49:38<36:58,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 900, 'avg_loss': 8.30162570373332, 'loss': 5.428715705871582}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  64%|| 1001/1563 [55:18<32:15,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1000, 'avg_loss': 7.979950142192555, 'loss': 4.873656749725342}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  70%|| 1101/1563 [1:00:48<26:18,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1100, 'avg_loss': 7.690601850400504, 'loss': 4.651508808135986}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  77%|| 1201/1563 [1:06:48<21:01,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1200, 'avg_loss': 7.4260401975900105, 'loss': 4.370693683624268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  83%|| 1301/1563 [1:12:23<14:38,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1300, 'avg_loss': 7.187353624736777, 'loss': 4.4637041091918945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  90%|| 1401/1563 [1:17:58<09:03,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1400, 'avg_loss': 6.9712318570847005, 'loss': 4.081660270690918}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  96%|| 1501/1563 [1:25:00<03:25,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 1500, 'avg_loss': 6.771714019743623, 'loss': 3.798304557800293}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 1563/1563 [1:28:25<00:00,  3.39s/it]\n",
      "EP_train:1:   0%|| 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_train, avg_loss= 6.658745258341061\n",
      "EP:0 Model Saved on: models/pretrained_bert_imdb_ep0.pt\n",
      "EP:0 Model Saved on: models/pretrained_bert_mlm_imdb_ep0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 1/1563 [00:03<1:25:29,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 4.054785251617432, 'loss': 4.054785251617432}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   6%|| 101/1563 [05:35<1:20:53,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 3.8205379259468306, 'loss': 3.733412742614746}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  13%|| 201/1563 [11:07<1:15:25,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 3.770584896429261, 'loss': 3.403268337249756}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  19%|| 301/1563 [16:39<1:09:49,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 3.725418607261886, 'loss': 3.6450743675231934}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  26%|| 401/1563 [22:11<1:04:32,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 3.701398490967596, 'loss': 3.31532621383667}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  32%|| 501/1563 [27:45<58:51,  3.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 3.674220739962336, 'loss': 3.4641294479370117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  38%|| 601/1563 [33:18<53:24,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 3.6508583955082443, 'loss': 3.413344144821167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  45%|| 701/1563 [38:51<47:57,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 3.6336541502349897, 'loss': 3.295443058013916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  51%|| 801/1563 [44:25<42:15,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 800, 'avg_loss': 3.6191450838143755, 'loss': 3.532745361328125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  58%|| 901/1563 [49:57<36:34,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 900, 'avg_loss': 3.6067620522438752, 'loss': 3.8155910968780518}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  64%|| 1001/1563 [55:30<31:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1000, 'avg_loss': 3.595968320057704, 'loss': 3.428986072540283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  70%|| 1101/1563 [1:01:04<25:43,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1100, 'avg_loss': 3.587662027923764, 'loss': 3.443204641342163}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  77%|| 1201/1563 [1:06:37<20:09,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1200, 'avg_loss': 3.5814723394792543, 'loss': 3.3196585178375244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  83%|| 1301/1563 [1:12:10<14:30,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1300, 'avg_loss': 3.5773279954615598, 'loss': 3.7282352447509766}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  90%|| 1401/1563 [1:17:42<08:58,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1400, 'avg_loss': 3.57540436202845, 'loss': 3.5696499347686768}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  96%|| 1501/1563 [1:23:15<03:27,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 1500, 'avg_loss': 3.5746220460659184, 'loss': 3.5754637718200684}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 1563/1563 [1:26:40<00:00,  3.33s/it]\n",
      "EP_train:2:   0%|| 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_train, avg_loss= 3.574163303417955\n",
      "EP:1 Model Saved on: models/pretrained_bert_imdb_ep1.pt\n",
      "EP:1 Model Saved on: models/pretrained_bert_mlm_imdb_ep1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:   0%|| 1/1563 [00:03<1:25:04,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 3.79193115234375, 'loss': 3.79193115234375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:   6%|| 101/1563 [05:36<1:21:20,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 3.575910513943965, 'loss': 3.5780179500579834}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  13%|| 201/1563 [11:09<1:15:47,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 3.5715106269020347, 'loss': 3.27047061920166}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  19%|| 301/1563 [16:42<1:09:38,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 3.588355082610121, 'loss': 3.561704635620117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  26%|| 401/1563 [22:13<1:03:56,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 3.5986989561161793, 'loss': 3.598663568496704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  32%|| 501/1563 [27:47<59:26,  3.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 3.603646917971308, 'loss': 3.487985372543335}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  38%|| 601/1563 [33:19<53:27,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 3.60939435792247, 'loss': 3.5795154571533203}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  45%|| 701/1563 [38:53<47:45,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 3.617397413444247, 'loss': 3.7290163040161133}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  51%|| 801/1563 [44:25<42:19,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 800, 'avg_loss': 3.6240241834138067, 'loss': 3.7559638023376465}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  58%|| 901/1563 [49:59<36:59,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 900, 'avg_loss': 3.6326325685943535, 'loss': 3.7899270057678223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  64%|| 1001/1563 [55:33<31:16,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1000, 'avg_loss': 3.6404307605503323, 'loss': 3.6170263290405273}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  70%|| 1101/1563 [1:01:08<25:52,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1100, 'avg_loss': 3.6470727645517154, 'loss': 3.709679126739502}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  77%|| 1201/1563 [1:06:45<20:21,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1200, 'avg_loss': 3.654342211255622, 'loss': 3.7558393478393555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  83%|| 1301/1563 [1:12:26<14:59,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1300, 'avg_loss': 3.658344217120089, 'loss': 3.7786026000976562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  90%|| 1401/1563 [1:18:11<09:18,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1400, 'avg_loss': 3.6630661807512914, 'loss': 3.6158828735351562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  96%|| 1501/1563 [1:23:53<03:31,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 1500, 'avg_loss': 3.668805219426622, 'loss': 3.9724879264831543}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 1563/1563 [1:27:22<00:00,  3.35s/it]\n",
      "EP_train:3:   0%|| 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_train, avg_loss= 3.6704552826481756\n",
      "EP:2 Model Saved on: models/pretrained_bert_imdb_ep2.pt\n",
      "EP:2 Model Saved on: models/pretrained_bert_mlm_imdb_ep2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:   0%|| 1/1563 [00:03<1:28:55,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 3.6901683807373047, 'loss': 3.6901683807373047}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:   6%|| 101/1563 [06:00<1:30:10,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 3.7375717989288932, 'loss': 3.8158910274505615}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  13%|| 201/1563 [12:22<1:27:32,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 3.7339510383890637, 'loss': 3.6607964038848877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  19%|| 301/1563 [19:02<1:25:20,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 3.7349421867104464, 'loss': 3.6039233207702637}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  26%|| 401/1563 [25:45<1:17:22,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 3.7326884121074344, 'loss': 3.850945472717285}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  32%|| 501/1563 [32:17<1:09:24,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 3.7295575432196824, 'loss': 3.5935637950897217}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  38%|| 601/1563 [38:43<1:01:28,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 3.7282499699743337, 'loss': 3.4811928272247314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  45%|| 701/1563 [45:07<55:02,  3.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 3.729441330878439, 'loss': 3.6450796127319336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  51%|| 801/1563 [51:28<48:20,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 800, 'avg_loss': 3.7265280227684947, 'loss': 3.7261996269226074}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  58%|| 901/1563 [57:50<41:48,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 900, 'avg_loss': 3.7251465026863935, 'loss': 3.803724527359009}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  64%|| 1001/1563 [1:04:10<35:39,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1000, 'avg_loss': 3.7214676802689497, 'loss': 3.7794032096862793}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  70%|| 1101/1563 [1:10:30<29:06,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1100, 'avg_loss': 3.7200956060927526, 'loss': 3.486332416534424}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  77%|| 1201/1563 [1:16:51<22:59,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1200, 'avg_loss': 3.7183928668350106, 'loss': 3.7254111766815186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  83%|| 1301/1563 [1:23:10<16:37,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1300, 'avg_loss': 3.7149983456646085, 'loss': 3.789834499359131}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  90%|| 1401/1563 [1:29:29<10:17,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1400, 'avg_loss': 3.7125929964175826, 'loss': 3.6774072647094727}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  96%|| 1501/1563 [1:35:49<03:55,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 1500, 'avg_loss': 3.7087008386036304, 'loss': 3.657022714614868}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 1563/1563 [1:39:44<00:00,  3.83s/it]\n",
      "EP_train:4:   0%|| 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_train, avg_loss= 3.7056290476427427\n",
      "EP:3 Model Saved on: models/pretrained_bert_imdb_ep3.pt\n",
      "EP:3 Model Saved on: models/pretrained_bert_mlm_imdb_ep3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:   0%|| 1/1563 [00:03<1:37:12,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 3.7270054817199707, 'loss': 3.7270054817199707}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:   6%|| 101/1563 [06:25<1:33:33,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 3.67496123644385, 'loss': 3.745459794998169}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  13%|| 201/1563 [12:48<1:27:12,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 3.6575916204879535, 'loss': 3.4743785858154297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  19%|| 301/1563 [19:14<1:20:59,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 3.6503711301226947, 'loss': 3.515561580657959}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  26%|| 401/1563 [25:40<1:15:12,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 3.6413358066444683, 'loss': 3.736457586288452}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  32%|| 501/1563 [32:06<1:07:59,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 3.63291634818513, 'loss': 3.459174633026123}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  38%|| 601/1563 [38:32<1:01:49,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 3.623168648181858, 'loss': 3.5275418758392334}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  45%|| 701/1563 [44:51<54:04,  3.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 3.6101035782682063, 'loss': 3.5620720386505127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  51%|| 801/1563 [51:05<47:41,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 800, 'avg_loss': 3.595455558410149, 'loss': 3.4742703437805176}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  58%|| 901/1563 [57:24<42:02,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 900, 'avg_loss': 3.580404379788567, 'loss': 3.4566409587860107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  64%|| 1001/1563 [1:03:47<35:49,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1000, 'avg_loss': 3.5649189351202843, 'loss': 3.416919708251953}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  70%|| 1101/1563 [1:10:09<29:16,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1100, 'avg_loss': 3.5474761389906466, 'loss': 3.3122475147247314}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  77%|| 1201/1563 [1:16:27<22:35,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1200, 'avg_loss': 3.529168447388102, 'loss': 3.480825424194336}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  83%|| 1301/1563 [1:22:33<15:52,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1300, 'avg_loss': 3.511002968678925, 'loss': 3.2279672622680664}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  90%|| 1401/1563 [1:28:30<09:45,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1400, 'avg_loss': 3.4938600099061237, 'loss': 3.1182541847229004}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  96%|| 1501/1563 [1:34:13<03:29,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 1500, 'avg_loss': 3.475259596311911, 'loss': 3.1342809200286865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 1563/1563 [1:37:41<00:00,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_train, avg_loss= 3.465032256724967\n",
      "EP:4 Model Saved on: models/pretrained_bert_imdb_ep4.pt\n",
      "EP:4 Model Saved on: models/pretrained_bert_mlm_imdb_ep4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building BERT model\")\n",
    "bert = BERT(config.VOCAB_SIZE, max_len=config.MAX_LEN, hidden=config.EMBED_DIM, n_layers=config.NUM_LAYERS, attn_heads=config.NUM_HEAD)\n",
    "\n",
    "print(\"Creating BERTMLM Pre-Trainer\")\n",
    "trainer = BERTMLMTrainer(bert, config.VOCAB_SIZE, train_dataloader=mlm_loader, test_dataloader=None, lr=config.LR)\n",
    "\n",
    "print(\"Training Start\")\n",
    "# 17m / epoch (# of data samples : 5000), 1h 30m / epoch (# of data samples : 25000)\n",
    "for epoch in range(5): \n",
    "    trainer.train(epoch)\n",
    "    # Save BERT\n",
    "    trainer.save(epoch, file_path=\"models/pretrained_bert_imdb\" + \"_ep%d\" % epoch + \".pt\", mlm = False)\n",
    "    # Save BERT MLM\n",
    "    trainer.save(epoch, file_path=\"models/pretrained_bert_mlm_imdb\" + \"_ep%d\" % epoch + \".pt\", mlm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    - id2token  : tokenizer.decode\n",
    "    - token2id  : tokenizer.encode\n",
    "\"\"\"\n",
    "\n",
    "class MaskedTextGenerator():\n",
    "    def __init__(self, model_path, sample_tokens, top_k=5):\n",
    "        self.path = model_path\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return tokenizer.decode(tokens)\n",
    "    \n",
    "    def get_prediction(self):\n",
    "        # load model and set to eval mode\n",
    "        model = torch.load(self.path)\n",
    "        model.eval()\n",
    "        \n",
    "        prediction = model.forward(self.sample_tokens)\n",
    "        \n",
    "        masked_index = np.where(self.sample_tokens == tokenizer.mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = torch.exp(values[i]) #log softmax에서 log 상쇄\n",
    "            tokens = np.copy(self.sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            \n",
    "            result = {\n",
    "                \"input_text\": self.decode(self.sample_tokens[0]),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.decode(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': '[CLS] i have watched this [MASK] and it was awesome [SEP]',\n",
      " 'predicted mask token': 't h e',\n",
      " 'prediction': '[CLS] i have watched this the and it was awesome [SEP]',\n",
      " 'probability': tensor(0.0196, grad_fn=<ExpBackward>)}\n",
      "\n",
      "\n",
      "{'input_text': '[CLS] i have watched this [MASK] and it was awesome [SEP]',\n",
      " 'predicted mask token': 'b u t',\n",
      " 'prediction': '[CLS] i have watched this but and it was awesome [SEP]',\n",
      " 'probability': tensor(0.0227, grad_fn=<ExpBackward>)}\n",
      "\n",
      "\n",
      "{'input_text': '[CLS] i have watched this [MASK] and it was awesome [SEP]',\n",
      " 'predicted mask token': 'a n d',\n",
      " 'prediction': '[CLS] i have watched this and and it was awesome [SEP]',\n",
      " 'probability': tensor(0.0296, grad_fn=<ExpBackward>)}\n",
      "\n",
      "\n",
      "{'input_text': '[CLS] i have watched this [MASK] and it was awesome [SEP]',\n",
      " 'predicted mask token': 'h a v e',\n",
      " 'prediction': '[CLS] i have watched this have and it was awesome [SEP]',\n",
      " 'probability': tensor(0.0348, grad_fn=<ExpBackward>)}\n",
      "\n",
      "\n",
      "{'input_text': '[CLS] i have watched this [MASK] and it was awesome [SEP]',\n",
      " 'predicted mask token': 'f i l m',\n",
      " 'prediction': '[CLS] i have watched this film and it was awesome [SEP]',\n",
      " 'probability': tensor(0.0718, grad_fn=<ExpBackward>)}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_tokens = tokenizer.encode(\"I have watched this [MASK] and it was awesome\") # list\n",
    "MaskedTextGenerator('models/pretrained_bert_mlm_imdb_ep3.pt', torch.tensor(sample_tokens).reshape(1,-1)).get_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcGFKG6e0fcK"
   },
   "source": [
    "## Fine-tune a sentiment classification model\n",
    "\n",
    "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
    "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
    "pretrained BERT features.\n",
    "\n",
    "* Reference 1 : https://www.kaggle.com/kernelk/imdb-sa-bert-fine-tuning-gpu-92-acc\n",
    "* Reference 2 : https://github.com/rahulbhalley/sentiment-analysis-bert.pytorch/blob/master/main.py\n",
    "\n",
    "**ISSUE 1** : `requires_grad` vs `no_grad` 차이\n",
    "* Reference 1 [(링크)](https://stackoverflow.com/questions/63785319/pytorch-torch-no-grad-versus-requires-grad-false)\n",
    "> torch.no_grad()는 오차 역전파에 사용하는 계산량을 줄여서 처리 속도를 높임\n",
    "* Reference 2 [(링크)](https://stackoverflow.com/questions/51748138/pytorch-how-to-set-requires-grad-false)\n",
    "> torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
    "> autograd에서는 이를 추적하지 않음\n",
    "* Reference 3 [(링크1)](https://statisticsplaybook.tistory.com/12) [(링크2)](https://codlingual.tistory.com/72)\n",
    "> autograd / detach / requires_grad / no_grad 설명\n",
    "> - 해당 텐서에 대한 계산 모두 tracking해서 기울기 구해주기 : requires_grad=True \n",
    "> - 중간에 requires_grad 넣어주고 싶다면(in-place) : .requires_grad_(True)\n",
    "> - 중간에 requires_grad 그만하고 싶다면 : with torch.no_grad() \n",
    "* Reference 4 [(링크)](https://discuss.pytorch.org/t/no-grad-vs-requires-grad/21272)\n",
    "> no_grad()는 backward() 호출하지 않음 (= gradient 계산하지 않음) <br>\n",
    "requires_grad 는 tensor 생성시와 관련, \n",
    "기본적으로 nn.Modules 패키지에서는 gradient 가 필요한 layer의 parameter들이 정의될 때, requires_grad가 True로 설정됨 \n",
    "\n",
    "**[정리]** : no_grad 는 gradient를 계산할 것인가 말 것인가 <br>\n",
    "&emsp; &emsp; &emsp; requires_grad는 학습 가능한 parameter로서 메모리에 올릴 것인가 말 것인가\n",
    "\n",
    "&emsp; &emsp; &emsp; no_grad() 로 감싸더라도, 정의된 parameter의 tensor 속성에서 reuqires_grad = True일 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "2yL9Otod0fcK"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifierBERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT based Sentiment Classification Model\n",
    "    \"\"\"\n",
    "    def __init__(self, bert: BERT, max_len, hidden, num_class, n_layers, freeze):\n",
    "        super().__init__()\n",
    "        self.bert = bert # pre-trained BERT, output size : (32, 256, 128)\n",
    "        self.max_len = max_len\n",
    "        self.hidden = hidden\n",
    "        self.num_class = num_class\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.linear_1 = nn.Linear(self.hidden, int(self.hidden/2))        # 128 -> 64\n",
    "        self.linear_2 = nn.Linear(int(self.hidden/2), self.num_class-1)   # 64 -> 1\n",
    "        \n",
    "        # True for freezing the BERT model when train the classifier \n",
    "        # False for unfreezing the BERT model for fine-tuning\n",
    "        self.freeze = freeze\n",
    "        \n",
    "        # ISSUE\n",
    "        if self.freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "    # ISSUE      \n",
    "    def forward(self, x):\n",
    "#         if self.freeze:\n",
    "#             with torch.no_grad():\n",
    "#                 out = self.bert(x)\n",
    "#         else:\n",
    "#             out = self.bert(x)\n",
    "        out = self.bert(x)\n",
    "    \n",
    "        # CLS representation 부분만 추출 (32, 256, 128) -> (32, 128)\n",
    "        # https://www.kaggle.com/kernelk/imdb-sa-bert-fine-tuning-gpu-92-acc\n",
    "        out = out[:,:1,:].reshape(-1, self.hidden)\n",
    "        out = self.linear_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = F.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "# Train the classifier\n",
    "class SentimentClassifierBERTTrainer:\n",
    "    \"\"\"\n",
    "    BERTMLMTrainer make the pretrained BERT model with Masked Language Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: SentimentClassifierBERT,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                 with_cuda: bool = True, log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param model: BERT based Sentiment Classification Model which you want to train\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        # CUDA out of memory.로 False 처리\n",
    "        cuda_condition = False # torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # Initialize the BERT based Sentiment Classification Model\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.model.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "    \n",
    "        \n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "        \n",
    "    # computes accuracy\n",
    "    def binary_accuracy(self, preds, y):\n",
    "        # rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        rounded_preds = torch.round(preds)\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        \n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the bert classifier model\n",
    "            classifier_output = self.model.forward(data[\"bert_input\"]) # input : torch.Size([32, 256, 30000])\n",
    "\n",
    "            # Binary Cross Entropy Loss\n",
    "            target = data[\"bert_label\"].reshape(classifier_output.shape[0], -1)\n",
    "            loss = self.criterion(classifier_output, target.to(torch.float32)) # torch.Size([32, 1]), torch.Size([32, 1])\n",
    "\n",
    "            # 2. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "    \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # 3. Calculate Accuracy during Test\n",
    "            if not train:\n",
    "                acc = self.binary_accuracy(torch.tensor(classifier_output.reshape(-1)), data[\"bert_label\"]) # torch.Size([32])\n",
    "                avg_acc += acc.item()\n",
    "                \n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"loss\": loss.item(),\n",
    "                \"avg_acc\": avg_acc / (i + 1)\n",
    "           }\n",
    "                            \n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter))\n",
    "        \n",
    "        if not train:\n",
    "            print(\"** Average Accuracy=\", avg_acc / len(data_iter))\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "    def save(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "        Saving the current BERT Classification Model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        \n",
    "        output_path = file_path\n",
    "        torch.save(self.model.cpu(), output_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier with frozen BERT stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/782 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "EP_train:0:   0%|| 1/782 [00:00<02:29,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT based Sentiment Classification model\n",
      "===================================\n",
      "Total Parameters: 3947905\n",
      "Trainable Parameters: 8321\n",
      "Non-trainable Parameters: 3939584\n",
      "===================================\n",
      "Creating SentimentClassifierBERT Trainer\n",
      "Training Start\n",
      "> Train the classifier with frozen BERT stage\n",
      "{'epoch': 0, 'iter': 0, 'avg_loss': 0.8372408747673035, 'loss': 0.8372408747673035, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  13%|| 102/782 [00:19<02:10,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 0.7326829687203511, 'loss': 0.6723736524581909, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  26%|| 202/782 [00:38<01:50,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 0.7206003381245172, 'loss': 0.7028611898422241, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  39%|| 302/782 [00:57<01:33,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 0.714116308380203, 'loss': 0.7097827792167664, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  51%|| 402/782 [01:17<01:14,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 0.7098862756220182, 'loss': 0.6920859217643738, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  64%|| 501/782 [01:35<00:52,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 0.7072109860812357, 'loss': 0.6763255596160889, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  77%|| 602/782 [01:55<00:34,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 0.7054477796181664, 'loss': 0.6936476826667786, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  90%|| 702/782 [02:14<00:15,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 0.7036543609242296, 'loss': 0.6812425255775452, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 782/782 [02:29<00:00,  5.23it/s]\n",
      "EP_test:0:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_train, avg_loss= 0.7027720026957714\n",
      "EP:0 Model Saved on: models/frozen_bert_sent_classifier_imdb_ep0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "EP_test:0:   0%|| 1/782 [00:00<02:32,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 0.6935998201370239, 'loss': 0.6935998201370239, 'avg_acc': 0.5625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  13%|| 102/782 [00:19<02:09,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 0.6941214451695433, 'loss': 0.691312849521637, 'avg_acc': 0.5077351485148515}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  26%|| 202/782 [00:38<01:53,  5.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 0.6944519402968943, 'loss': 0.692236065864563, 'avg_acc': 0.5009328358208955}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  39%|| 302/782 [00:57<01:31,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 0.6938447065131609, 'loss': 0.6936025023460388, 'avg_acc': 0.5060215946843853}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  51%|| 402/782 [01:16<01:11,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 0.6941319333942156, 'loss': 0.6993346810340881, 'avg_acc': 0.5041302992518704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  64%|| 502/782 [01:35<00:53,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 0.6938351574772132, 'loss': 0.6733691096305847, 'avg_acc': 0.5062375249500998}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  77%|| 602/782 [01:54<00:34,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 0.6932642514416064, 'loss': 0.6702039837837219, 'avg_acc': 0.5097233777038269}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  90%|| 702/782 [02:14<00:15,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 0.6933398903181481, 'loss': 0.7041476964950562, 'avg_acc': 0.5086929386590585}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0: 100%|| 782/782 [02:29<00:00,  5.25it/s]\n",
      "EP_train:1:   0%|| 1/782 [00:00<02:30,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_test, avg_loss= 0.6933572802244855\n",
      "** Average Accuracy= 0.5076726342710998\n",
      "\n",
      "\n",
      "{'epoch': 1, 'iter': 0, 'avg_loss': 0.6975748538970947, 'loss': 0.6975748538970947, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  13%|| 102/782 [00:19<02:09,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 0.6921120038126954, 'loss': 0.6841808557510376, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  26%|| 202/782 [00:38<01:48,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 0.6935400933175537, 'loss': 0.6848986744880676, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  39%|| 302/782 [00:57<01:31,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 0.6944372281679679, 'loss': 0.6909135580062866, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  51%|| 402/782 [01:17<01:13,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 0.6943746244818195, 'loss': 0.7187724113464355, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  64%|| 502/782 [01:36<00:53,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 0.6945163479108296, 'loss': 0.6970822215080261, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  77%|| 602/782 [01:55<00:34,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 0.6951956585322363, 'loss': 0.6985996961593628, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  90%|| 702/782 [02:14<00:15,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 0.6949623694773578, 'loss': 0.6778969764709473, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 782/782 [02:29<00:00,  5.23it/s]\n",
      "EP_test:1:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_train, avg_loss= 0.6949799536439158\n",
      "EP:1 Model Saved on: models/frozen_bert_sent_classifier_imdb_ep1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:   0%|| 2/782 [00:00<02:29,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 0.6907983422279358, 'loss': 0.6907983422279358, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  13%|| 102/782 [00:19<02:08,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 0.6952131957110792, 'loss': 0.6886323690414429, 'avg_acc': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  26%|| 202/782 [00:38<01:49,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 0.6934152844533399, 'loss': 0.7092522978782654, 'avg_acc': 0.509639303482587}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  38%|| 301/782 [00:57<01:37,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 0.6939746608765814, 'loss': 0.7141644954681396, 'avg_acc': 0.5057101328903655}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  51%|| 401/782 [01:16<01:19,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 0.694219509562352, 'loss': 0.6918331384658813, 'avg_acc': 0.5038965087281796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  64%|| 502/782 [01:35<00:53,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 0.6939664992267738, 'loss': 0.6868457794189453, 'avg_acc': 0.5051771457085829}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  77%|| 602/782 [01:54<00:34,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 0.6945508972380602, 'loss': 0.712501585483551, 'avg_acc': 0.5011439267886856}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  90%|| 702/782 [02:13<00:15,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 0.6944295147516248, 'loss': 0.7003517746925354, 'avg_acc': 0.5025410128388017}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1: 100%|| 782/782 [02:28<00:00,  5.25it/s]\n",
      "EP_train:2:   0%|| 1/782 [00:00<02:28,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_test, avg_loss= 0.6947336402695502\n",
      "** Average Accuracy= 0.501238810741688\n",
      "\n",
      "\n",
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.6847983002662659, 'loss': 0.6847983002662659, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  13%|| 102/782 [00:19<02:08,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 0.6984588214666536, 'loss': 0.6993970274925232, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  26%|| 202/782 [00:38<01:50,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 0.6970613865710017, 'loss': 0.6904000639915466, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  38%|| 301/782 [00:57<01:30,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 0.6962155079920822, 'loss': 0.6946625709533691, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  51%|| 401/782 [01:17<01:11,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 0.6963142725892197, 'loss': 0.7180339097976685, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  64%|| 502/782 [01:36<00:54,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 0.6958215035602242, 'loss': 0.6912544965744019, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  77%|| 602/782 [01:55<00:35,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 0.6959143745125629, 'loss': 0.7070531845092773, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  90%|| 702/782 [02:15<00:15,  5.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 0.6958309144164289, 'loss': 0.6940892338752747, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 782/782 [02:30<00:00,  5.20it/s]\n",
      "EP_test:2:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_train, avg_loss= 0.6959463668906171\n",
      "EP:2 Model Saved on: models/frozen_bert_sent_classifier_imdb_ep2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:   0%|| 2/782 [00:00<02:25,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.7022498846054077, 'loss': 0.7022498846054077, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  13%|| 102/782 [00:19<02:10,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 0.705781898286083, 'loss': 0.7104378938674927, 'avg_acc': 0.4969059405940594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  26%|| 202/782 [00:38<01:49,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 0.7013423383532472, 'loss': 0.7205120325088501, 'avg_acc': 0.507773631840796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  39%|| 302/782 [00:57<01:31,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 0.702906066991166, 'loss': 0.7458125948905945, 'avg_acc': 0.5037375415282392}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  51%|| 401/782 [01:16<01:14,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 0.703130965369598, 'loss': 0.6988612413406372, 'avg_acc': 0.5028834164588528}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  64%|| 502/782 [01:35<00:54,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 0.7024619638800859, 'loss': 0.6833321452140808, 'avg_acc': 0.5043662674650699}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  77%|| 602/782 [01:54<00:34,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 0.7035155834850177, 'loss': 0.7579454779624939, 'avg_acc': 0.5007799500831946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  90%|| 702/782 [02:13<00:15,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 0.7034609396855603, 'loss': 0.704017698764801, 'avg_acc': 0.50160485021398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2: 100%|| 782/782 [02:28<00:00,  5.26it/s]\n",
      "EP_train:3:   0%|| 1/782 [00:00<02:30,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_test, avg_loss= 0.7037517400958654\n",
      "** Average Accuracy= 0.5002797314578005\n",
      "\n",
      "\n",
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.7362059354782104, 'loss': 0.7362059354782104, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  13%|| 102/782 [00:19<02:09,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 0.696674394135428, 'loss': 0.6924814581871033, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  26%|| 201/782 [00:38<01:51,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 0.6972500228170139, 'loss': 0.6878581047058105, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  39%|| 302/782 [00:57<01:29,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 0.696761871691162, 'loss': 0.6838343739509583, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  51%|| 402/782 [01:16<01:13,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 0.6962556630893241, 'loss': 0.6683324575424194, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  64%|| 502/782 [01:35<00:53,  5.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 0.6963780452153402, 'loss': 0.6903771758079529, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  77%|| 601/782 [01:54<00:39,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 0.6968694054544865, 'loss': 0.7062279582023621, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  90%|| 702/782 [02:13<00:15,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 0.6974458719116134, 'loss': 0.6994295120239258, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 782/782 [02:29<00:00,  5.25it/s]\n",
      "EP_test:3:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_train, avg_loss= 0.6971708307485751\n",
      "EP:3 Model Saved on: models/frozen_bert_sent_classifier_imdb_ep3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:   0%|| 2/782 [00:00<02:28,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.6864240169525146, 'loss': 0.6864240169525146, 'avg_acc': 0.5625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  13%|| 102/782 [00:19<02:09,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 0.693004323114263, 'loss': 0.6865842342376709, 'avg_acc': 0.500309405940594}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  26%|| 202/782 [00:38<01:51,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 0.6926989353711333, 'loss': 0.6963832378387451, 'avg_acc': 0.5046641791044776}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  39%|| 302/782 [00:57<01:31,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 0.692268161282587, 'loss': 0.6860608458518982, 'avg_acc': 0.5118355481727574}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  51%|| 402/782 [01:16<01:13,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 0.6924411713928356, 'loss': 0.6955655217170715, 'avg_acc': 0.5116895261845387}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  64%|| 502/782 [01:35<00:53,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 0.692532124752532, 'loss': 0.6922560930252075, 'avg_acc': 0.5111027944111777}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  77%|| 602/782 [01:54<00:33,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 0.6923321419071635, 'loss': 0.6813623905181885, 'avg_acc': 0.5142470881863561}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  90%|| 702/782 [02:13<00:15,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 0.6925283943365372, 'loss': 0.7004382014274597, 'avg_acc': 0.5121255349500713}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3: 100%|| 782/782 [02:29<00:00,  5.25it/s]\n",
      "EP_train:4:   0%|| 1/782 [00:00<02:27,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_test, avg_loss= 0.69251077452584\n",
      "** Average Accuracy= 0.5116687979539642\n",
      "\n",
      "\n",
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.6978344917297363, 'loss': 0.6978344917297363, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  13%|| 102/782 [00:19<02:10,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 0.6980472814918744, 'loss': 0.7014200091362, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  26%|| 202/782 [00:38<01:49,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 0.6973533037290052, 'loss': 0.7180112600326538, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  39%|| 302/782 [00:57<01:30,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 0.6965466226850238, 'loss': 0.6993234157562256, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  51%|| 402/782 [01:16<01:12,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 0.6963900740901728, 'loss': 0.723945677280426, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  64%|| 502/782 [01:35<00:53,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 0.6964390457509283, 'loss': 0.6874792575836182, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  77%|| 602/782 [01:54<00:34,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 0.6965688869679431, 'loss': 0.7117697596549988, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  90%|| 702/782 [02:13<00:15,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 0.6962871555765073, 'loss': 0.6867028474807739, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 782/782 [02:29<00:00,  5.25it/s]\n",
      "EP_test:4:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_train, avg_loss= 0.6965518442871016\n",
      "EP:4 Model Saved on: models/frozen_bert_sent_classifier_imdb_ep4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:   0%|| 2/782 [00:00<02:33,  5.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.689827561378479, 'loss': 0.689827561378479, 'avg_acc': 0.46875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  13%|| 102/782 [00:19<02:08,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 0.6937542799675819, 'loss': 0.6952723860740662, 'avg_acc': 0.5071163366336634}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  26%|| 202/782 [00:38<01:49,  5.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 0.6941451258327237, 'loss': 0.6864056587219238, 'avg_acc': 0.5007773631840796}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  39%|| 302/782 [00:57<01:31,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 0.6935451810938179, 'loss': 0.6767478585243225, 'avg_acc': 0.5070598006644518}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  51%|| 402/782 [01:16<01:11,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 0.6937835805136664, 'loss': 0.7023389935493469, 'avg_acc': 0.5066240648379052}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  64%|| 502/782 [01:35<00:52,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 0.6937460877938185, 'loss': 0.7005965113639832, 'avg_acc': 0.5066117764471058}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  77%|| 602/782 [01:54<00:33,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 0.6934431920234058, 'loss': 0.6802788972854614, 'avg_acc': 0.5091514143094842}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  90%|| 702/782 [02:13<00:15,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 0.6936264697053123, 'loss': 0.7100619673728943, 'avg_acc': 0.5080242510699001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4: 100%|| 782/782 [02:28<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_test, avg_loss= 0.6935905889629403\n",
      "** Average Accuracy= 0.5083120204603581\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# mlm_model = torch.load('models/pretrained_bert_mlm_imdb_ep4.pt')\n",
    "# mlm_model.train()\n",
    "\n",
    "# pretrained_bert_model = mlm_model.bert\n",
    "\n",
    "pretrained_bert_model = torch.load('models/pretrained_bert_imdb_ep4.pt')\n",
    "pretrained_bert_model.train()\n",
    "\n",
    "# Set freeze=True\n",
    "print(\"Building BERT based Sentiment Classification model\")\n",
    "sentiment_classifer_bert = SentimentClassifierBERT(pretrained_bert_model, max_len=config.MAX_LEN,\n",
    "                                                   hidden=config.EMBED_DIM, num_class = 2, \n",
    "                                                   n_layers=config.NUM_LAYERS, freeze=True)\n",
    "\n",
    "\n",
    "# Method 1\n",
    "# pretrained_bert_model.eval() \n",
    "# #질문 : eval 사용시 dropout 이나 batchnorm 수행 안 되는데, \n",
    "# #이렇게 freeze하고 학습시키는 것이 일반적인가? 아니면, 직접 gradient 계산 안 되게 하고 dropout이나 batchnor 수행되게 해야하는지\n",
    "\n",
    "# Method 2\n",
    "# # 직접 requires_grad False처리 -> 아래 기능을 SentimentClassifierBERT 내에서 option 으로 정의해둠\n",
    "# for _ in pretrained_bert_model.parameters():\n",
    "#     _.requires_grad_(False)\n",
    "\n",
    "# sentiment_classifer_bert = SentimentClassifierBERT(pretrained_bert_model, max_len=config.MAX_LEN,\n",
    "#                                                    hidden=config.EMBED_DIM, num_class = 2, \n",
    "#                                                    n_layers=config.NUM_LAYERS, freeze=True)\n",
    "\n",
    "trainable_para = sum([p.nelement() for p in sentiment_classifer_bert.parameters() if p.requires_grad])\n",
    "non_trainable_para = sum([p.nelement() for p in sentiment_classifer_bert.bert.parameters() if not p.requires_grad])\n",
    "\n",
    "print(\"===================================\")\n",
    "print(\"Total Parameters:\", trainable_para + non_trainable_para)\n",
    "print(\"Trainable Parameters:\", trainable_para)\n",
    "print(\"Non-trainable Parameters:\", non_trainable_para)\n",
    "print(\"===================================\")\n",
    "\n",
    "print(\"Creating SentimentClassifierBERT Trainer\")\n",
    "trainer = SentimentClassifierBERTTrainer(sentiment_classifer_bert, train_dataloader=train_loader,\n",
    "                         test_dataloader=test_loader, lr=config.LR, log_freq=100)\n",
    "\n",
    "print(\"Training Start\")\n",
    "print(\"> Train the classifier with frozen BERT stage\")\n",
    "for epoch in range(5):\n",
    "    trainer.train(epoch)\n",
    "\n",
    "    # Save trained classifier\n",
    "    trainer.save(epoch, file_path=\"models/frozen_bert_sent_classifier_imdb\" + \"_ep%d\" % epoch + \".pt\")\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier with unfrozen BERT stage\n",
    "* Unfreeze the BERT model for fine-tuning\n",
    "\n",
    "**ISSUE 2** : ckpt 불러와서 모든 parameter trainable하게 만들기 \n",
    "\n",
    "   * 모델 정의 : args로 분기 태워서 freeze / unfreeze 하도록 함 (자체적으로 모델 학습 시에는 적용 ok)\n",
    "   * freeze = True로 학습하고 저장 후 모델 불러왔을 때에는 freeze=True가 그대로 유지.\n",
    "     아래처럼 ckpt를 load하고 freeze=False해도 반영이 되지 않음\n",
    "    \n",
    "```Python\n",
    "# ISSUE\n",
    "# trained_sentiment_classifier.freeze = False \n",
    "```\n",
    "\n",
    "   * transfer learning시에 모델 받아서 직접 특정 layer를 `param.requires_grad=True/False`로 명시해야 함\n",
    "\n",
    "\n",
    "**[참고] `model.eval()`과 `no_grad()`의 차이** [(링크)](https://coffeedjimmy.github.io/pytorch/2019/11/05/pytorch_nograd_vs_train_eval/)\n",
    "- `model.eval()` : `torch.no_grad()`의 주된 목적은 autograd를 끔으로써 메모리 사용량을 줄이고 연산 속도를 높히기 위함이다. 사실상 어짜피 안쓸 gradient인데 inference시에 굳이 계산할 필요가 없지 않은가?\n",
    "- `torch.no_grad()`만 쓰면 되지 않나? gradient 계산 안하고 이제 됐잖아 라고 생각할 수 있다. 맞는 말이지만, `model.eval()`의 역할은 약간 다르다. 현재(2019년) 시점에서는 모델링 시 training과 inference시에 다르게 동작하는 layer들이 존재한다. 예를 들면, Dropout layer는 학습시에는 동작해야하지만, inference시에는 동작하지 않는 것과 같은 예시를 들 수 있다. BatchNorm같은 경우도 마찬가지다.\n",
    "\n",
    "- 사실상 `model.eval()`는 이런 layer들의 동작을 inference(eval) mode로 바꿔준다는 목적으로 사용된다. 따라서, 우리가 보통 원하는 모델의 동작을 위해서는 위의 두 가지를 모두 사용해야하는 것이 맞다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer learning with freezing\n",
    "* [freeze selected layers of a model in Pytorch?](https://stackoverflow.com/questions/62523912/how-to-freeze-selected-layers-of-a-model-in-pytorch)\n",
    "* [Transfer Learning tutorial(Py-Doc)](http://seba1511.net/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "* [Blog Post - freeze a speific layer](https://m.blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=laowaibang&logNo=222155729655&proxyReferer=)\n",
    "* [freeze example code](https://gist.github.com/L0SG/2f6d81e4ad119c4f798ab81fa8d62d3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained BERT based Sentiment Classifier\n",
      "Building BERT based Sentiment Classification model\n",
      "===================================\n",
      "Total Parameters: 3947905\n",
      "Trainable Parameters: 3947905\n",
      "Non-trainable Parameters: 0\n",
      "===================================\n",
      "Creating SentimentClassifierBERT Trainer\n",
      "Training Start\n",
      "> Train the classifier with unfrozen BERT stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 1/782 [00:00<04:24,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 1.3834377527236938, 'loss': 1.3834377527236938, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  13%|| 101/782 [00:31<03:37,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 0.9261812302145628, 'loss': 0.6959831118583679, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  26%|| 201/782 [01:03<03:04,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 0.8141611319276231, 'loss': 0.722547709941864, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  38%|| 301/782 [01:35<02:34,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 0.7764555782178708, 'loss': 0.6736135482788086, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  51%|| 401/782 [02:07<02:00,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 0.7574033540977801, 'loss': 0.7137646079063416, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  64%|| 501/782 [02:39<01:27,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 0.7453714367634284, 'loss': 0.6914842128753662, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  77%|| 601/782 [03:10<00:56,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 0.7371070376053428, 'loss': 0.6872071027755737, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  90%|| 701/782 [03:41<00:24,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 0.7309767861509119, 'loss': 0.7067776918411255, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 782/782 [04:06<00:00,  3.17it/s]\n",
      "EP_test:0:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_train, avg_loss= 0.7272990787273172\n",
      "EP:0 Model Saved on: models/ft_bert_sent_classifier_imdb_ep0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "EP_test:0:   0%|| 2/782 [00:00<02:15,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 0.6924170851707458, 'loss': 0.6924170851707458, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  13%|| 102/782 [00:16<01:44,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 0.6929010436086371, 'loss': 0.6869609355926514, 'avg_acc': 0.5064975247524752}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  26%|| 202/782 [00:32<01:33,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 0.6923891774457486, 'loss': 0.7045437097549438, 'avg_acc': 0.5127487562189055}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  39%|| 302/782 [00:48<01:21,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 0.6924717305506582, 'loss': 0.6889855861663818, 'avg_acc': 0.5115240863787376}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  51%|| 402/782 [01:03<00:59,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 0.6929001103612847, 'loss': 0.6858383417129517, 'avg_acc': 0.5087281795511222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  64%|| 502/782 [01:20<00:47,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 0.6924132626213714, 'loss': 0.6761914491653442, 'avg_acc': 0.5121631736526946}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  77%|| 602/782 [01:36<00:27,  6.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 0.692743931653694, 'loss': 0.7078056335449219, 'avg_acc': 0.5077475041597338}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  90%|| 702/782 [01:52<00:12,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 0.6929490799230448, 'loss': 0.709393322467804, 'avg_acc': 0.5071772467902995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0: 100%|| 782/782 [02:05<00:00,  6.22it/s]\n",
      "EP_train:1:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_test, avg_loss= 0.6931911559056139\n",
      "** Average Accuracy= 0.5059942455242967\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 1/782 [00:00<04:03,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 0.6913052797317505, 'loss': 0.6913052797317505, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  13%|| 101/782 [00:31<03:34,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 0.6948315920216022, 'loss': 0.6909817457199097, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  26%|| 201/782 [01:03<02:58,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 0.6954520344734192, 'loss': 0.6913866996765137, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  38%|| 301/782 [01:34<02:28,  3.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 0.6953620140338657, 'loss': 0.6954710483551025, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  51%|| 401/782 [02:05<02:01,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 0.6963499403951174, 'loss': 0.695523738861084, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  64%|| 501/782 [02:37<01:28,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 0.6963460391866947, 'loss': 0.6861171126365662, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  77%|| 601/782 [03:07<00:53,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 0.696193938247376, 'loss': 0.6829378008842468, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  90%|| 701/782 [03:37<00:23,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 0.696231978425966, 'loss': 0.6964172720909119, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 782/782 [04:00<00:00,  3.25it/s]\n",
      "EP_test:1:   0%|| 1/782 [00:00<01:40,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_train, avg_loss= 0.696256763108856\n",
      "EP:1 Model Saved on: models/ft_bert_sent_classifier_imdb_ep1.pt\n",
      "{'epoch': 1, 'iter': 0, 'avg_loss': 0.6964401602745056, 'loss': 0.6964401602745056, 'avg_acc': 0.46875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  13%|| 102/782 [00:14<01:43,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 0.6931739313767689, 'loss': 0.6872131824493408, 'avg_acc': 0.5058787128712872}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  26%|| 202/782 [00:28<01:21,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 0.6927910055687179, 'loss': 0.6982260346412659, 'avg_acc': 0.5107276119402985}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  39%|| 302/782 [00:41<01:07,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 0.6928680693588384, 'loss': 0.7081049084663391, 'avg_acc': 0.5105897009966778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  51%|| 402/782 [00:55<00:55,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 0.6933055870848106, 'loss': 0.6865959763526917, 'avg_acc': 0.5074033665835411}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  64%|| 502/782 [01:09<00:38,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 0.6929929522935026, 'loss': 0.691059410572052, 'avg_acc': 0.5092315369261478}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  77%|| 602/782 [01:23<00:23,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 0.6933316999981288, 'loss': 0.712873637676239, 'avg_acc': 0.5040557404326124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1:  90%|| 702/782 [01:37<00:11,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 0.693117199202577, 'loss': 0.6922761797904968, 'avg_acc': 0.5067760342368046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:1: 100%|| 782/782 [01:48<00:00,  7.19it/s]\n",
      "EP_train:2:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_test, avg_loss= 0.69321975470199\n",
      "** Average Accuracy= 0.5067934782608695\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:   0%|| 1/782 [00:00<03:37,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.6986697912216187, 'loss': 0.6986697912216187, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  13%|| 101/782 [00:29<03:21,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 0.692786709506913, 'loss': 0.6711193919181824, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  26%|| 201/782 [00:59<02:51,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 0.6937022182478834, 'loss': 0.6913369297981262, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  38%|| 301/782 [01:29<02:22,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 0.6938880672090474, 'loss': 0.6852484941482544, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  51%|| 401/782 [01:58<01:51,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 0.6942996107729296, 'loss': 0.7008100748062134, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  64%|| 501/782 [02:27<01:22,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 0.6942756639983125, 'loss': 0.7151300311088562, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  77%|| 601/782 [02:57<00:56,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 0.6943774626973068, 'loss': 0.7007508873939514, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  90%|| 701/782 [03:26<00:24,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 0.6943266344138457, 'loss': 0.6859517693519592, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 782/782 [03:49<00:00,  3.40it/s]\n",
      "EP_test:2:   0%|| 1/782 [00:00<01:40,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_train, avg_loss= 0.6948247574022054\n",
      "EP:2 Model Saved on: models/ft_bert_sent_classifier_imdb_ep2.pt\n",
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.6918513178825378, 'loss': 0.6918513178825378, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  13%|| 102/782 [00:13<01:27,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 0.7021178278592554, 'loss': 0.7000889182090759, 'avg_acc': 0.4962871287128713}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  26%|| 202/782 [00:27<01:20,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 0.6989722023555889, 'loss': 0.7300931811332703, 'avg_acc': 0.5071517412935324}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  39%|| 302/782 [00:41<01:05,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 0.6999345396048207, 'loss': 0.745032012462616, 'avg_acc': 0.5037375415282392}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  51%|| 402/782 [00:55<00:52,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 0.7002452030740771, 'loss': 0.6863038539886475, 'avg_acc': 0.5028054862842892}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  64%|| 502/782 [01:09<00:38,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 0.6998619434838286, 'loss': 0.6905147433280945, 'avg_acc': 0.5045533932135728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  77%|| 602/782 [01:23<00:24,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 0.7010028832367375, 'loss': 0.7553405165672302, 'avg_acc': 0.5002079866888519}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2:  90%|| 702/782 [01:37<00:11,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 0.7007754353245723, 'loss': 0.6958279609680176, 'avg_acc': 0.5013373751783167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:2: 100%|| 782/782 [01:48<00:00,  7.23it/s]\n",
      "EP_train:3:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_test, avg_loss= 0.7010932782726824\n",
      "** Average Accuracy= 0.500119884910486\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:   0%|| 1/782 [00:00<03:46,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.6752429008483887, 'loss': 0.6752429008483887, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  13%|| 101/782 [00:29<03:21,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 0.6916707618401783, 'loss': 0.687860906124115, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  26%|| 201/782 [00:59<02:46,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 0.694198287245053, 'loss': 0.6988617777824402, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  38%|| 301/782 [01:28<02:19,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 0.6941491610980113, 'loss': 0.6854762434959412, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  51%|| 401/782 [01:57<01:53,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 0.6939656223144912, 'loss': 0.7001341581344604, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  64%|| 501/782 [02:26<01:23,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 0.6940438484479329, 'loss': 0.6931577324867249, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  77%|| 601/782 [02:56<00:54,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 0.6940604682373326, 'loss': 0.6873683929443359, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  90%|| 701/782 [03:26<00:24,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 0.6939111973521713, 'loss': 0.694148063659668, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 782/782 [03:50<00:00,  3.39it/s]\n",
      "EP_test:3:   0%|| 1/782 [00:00<01:40,  7.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_train, avg_loss= 0.6937951920434947\n",
      "EP:3 Model Saved on: models/ft_bert_sent_classifier_imdb_ep3.pt\n",
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.6933778524398804, 'loss': 0.6933778524398804, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  13%|| 102/782 [00:13<01:32,  7.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 0.6923752226451836, 'loss': 0.6924172639846802, 'avg_acc': 0.5167079207920792}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  26%|| 202/782 [00:27<01:20,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 0.6919139433856034, 'loss': 0.6977490186691284, 'avg_acc': 0.5242537313432836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  39%|| 302/782 [00:41<01:05,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 0.6920265939544602, 'loss': 0.6929969787597656, 'avg_acc': 0.520141196013289}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  51%|| 402/782 [00:55<00:51,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 0.6923623896596438, 'loss': 0.6904222369194031, 'avg_acc': 0.5136377805486284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  64%|| 502/782 [01:09<00:38,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 0.6923004498262844, 'loss': 0.6878274083137512, 'avg_acc': 0.5157809381237525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  77%|| 602/782 [01:23<00:25,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 0.6924160990659488, 'loss': 0.6969225406646729, 'avg_acc': 0.5137271214642263}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3:  90%|| 702/782 [01:37<00:11,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 0.6924194225911237, 'loss': 0.6972069144248962, 'avg_acc': 0.5141315977175464}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:3: 100%|| 782/782 [01:48<00:00,  7.20it/s]\n",
      "EP_train:4:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_test, avg_loss= 0.6924209343960218\n",
      "** Average Accuracy= 0.5138666879795396\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:   0%|| 1/782 [00:00<03:46,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.6906400322914124, 'loss': 0.6906400322914124, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  13%|| 101/782 [00:29<03:22,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 0.6916895862853173, 'loss': 0.6939745545387268, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  26%|| 201/782 [00:59<02:48,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 0.6920957004846032, 'loss': 0.6944126486778259, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  38%|| 301/782 [01:28<02:23,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 0.6923433962058387, 'loss': 0.6908562183380127, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  51%|| 401/782 [01:57<01:48,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 0.6926245597235282, 'loss': 0.6911389827728271, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  64%|| 501/782 [02:27<01:25,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 0.6928256336086525, 'loss': 0.6996395587921143, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  77%|| 601/782 [02:56<00:53,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 0.6928431889578427, 'loss': 0.6934700012207031, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  90%|| 701/782 [03:26<00:23,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 0.6929204963923521, 'loss': 0.6940878629684448, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 782/782 [03:50<00:00,  3.40it/s]\n",
      "EP_test:4:   0%|| 1/782 [00:00<01:41,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_train, avg_loss= 0.6930206586485324\n",
      "EP:4 Model Saved on: models/ft_bert_sent_classifier_imdb_ep4.pt\n",
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.6925820112228394, 'loss': 0.6925820112228394, 'avg_acc': 0.53125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  13%|| 102/782 [00:14<01:32,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 0.6935104261530508, 'loss': 0.6932932734489441, 'avg_acc': 0.4962871287128713}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  26%|| 202/782 [00:28<01:21,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 0.6930005719412619, 'loss': 0.6989334225654602, 'avg_acc': 0.5071517412935324}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  39%|| 302/782 [00:42<01:06,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 0.6931919560479959, 'loss': 0.7003202438354492, 'avg_acc': 0.5037375415282392}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  51%|| 402/782 [00:56<00:51,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 0.693274387249031, 'loss': 0.6906512975692749, 'avg_acc': 0.5028054862842892}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  64%|| 502/782 [01:10<00:39,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 0.6931793862236236, 'loss': 0.6906706094741821, 'avg_acc': 0.5045533932135728}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  77%|| 602/782 [01:24<00:25,  7.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 0.6933886758896356, 'loss': 0.7041993737220764, 'avg_acc': 0.5002079866888519}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4:  90%|| 702/782 [01:38<00:11,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 0.6933266299427321, 'loss': 0.6922998428344727, 'avg_acc': 0.5013373751783167}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:4: 100%|| 782/782 [01:49<00:00,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_test, avg_loss= 0.6933846326587755\n",
      "** Average Accuracy= 0.500119884910486\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load trained BERT based sentiment classification model\n",
    "print(\"Loading trained BERT based Sentiment Classifier\")\n",
    "trained_sentiment_classifier = torch.load('models/frozen_bert_sent_classifier_imdb_ep4.pt')\n",
    "finetuned_frozen_bert_model = trained_sentiment_classifier.bert\n",
    "\n",
    "# for param in trained_sentiment_classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# Set freeze=True\n",
    "print(\"Building BERT based Sentiment Classification model\")\n",
    "finetuned_sentiment_classifer_bert = SentimentClassifierBERT(finetuned_frozen_bert_model, max_len=config.MAX_LEN,\n",
    "                                                   hidden=config.EMBED_DIM, num_class = 2, \n",
    "                                                   n_layers=config.NUM_LAYERS, freeze=False)\n",
    "\n",
    "    \n",
    "trainable_para = sum([p.nelement() for p in finetuned_sentiment_classifer_bert.parameters() if p.requires_grad])\n",
    "non_trainable_para = sum([p.nelement() for p in finetuned_sentiment_classifer_bert.bert.parameters() if not p.requires_grad])\n",
    "\n",
    "print(\"===================================\")\n",
    "print(\"Total Parameters:\", trainable_para + non_trainable_para)\n",
    "print(\"Trainable Parameters:\", trainable_para)\n",
    "print(\"Non-trainable Parameters:\", non_trainable_para)\n",
    "print(\"===================================\")\n",
    "\n",
    "print(\"Creating SentimentClassifierBERT Trainer\")\n",
    "trainer = SentimentClassifierBERTTrainer(finetuned_sentiment_classifer_bert, train_dataloader=train_loader,\n",
    "                         test_dataloader=test_loader, lr=config.LR, log_freq=100)\n",
    "\n",
    "print(\"Training Start\")\n",
    "print(\"> Train the classifier with unfrozen BERT stage\")\n",
    "for epoch in range(5):\n",
    "    trainer.train(epoch)\n",
    "\n",
    "    # Save fine-tuned classifier\n",
    "    trainer.save(epoch, file_path=\"models/ft_bert_sent_classifier_imdb\" + \"_ep%d\" % epoch + \".pt\")\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        trainer.test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbmn-1gL0fcK"
   },
   "source": [
    "## Create an end-to-end model and evaluate it\n",
    "\n",
    "When you want to deploy a model, it's best if it already includes its preprocessing\n",
    "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
    "production environment. Let's create an end-to-end model that incorporates\n",
    "the `Input Representation` layer, and let's evaluate. Our model will accept raw strings\n",
    "as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For End to End Training in details\n",
    "class E2ESentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    End to end BERT-based Sentiment Classification model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, max_len=256, hidden=128, n_layers=1, attn_heads=8, num_class=2):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param num_class: number of classes\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.max_len= max_len\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # Keras example used hidden_size=128 for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, max_len=max_len, embed_size=hidden)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden, dropout) for _ in range(n_layers)]) \n",
    "\n",
    "        # classification layer\n",
    "        self.num_class = num_class\n",
    "        self.linear_1 = nn.Linear(self.hidden, int(self.hidden/2))        # 128 -> 64\n",
    "        self.linear_2 = nn.Linear(int(self.hidden/2), self.num_class-1)   # 64 -> 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Encoding layer \n",
    "        \n",
    "        x = torch.tensor([encode(x[i]) for i in range(len(x))])\n",
    "        \n",
    "        # 2. BERT Representation layer \n",
    "        \n",
    "        # attention masking for padded token\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1) \n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "        \n",
    "        # 3. Classifcation layer   \n",
    "        \n",
    "        # Extract CLS representation \n",
    "        out = out[:,:1,:].reshape(-1, self.hidden)\n",
    "        out = self.linear_1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear_2(out)\n",
    "        out = F.sigmoid(out)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Ver.\n",
    "class E2ESentimentClassifierSimple(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT based Sentiment Classification Model\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.bert_classifier = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Encoding layer      \n",
    "        encoded_x = torch.tensor([encode(x[i]) for i in range(len(x))])\n",
    "\n",
    "        # 2. bert-based sentiment classifier\n",
    "        out = self.bert_classifier(encoded_x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "class E2ESentimentClassifierTrainer:\n",
    "    \"\"\"\n",
    "    E2ESentimentClassifierTrainer make the End to end BERT based Sentiment Classification Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model,\n",
    "                 train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                 lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                 with_cuda: bool = True, log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param model: BERT based Sentiment Classification Model which you want to train\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        # CUDA out of memory.로 False 처리\n",
    "        cuda_condition = False # torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "\n",
    "        # Initialize the BERT based Sentiment Classification Model\n",
    "        # End to End Model (sentence to sentiment label)\n",
    "        self.model = model.to(self.device)\n",
    "        # Fine-tuned Model (token ids to label)\n",
    "        self.bert_classifier = model.bert_classifier\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim,  self.bert_classifier.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "        \n",
    "    # computes accuracy\n",
    "    def binary_accuracy(self, preds, y):\n",
    "        # rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        rounded_preds = torch.round(preds)\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        \"\"\"\n",
    "        loop over the data_loader for training or testing\n",
    "        if on train status, backward operation is activated\n",
    "        and also auto save the model every peoch\n",
    "        :param epoch: current epoch index\n",
    "        :param data_loader: torch.utils.data.DataLoader for iteration\n",
    "        :param train: boolean value of is train or test\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        \n",
    "        for i, data in data_iter:\n",
    "\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            # data = {key: value.to(self.device) for key, value in data.items()}\n",
    "\n",
    "            # 1. forward the bert classifier model\n",
    "            classifier_output = self.model.forward(data[\"bert_input\"]) # raw sentence (list)\n",
    "\n",
    "            # Binary Cross Entropy Loss\n",
    "            target = data[\"bert_label\"].reshape(classifier_output.shape[0], -1)\n",
    "            loss = self.criterion(classifier_output, target.to(torch.float32)) # torch.Size([32, 1]), torch.Size([32, 1])\n",
    "\n",
    "            # 2. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "    \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            # 3. Calculate Accuracy during Test\n",
    "            if not train:\n",
    "                acc = self.binary_accuracy(torch.tensor(classifier_output.reshape(-1)), data[\"bert_label\"]) # torch.Size([32])\n",
    "                avg_acc += acc.item()\n",
    "                \n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"loss\": loss.item(),\n",
    "                \"avg_acc\": avg_acc / (i + 1)\n",
    "           }\n",
    "                            \n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter))\n",
    "        \n",
    "        if not train:\n",
    "            print(\"** Average Accuracy=\", avg_acc / len(data_iter))\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "    def save(self, epoch, file_path):\n",
    "        \"\"\"\n",
    "        Saving the current BERT Classification Model on file_path\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        \n",
    "        output_path = file_path\n",
    "        torch.save(self.model.cpu(), output_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2ESentimentClassifierSimple(trained_sentiment_classifier).bert_classifier.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "lyK6uWGC0fcL"
   },
   "outputs": [],
   "source": [
    "# test_raw_dataset = BERTMLMDataset(test_df_sample.review.values, y_test)\n",
    "# test_raw_loader = DataLoader(dataset=test_raw_dataset, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "train_raw_loader = test_raw_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building End to end BERT based Sentiment Classification model\n",
      "Creating End to end BERT based Sentiment Classification Model Pre-Trainer\n",
      "Training Start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 1/782 [00:00<06:45,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 0.6973283290863037, 'loss': 0.6973283290863037, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  13%|| 101/782 [00:52<05:40,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 100, 'avg_loss': 0.6933576234496466, 'loss': 0.6934188604354858, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  26%|| 201/782 [01:44<05:17,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 200, 'avg_loss': 0.6934716419794074, 'loss': 0.6949549317359924, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  38%|| 301/782 [02:37<04:25,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 300, 'avg_loss': 0.6933421005442293, 'loss': 0.6918231248855591, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  51%|| 401/782 [03:32<03:36,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 400, 'avg_loss': 0.6932702498542995, 'loss': 0.6936849355697632, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  64%|| 501/782 [04:25<02:29,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 500, 'avg_loss': 0.6931426146787084, 'loss': 0.6955307722091675, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  77%|| 601/782 [05:17<01:39,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 600, 'avg_loss': 0.6932627850284989, 'loss': 0.6920150518417358, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  90%|| 701/782 [06:09<00:43,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 700, 'avg_loss': 0.6932474714883214, 'loss': 0.6917769908905029, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 782/782 [06:51<00:00,  1.90it/s]\n",
      "EP_train:1:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0_train, avg_loss= 0.6932618060837621\n",
      "EP:0 Model Saved on: models/e2e_bert_imdb_ep0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:   0%|| 1/782 [00:00<06:33,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 0.6959201693534851, 'loss': 0.6959201693534851, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  13%|| 101/782 [00:52<05:45,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 100, 'avg_loss': 0.6932929677538352, 'loss': 0.6931483149528503, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  26%|| 201/782 [01:43<05:05,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 200, 'avg_loss': 0.6932669803870851, 'loss': 0.6870962977409363, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  38%|| 301/782 [02:34<04:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 300, 'avg_loss': 0.6932580809656568, 'loss': 0.6914897561073303, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  51%|| 401/782 [03:26<03:21,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 400, 'avg_loss': 0.6932282367549335, 'loss': 0.6932811141014099, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  64%|| 501/782 [04:18<02:29,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 500, 'avg_loss': 0.6931224634309491, 'loss': 0.6957904100418091, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  77%|| 601/782 [05:09<01:35,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 600, 'avg_loss': 0.6932356594604581, 'loss': 0.6913183331489563, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1:  90%|| 701/782 [06:01<00:43,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 700, 'avg_loss': 0.6932121258489415, 'loss': 0.691987156867981, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:1: 100%|| 782/782 [06:43<00:00,  1.94it/s]\n",
      "EP_train:2:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP1_train, avg_loss= 0.6932347942038876\n",
      "EP:1 Model Saved on: models/e2e_bert_imdb_ep1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:   0%|| 1/782 [00:00<06:36,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 0.6956122517585754, 'loss': 0.6956122517585754, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  13%|| 101/782 [00:51<05:42,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 100, 'avg_loss': 0.6932624861745551, 'loss': 0.6931557655334473, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  26%|| 201/782 [01:43<05:10,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 200, 'avg_loss': 0.6932270010905479, 'loss': 0.6870681047439575, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  38%|| 301/782 [02:35<03:59,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 300, 'avg_loss': 0.6932449847756826, 'loss': 0.6917574405670166, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  51%|| 401/782 [03:27<03:25,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 400, 'avg_loss': 0.6932139059254654, 'loss': 0.6932945847511292, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  64%|| 501/782 [04:19<02:25,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 500, 'avg_loss': 0.6931057840050338, 'loss': 0.695604681968689, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  77%|| 601/782 [05:10<01:36,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 600, 'avg_loss': 0.6932328969190601, 'loss': 0.6919404864311218, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2:  90%|| 701/782 [06:03<00:43,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 700, 'avg_loss': 0.6932118936374082, 'loss': 0.6920902132987976, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:2: 100%|| 782/782 [06:45<00:00,  1.93it/s]\n",
      "EP_train:3:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP2_train, avg_loss= 0.693235467004654\n",
      "EP:2 Model Saved on: models/e2e_bert_imdb_ep2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:   0%|| 1/782 [00:00<06:53,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.6951404809951782, 'loss': 0.6951404809951782, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  13%|| 101/782 [00:52<05:46,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 100, 'avg_loss': 0.6932473713808721, 'loss': 0.6931476593017578, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  26%|| 201/782 [01:43<05:16,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 200, 'avg_loss': 0.6932194416795797, 'loss': 0.6880196928977966, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  38%|| 301/782 [02:35<04:03,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 300, 'avg_loss': 0.6932446762572887, 'loss': 0.6922767758369446, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  51%|| 401/782 [03:27<03:24,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 400, 'avg_loss': 0.6932042709312534, 'loss': 0.6932785511016846, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  64%|| 501/782 [04:19<02:24,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 500, 'avg_loss': 0.6931051017043596, 'loss': 0.6953201293945312, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  77%|| 601/782 [05:11<01:35,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 600, 'avg_loss': 0.6932421795738716, 'loss': 0.6928403973579407, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3:  90%|| 701/782 [06:03<00:43,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 700, 'avg_loss': 0.6932322359119094, 'loss': 0.69202721118927, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:3: 100%|| 782/782 [06:45<00:00,  1.93it/s]\n",
      "EP_train:4:   0%|| 0/782 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP3_train, avg_loss= 0.6932421984422542\n",
      "EP:3 Model Saved on: models/e2e_bert_imdb_ep3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:   0%|| 1/782 [00:00<06:42,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.6951477527618408, 'loss': 0.6951477527618408, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  13%|| 101/782 [00:52<05:50,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 100, 'avg_loss': 0.6932463486595909, 'loss': 0.6931620836257935, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  26%|| 201/782 [01:44<05:08,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 200, 'avg_loss': 0.6932324461675995, 'loss': 0.6894408464431763, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  38%|| 301/782 [02:35<03:59,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 300, 'avg_loss': 0.6932513070264924, 'loss': 0.6928225159645081, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  51%|| 401/782 [03:27<03:25,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 400, 'avg_loss': 0.6932288937437862, 'loss': 0.6931719779968262, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  64%|| 501/782 [04:18<02:25,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 500, 'avg_loss': 0.6931476004109411, 'loss': 0.6944177746772766, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  77%|| 601/782 [05:10<01:34,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 600, 'avg_loss': 0.6932224418081578, 'loss': 0.6934870481491089, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4:  90%|| 701/782 [06:06<00:43,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 700, 'avg_loss': 0.6932213333635969, 'loss': 0.6932365894317627, 'avg_acc': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:4: 100%|| 782/782 [06:49<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP4_train, avg_loss= 0.693203994608901\n",
      "EP:4 Model Saved on: models/e2e_bert_imdb_ep4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_sentiment_classifier = torch.load('models/ft_bert_sent_classifier_imdb_ep4.pt')\n",
    "trained_sentiment_classifier.train()\n",
    "\n",
    "print(\"Building End to end BERT based Sentiment Classification model\")\n",
    "\n",
    "e2e_classifier = E2ESentimentClassifierSimple(trained_sentiment_classifier)\n",
    "\n",
    "print(\"Creating End to end BERT based Sentiment Classification Model Pre-Trainer\")\n",
    "trainer = E2ESentimentClassifierTrainer(e2e_classifier, train_dataloader=train_raw_loader, test_dataloader=None, lr=config.LR, log_freq=100)\n",
    "\n",
    "print(\"Training Start\")\n",
    "for epoch in range(5):\n",
    "    trainer.train(epoch)\n",
    "    # Save BERT\n",
    "    trainer.save(epoch, file_path=\"models/e2e_bert_imdb\" + \"_ep%d\" % epoch + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_bert_sentiment_classifier = torch.load('models/e2e_bert_imdb_ep0.pt')\n",
    "e2e_bert_sentiment_classifier.train()\n",
    "\n",
    "# function to make sentiment prediction during inference\n",
    "def predict_sentiment(model, sentence):\n",
    "    \"\"\"\n",
    "    :model : end-to-end bert-based classification model\n",
    "    :sentence : text sentence, input of the end-to-end model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    prediction = model.forward(sentence)\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"inputs\")\n",
    "# print(iter(train_raw_loader).next()['bert_input'][:5])\n",
    "# print(\"labels\")\n",
    "# print(iter(train_raw_loader).next()['bert_label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506757378578186"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(e2e_bert_sentiment_classifier, ['hate this movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5079782009124756"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(e2e_bert_sentiment_classifier, ['I like this movie.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666865348816"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computes accuracy\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(preds) # preds : sigmoid output\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "binary_accuracy(torch.tensor([0.5, 0.6, 0.3]), torch.tensor([1.0,1.0, 0.0])).item()\n",
    "# torch.tensor([[0.5, 0.6, 0.3]]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 16])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10, 16)\n",
    "print(x.shape)\n",
    "mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://seducinghyeok.tistory.com/9\n",
    "x.unsqueeze(1).repeat(1, x.size(1), 1).shape # 특정 차원의 텐서 반복 ex. dim 0으로 1번, dim 1로 16번, dim 2로 1번"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(1).repeat(1, x.size(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a new tensor with a dimension of size one inserted at the specified position. \n",
    "# torch.unsqueeze(input, dim)\n",
    "x.unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mlm_and_finetune_with_bert의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
